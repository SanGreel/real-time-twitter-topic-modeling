{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import itertools\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_frame = pd.read_csv('get-tweets-by-geolocation/data/new_york_training_tweets_15_06.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tweets_LDA():\n",
    "    \n",
    "    '''\n",
    "    Class gets as input raw-data from twitter in csv format.\n",
    "    There realized methods for data preprocessing and perfoming LDA on the preprocessed data.\n",
    "    As output - words that are indicators of particular topic for tweets.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self,):\n",
    "        self.path_to_file = ''\n",
    "        self.tweets_frame = pd.DataFrame()\n",
    "        \n",
    "    def load_data(self,path_to_file):\n",
    "        self.path_to_file = path_to_file\n",
    "        self.tweets_frame = pd.read_csv(self.path_to_file)\n",
    "        \n",
    "        \n",
    "    def data_preprocessing(self,date='',channels_not_to_consider=[]):\n",
    "        '''\n",
    "        date format example: 'Sat Jun 01 00:00:02'\n",
    "        '''\n",
    "        WPT = nltk.WordPunctTokenizer()\n",
    "        lemmanizer = WordNetLemmatizer()\n",
    "        stop_word_list = nltk.corpus.stopwords.words('english')\n",
    "        \n",
    "        if date!='':\n",
    "            self.tweets_frame = self.tweets_frame[self.tweets_frame['created_at'].map(lambda x: x[:len(date)])==date]\n",
    "        if len(channels_not_to_consider)>0:\n",
    "            self.tweets_frame = self.tweets_frame[self.tweets_frame['screen_name'].isin(channels_not_to_consider)==False]\n",
    "            self.tweets_frame = self.tweets_frame[self.tweets_frame['screen_name'].map(lambda x: x.startswith('tmj_'))==False]\n",
    "        \n",
    "        self.tweets_frame['tweets_processed'] =\\\n",
    "                self.tweets_frame['tweet'].map(lambda x: [lemmanizer.lemmatize(y) for y in re.sub(\"[\\d+0-9.â€¦#!'\\\"_?,;/:()â€™%*ðŸ¤¯â€œâ€&ðŸ§¨$ðŸ§¨ðŸ§¡]\", \"\", x.lower()).split() \n",
    "                                         if y not in stop_word_list \n",
    "                                         and y not in ['ãƒ»ãƒ»ãƒ»','','-']\n",
    "                                         and not y.startswith('http')\n",
    "                                         and not y.startswith('@')])\n",
    "        \n",
    "        self.tweets_frame['tweets_processed'] =\\\n",
    "                self.tweets_frame['tweets_processed'].map(lambda x: [y for y in x if len(y)>2])\n",
    "        \n",
    "        self.tweets_frame = self.tweets_frame[self.tweets_frame['tweets_processed'].map(len)>0]\n",
    "        \n",
    "    def LDA(self,n_topics):\n",
    "        \n",
    "        def create_co_occurences_matrix(allowed_words, documents):\n",
    "            word_to_id = dict(zip(allowed_words, range(len(allowed_words))))\n",
    "            documents_as_ids = [np.sort([word_to_id[w] for w in doc if w in word_to_id]).astype('uint32') for doc in documents]\n",
    "            row_ind, col_ind = zip(*itertools.chain(*[[(i, w) for w in doc] for i, doc in enumerate(documents_as_ids)]))\n",
    "            data = np.ones(len(row_ind), dtype='uint32')  # use unsigned int for better memory utilization\n",
    "            max_word_id = max(itertools.chain(*documents_as_ids)) + 1\n",
    "            docs_words_matrix = csr_matrix((data, (row_ind, col_ind)), shape=(len(documents_as_ids), max_word_id))  # efficient arithmetic operations with CSR * CSR\n",
    "            words_cooc_matrix = docs_words_matrix.T * docs_words_matrix  # multiplying docs_words_matrix with its transpose matrix would generate the co-occurences matrix\n",
    "            words_cooc_matrix.setdiag(0)\n",
    "            return words_cooc_matrix, word_to_id \n",
    "        \n",
    "        def print_topics(model, count_vectorizer, n_top_words,words):\n",
    "            #words = cross_tab.columns\n",
    "            for topic_idx, topic in enumerate(model.components_):\n",
    "                print(\"\\nTopic #%d:\" % topic_idx)\n",
    "                print(\" \".join([words[i]+':'+str(round(topic[i]/topic.max(),4))\n",
    "                                for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "        \n",
    "        merged = list(itertools.chain.from_iterable(self.tweets_frame['tweets_processed'].values))\n",
    "        merged_cnts = np.unique(merged,return_counts=True)\n",
    "        words = merged_cnts[0][merged_cnts[1]>2]\n",
    "        a, b = create_co_occurences_matrix(words,self.tweets_frame['tweets_processed'].values)\n",
    "        lda_ = LDA(n_components=n_topics)\n",
    "        lda_.fit(a)\n",
    "        print_topics(lda_,' ',10,words)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Tweets_LDA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.load_data('get-tweets-by-geolocation/data/new_york_training_tweets_15_06.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "332548"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a.tweets_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Fri May 31 09:01:59 +0000 2019    31\n",
       "Thu Jun 13 09:01:54 +0000 2019    25\n",
       "Thu Jun 06 10:02:34 +0000 2019    25\n",
       "Thu May 30 21:04:47 +0000 2019    24\n",
       "Sat Jun 01 19:24:29 +0000 2019    22\n",
       "Thu May 30 21:20:35 +0000 2019    19\n",
       "Sat Jun 01 19:24:28 +0000 2019    19\n",
       "Thu May 30 21:04:49 +0000 2019    19\n",
       "Thu May 30 21:20:34 +0000 2019    19\n",
       "Thu May 30 21:04:48 +0000 2019    18\n",
       "Fri May 31 09:02:00 +0000 2019    18\n",
       "Thu Jun 13 09:01:55 +0000 2019    18\n",
       "Thu Jun 06 09:01:45 +0000 2019    17\n",
       "Thu May 30 21:13:44 +0000 2019    17\n",
       "Sun Jun 02 04:03:29 +0000 2019    16\n",
       "Sat Jun 01 14:01:33 +0000 2019    15\n",
       "Sat Jun 01 19:24:27 +0000 2019    15\n",
       "Thu Jun 13 10:02:30 +0000 2019    15\n",
       "Fri May 31 18:31:41 +0000 2019    15\n",
       "Thu May 30 21:13:43 +0000 2019    15\n",
       "Sun Jun 02 03:01:27 +0000 2019    15\n",
       "Thu Jun 06 09:01:46 +0000 2019    15\n",
       "Thu Jun 06 01:29:49 +0000 2019    14\n",
       "Thu May 30 21:20:36 +0000 2019    14\n",
       "Sat Jun 01 10:01:30 +0000 2019    14\n",
       "Thu Jun 06 09:01:47 +0000 2019    14\n",
       "Fri May 31 12:59:27 +0000 2019    14\n",
       "Thu May 30 21:13:46 +0000 2019    14\n",
       "Thu Jun 06 11:02:40 +0000 2019    13\n",
       "Wed Jun 12 18:31:31 +0000 2019    13\n",
       "                                  ..\n",
       "Fri Jun 07 02:53:08 +0000 2019     1\n",
       "Wed Jun 05 23:53:44 +0000 2019     1\n",
       "Wed Jun 12 23:54:05 +0000 2019     1\n",
       "Fri May 31 23:04:47 +0000 2019     1\n",
       "Sat Jun 01 09:55:38 +0000 2019     1\n",
       "Sun Jun 02 10:57:08 +0000 2019     1\n",
       "Fri May 31 21:47:43 +0000 2019     1\n",
       "Sat Jun 01 07:25:03 +0000 2019     1\n",
       "Thu Jun 06 12:57:03 +0000 2019     1\n",
       "Sun Jun 02 01:34:56 +0000 2019     1\n",
       "Sat Jun 01 17:14:19 +0000 2019     1\n",
       "Thu Jun 06 09:54:20 +0000 2019     1\n",
       "Fri May 31 19:31:46 +0000 2019     1\n",
       "Thu Jun 06 12:22:51 +0000 2019     1\n",
       "Sun Jun 02 03:00:20 +0000 2019     1\n",
       "Thu Jun 13 00:23:44 +0000 2019     1\n",
       "Wed Jun 12 21:27:02 +0000 2019     1\n",
       "Fri Jun 07 01:31:44 +0000 2019     1\n",
       "Fri May 31 10:50:19 +0000 2019     1\n",
       "Thu Jun 13 07:04:55 +0000 2019     1\n",
       "Fri May 31 19:48:23 +0000 2019     1\n",
       "Thu Jun 13 11:57:47 +0000 2019     1\n",
       "Thu Jun 13 06:49:11 +0000 2019     1\n",
       "Fri May 31 07:07:47 +0000 2019     1\n",
       "Fri Jun 07 01:36:32 +0000 2019     1\n",
       "Fri May 31 11:38:11 +0000 2019     1\n",
       "Thu Jun 06 04:35:05 +0000 2019     1\n",
       "Sun Jun 02 05:27:44 +0000 2019     1\n",
       "Wed Jun 05 23:54:40 +0000 2019     1\n",
       "Wed Jun 12 23:27:19 +0000 2019     1\n",
       "Name: created_at, Length: 217380, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.tweets_frame['created_at'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.data_preprocessing('Sat Jun 01',channels_not_to_consider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53875"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a.tweets_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic #0:\n",
      "york:1.0 new:0.1855 amp:0.1341 jersey:0.1104 nyc:0.0927 dance:0.0903 shop:0.0841 play:0.0741 city:0.0731 photo:0.0664\n",
      "\n",
      "Topic #1:\n",
      "brooklyn:1.0 know:0.8735 message:0.8705 office:0.7549 text:0.6701 robert:0.6395 lisa:0.6302 deleted:0.6096 muellers:0.6051 bronx:0.5859\n",
      "\n",
      "Topic #2:\n",
      "chance:1.0 t-storm:0.8854 tonight:0.8594 sat:0.8485 jun:0.8024 forecast:0.7899 today:0.4183 mostly:0.4019 sunny:0.3638 sunday:0.3618\n",
      "\n",
      "Topic #3:\n",
      "new:1.0 york:0.3917 nyc:0.1999 city:0.1529 time:0.125 love:0.0834 like:0.0786 brooklyn:0.0765 day:0.0763 jersey:0.0763\n",
      "\n",
      "Topic #4:\n",
      "today:1.0 amp:0.8258 day:0.7734 good:0.6666 night:0.6156 great:0.5367 morning:0.4604 come:0.4561 see:0.4435 new:0.428\n",
      "\n",
      "Topic #5:\n",
      "trump:1.0 people:0.464 amp:0.3745 president:0.337 dont:0.303 like:0.29 gun:0.2889 need:0.28 god:0.271 would:0.2665\n",
      "\n",
      "Topic #6:\n",
      "time:1.0 one:0.9948 like:0.9868 day:0.9208 get:0.8783 love:0.6939 year:0.6431 today:0.6284 see:0.6132 ive:0.6086\n",
      "\n",
      "Topic #7:\n",
      "food:1.0 music:0.7844 hookah:0.6655 djfrankswift:0.6045 valet:0.6045 sugardaddysfridays:0.6045 report:0.504 sexyentertainers:0.4882 nycnightlife:0.4423 edt:0.2258\n",
      "\n",
      "Topic #8:\n",
      "get:1.0 one:0.8745 like:0.7922 amp:0.7117 make:0.6704 people:0.5974 new:0.5448 time:0.5354 dont:0.5313 need:0.5176\n",
      "\n",
      "Topic #9:\n",
      "state:1.0 beach:0.8901 shooting:0.8602 day:0.8297 another:0.7759 time:0.7556 island:0.731 virginia:0.7082 mass:0.6673 show:0.6567\n",
      "\n",
      "Topic #10:\n",
      "ball:1.0 high:0.9226 school:0.8053 governor:0.5547 music:0.4963 gov:0.4526 festival:0.4418 day:0.4188 class:0.3942 link:0.3865\n",
      "\n",
      "Topic #11:\n",
      "liverpool:1.0 year:0.9263 final:0.9083 champion:0.8071 uclfinal:0.754 time:0.7514 team:0.7477 today:0.7458 league:0.7253 win:0.6363\n",
      "\n",
      "Topic #12:\n",
      "new:1.0 photo:0.8723 posted:0.7348 amp:0.4637 york:0.4567 nyc:0.3614 park:0.3432 art:0.3263 street:0.3247 jersey:0.2948\n",
      "\n",
      "Topic #13:\n",
      "people:1.0 new:0.9671 like:0.9139 amp:0.7303 get:0.711 trump:0.6935 one:0.6653 time:0.6159 public:0.6107 social:0.5787\n",
      "\n",
      "Topic #14:\n",
      "happy:1.0 pride:0.9665 month:0.8173 birthday:0.2788 june:0.2622 day:0.2265 pridemonth:0.2263 love:0.2244 amp:0.1896 new:0.1807\n",
      "\n",
      "Topic #15:\n",
      "saturday:1.0 park:0.8629 amp:0.6525 music:0.6229 free:0.496 day:0.4753 height:0.4262 new:0.4216 today:0.3803 nyc:0.3486\n",
      "\n",
      "Topic #16:\n",
      "freeship:1.0 case:0.9575 america:0.4648 hbo:0.4299 dvd:0.4187 bill:0.4156 claiming:0.405 tit:0.399 sayin:0.3903 unofficial:0.3871\n",
      "\n",
      "Topic #17:\n",
      "yankee:1.0 game:0.5596 red:0.4547 sox:0.3852 stadium:0.2956 win:0.2455 baseball:0.2245 tonight:0.2141 team:0.1906 mlb:0.1645\n",
      "\n",
      "Topic #18:\n",
      "like:1.0 dont:0.7717 get:0.5357 know:0.4927 people:0.46 shit:0.4067 really:0.3696 one:0.3582 lol:0.2968 cant:0.294\n",
      "\n",
      "Topic #19:\n",
      "min:1.0 delay:0.9288 due:0.8631 paper:0.8506 president:0.8344 get:0.7856 biden:0.7093 currently:0.707 need:0.6418 experiencing:0.6308\n"
     ]
    }
   ],
   "source": [
    "a.LDA(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.google.com/search?q=liverpool+%D0%BC%D0%B0%D1%82%D1%87&rlz=1C1CHZL_enUA833UA833&oq=liverpool+%D0%BC%D0%B0%D1%82%D1%87+&aqs=chrome..69i57j0l5.8428j0j7&sourceid=chrome&ie=UTF-8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "alternative_date = Tweets_LDA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "alternative_date.load_data('get-tweets-by-geolocation/data/new_york_training_tweets_15_06.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "332548"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(alternative_date.tweets_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Thu Jun 06    67546\n",
       "Sat Jun 01    59087\n",
       "Fri May 31    46951\n",
       "Wed Jun 05    38703\n",
       "Wed Jun 12    29989\n",
       "Sun Jun 02    29291\n",
       "Thu Jun 13    27292\n",
       "Fri Jun 07    16485\n",
       "Tue Jun 04    14549\n",
       "Thu May 30     2655\n",
       "Name: created_at, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alternative_date.tweets_frame['created_at'].map(lambda x: x[:10]).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "alternative_date.data_preprocessing('Thu Jun 13',channels_not_to_consider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24971"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(alternative_date.tweets_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic #0:\n",
      "trump:1.0 would:0.6534 get:0.6515 people:0.594 know:0.5672 dont:0.5633 one:0.5334 amp:0.5299 like:0.4981 say:0.4798\n",
      "\n",
      "Topic #1:\n",
      "amp:1.0 printing:0.6949 free:0.6494 card:0.582 business:0.5169 full:0.4942 new:0.4853 print:0.4441 price:0.4436 flyer:0.3729\n",
      "\n",
      "Topic #2:\n",
      "size:1.0 birthday:0.8868 rare:0.8036 white:0.7967 bid:0.7441 party:0.7313 welcome:0.7195 nike:0.711 hurry-up:0.7064 preowned:0.6657\n",
      "\n",
      "Topic #3:\n",
      "construction:1.0 town:0.9676 bloomfield:0.8781 direction:0.7982 line:0.7671 pulse:0.6917 year:0.4173 street:0.3945 center:0.3859 road:0.3562\n",
      "\n",
      "Topic #4:\n",
      "mph:1.0 wind:0.885 humidity:0.7158 -gt:0.6552 weather:0.5592 pressure:0.5485 sky:0.5244 cloud:0.507 clear:0.5029 current:0.4776\n",
      "\n",
      "Topic #5:\n",
      "cup:1.0 blue:0.8811 stanley:0.6443 boston:0.5735 stanleycup:0.5678 game:0.5438 win:0.524 team:0.464 louis:0.372 year:0.3688\n",
      "\n",
      "Topic #6:\n",
      "time:1.0 amp:0.5801 one:0.4295 real:0.3862 would:0.3442 create:0.2982 year:0.2931 people:0.2919 wednesday:0.2906 need:0.2825\n",
      "\n",
      "Topic #7:\n",
      "game:1.0 love:0.5552 blue:0.5097 fan:0.3867 one:0.3348 team:0.3242 hockey:0.3139 like:0.3063 great:0.3026 tonight:0.2936\n",
      "\n",
      "Topic #8:\n",
      "ive:1.0 one:0.9349 like:0.922 year:0.8419 time:0.8036 never:0.7408 think:0.6952 dont:0.6322 know:0.584 get:0.5543\n",
      "\n",
      "Topic #9:\n",
      "like:1.0 dont:0.7432 get:0.5517 know:0.4524 people:0.3856 one:0.3855 shit:0.3584 really:0.3558 love:0.3376 lol:0.313\n",
      "\n",
      "Topic #10:\n",
      "tonight:1.0 today:0.989 forecast:0.8248 jun:0.7911 thu:0.7856 rain:0.7582 chance:0.7411 shower:0.6132 cloudy:0.373 mostly:0.2905\n",
      "\n",
      "Topic #11:\n",
      "dream:1.0 biden:0.9708 joe:0.7962 please:0.7802 good:0.7657 checked:0.7493 hold:0.6627 palestinian:0.5989 puerto:0.5855 offer:0.5481\n",
      "\n",
      "Topic #12:\n",
      "like:1.0 dont:0.8937 make:0.7132 people:0.7017 cant:0.6141 one:0.5873 know:0.5767 see:0.5546 got:0.4829 link:0.4547\n",
      "\n",
      "Topic #13:\n",
      "new:1.0 york:0.8101 nyc:0.2151 city:0.1847 park:0.1463 brooklyn:0.1303 photo:0.1277 posted:0.112 manhattan:0.1082 street:0.0839\n",
      "\n",
      "Topic #14:\n",
      "day:1.0 get:0.7632 time:0.6872 back:0.6566 see:0.5564 good:0.5545 new:0.5244 love:0.5077 amp:0.4944 got:0.4743\n",
      "\n",
      "Topic #15:\n",
      "day:1.0 one:0.9306 like:0.7997 amp:0.7434 today:0.6095 love:0.5968 year:0.5378 last:0.5284 school:0.5276 time:0.5246\n",
      "\n",
      "Topic #16:\n",
      "case:1.0 freeship:0.874 plastic:0.4791 sleeve:0.4172 dvd:0.346 amp:0.341 black:0.3315 paper:0.284 bag:0.2827 medium:0.2643\n",
      "\n",
      "Topic #17:\n",
      "amp:1.0 food:0.4634 support:0.4324 facebook:0.4215 like:0.3734 want:0.365 new:0.3587 get:0.3422 eat:0.3271 cheese:0.3046\n",
      "\n",
      "Topic #18:\n",
      "jessica:1.0 biel:0.9976 nyc:0.8298 club:0.7602 join:0.6487 june:0.6165 like:0.587 new:0.5684 two:0.5456 day:0.5227\n",
      "\n",
      "Topic #19:\n",
      "trump:1.0 president:0.4648 amp:0.4188 family:0.3475 democrat:0.3098 support:0.2889 would:0.2886 need:0.2736 people:0.2626 dont:0.2509\n"
     ]
    }
   ],
   "source": [
    "alternative_date.LDA(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic #0:\n",
      "like:1.0 dont:0.6956 get:0.6158 one:0.6083 love:0.4831 know:0.4762 time:0.4445 people:0.4058 good:0.3962 see:0.3933\n",
      "\n",
      "Topic #1:\n",
      "new:1.0 york:0.6643 nyc:0.2856 day:0.2594 amp:0.2339 night:0.1725 time:0.1715 city:0.1676 great:0.1512 brooklyn:0.1476\n",
      "\n",
      "Topic #2:\n",
      "white:1.0 case:0.9274 size:0.9074 freeship:0.7902 rare:0.7414 bid:0.654 nike:0.6265 hurry-up:0.6208 preowned:0.585 amp:0.5696\n",
      "\n",
      "Topic #3:\n",
      "birthday:1.0 mph:0.8464 party:0.7657 welcome:0.7554 wind:0.726 humidity:0.6257 bash:0.6252 elbaeverlasting:0.5923 moneymachinewednesdays:0.5923 -gt:0.5707\n",
      "\n",
      "Topic #4:\n",
      "blue:1.0 cup:0.9341 game:0.8701 tonight:0.6438 stanleycup:0.6419 stanley:0.592 today:0.5614 boston:0.5469 win:0.5324 team:0.5231\n",
      "\n",
      "Topic #5:\n",
      "like:1.0 amp:0.9911 get:0.8736 dont:0.841 trump:0.7667 people:0.6034 know:0.5431 would:0.5285 time:0.528 want:0.5156\n"
     ]
    }
   ],
   "source": [
    "alternative_date.LDA(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://ru.wikipedia.org/wiki/%D0%9F%D0%BB%D0%B5%D0%B9-%D0%BE%D1%84%D1%84_%D0%9A%D1%83%D0%B1%D0%BA%D0%B0_%D0%A1%D1%82%D1%8D%D0%BD%D0%BB%D0%B8_2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Channels filtration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['richmintz', 'SeannyFK', 'JoshyTweetz', 'Lilsunshinegurl', 'hBencee',\n",
       "       'MichaelRMyers5', 'scumbagking__', 'BrownBagCycling', 'dchambersDPM',\n",
       "       'Ernzcognito', 'TheOnlyMikeQ', 'oheydiids', 'AshleyKrista', 'valoria_z',\n",
       "       'Eli_Rivs', 'VHLiv', 'pair_up_', 'jaydestro', 'so_many_amys', 'bjota13',\n",
       "       'Graphix_Divine7', 'johnnybebad6661', 'hannahcomedian',\n",
       "       'JoeyIannitelli', 'RichardPriem', '511nyAdirondack', 'TotalTrafficPHL',\n",
       "       'LivingLegend_23', 'TKYSK8R', 'GormoExJourno'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.tweets_frame[a.tweets_frame['tweets_processed'].map(lambda x: 'traffic' in x \n",
    "                                                      or 'lane' in x \n",
    "                                                      or 'incident' in x \n",
    "                                                      or 'blocked' in x)]\\\n",
    "                                                    ['screen_name'].value_counts().index[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['DianaLaRosa3', 'AppelHowie', 'ArrestALSNow', 'tmj_njs_hrta', 'Iam_ALW',\n",
       "       'tmj_nyc_pharm', 'tmj_RAM_itpm', 'tmj_cte_sales', 'NYPDPSA8',\n",
       "       'tmj_nwk_defben'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.tweets_frame[a.tweets_frame['tweets_processed'].map(lambda x: 'job' in x \n",
    "                                                      or 'hiring' in x \n",
    "                                                      or 'link' in x\n",
    "                                                     or 'apply' in x)]\\\n",
    "                                                    ['screen_name'].value_counts().index[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Guy173', 'Aussiethunda', 'DonMcKenzie', 'cpklapper', 'sdmack',\n",
       "       'andresflava', 'JustJoeyLopez', 'emoleechen', 'oalgarin',\n",
       "       'CharlesJHernan2', 'HippieHooper', 'tariqalhadi'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.tweets_frame[a.tweets_frame['tweets_processed'].map(lambda x: 'photo' in x \n",
    "                                                      or 'new' in x \n",
    "                                                      or 'york' in x\n",
    "                                                     or 'amp' in x)]\\\n",
    "                                                    ['screen_name'].value_counts().index[:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['WWEWomenMatter', 'almighty_red', 'riordainn', 'hammertime1009',\n",
       "       'kareemthagreat'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.tweets_frame[a.tweets_frame['tweets_processed'].map(lambda x: 'like' in x )]\\\n",
    "                                                    ['screen_name'].value_counts().index[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels_not_to_consider = [#traffic\n",
    "        '511NY', '511NYC', 'TotalTrafficNYC', '511nyNJ', '511NYMidHudson',\n",
    "       'Reported_NYC', '511ny456', '511nyAlbany', '511nyACE', '511ny123', \n",
    "        '511nyBDFV', '511nyWNY', '511nyRochester',\n",
    "       '511nyLongIsland', 'AllKindsWeather', \n",
    "       '511ny7',\n",
    "    #jobs\n",
    "    'tmj_nyc_jobs', 'tmj_nyc_adv', 'CalvaryHospJobs',\n",
    "       'tmj_RAM_cstsrv', 'tmj_nyc_mgmt', 'tmj_roc_cler',\n",
    "       'tmj_NAS_edu', 'tmj_nyc_it', 'tmj_NYC_schn', 'tmj_NJN_cstsrv',\n",
    "       'tmj_nwk_retail', 'tmj_nyc_nursing', 'tmj_nyc_sales', 'tmj_nyc_cler',\n",
    "       'USSJobs', 'tmj_nwk_sales', 'tmj_nyc_legal', 'tmj_nwk_eng',\n",
    "       'tmj_nyc_retail', 'tmj_nyc_transp', 'WGPNursingJobs', 'tmj_nwk_socsci',\n",
    "       'tmj_NYC_adm', 'nwkmeddevice', 'tmj_nyc_edu', 'tmj_nwk_secure',\n",
    "       'nwknursing',\n",
    "    'tmj_nyc_finance', 'tmj_NYC_secure', 'tmj_nyc_acct', 'tmj_nwk_cler',\n",
    "       'tmj_nyc_banking', 'tmj_ny_hrta', 'tmj_nwk_mgmt', 'tmj_nwk_auto',\n",
    "       'tmj_nyc_cstsrv', 'tmj_nyc_health', 'nwkhealth', 'tmj_nwk_cstsrv',\n",
    "       'tmj_nyc_manuf', 'tmj_cte_nursing', 'tmj_nyc_eng', 'tmj_nyc_itpm1',\n",
    "       'tmj_nyc_hr', 'tmj_NY_sales', 'CVSHealthJobs', 'tmj_NAS_mgmt',\n",
    "       'CompassJobBoard', 'tmj_nyc_hrta', 'tmj_NAS_nursing', 'BostonMarketJob',\n",
    "       'tmj_nyc_labor', 'MetsAvenue', 'tmj_nwk_schn', 'tmj_RAM_nursing',\n",
    "       'tmj_nwk_acct', 'tmj_nwk_jobs', 'tmj_nyc_art', 'tmj_nwk_labor',\n",
    "       'tmj_roc_eng', 'tmj_NAS_transp', 'CA_ROC_Jobs2', 'tmj_nyc_cosmo',\n",
    "       'tmj_RAM_edu', 'tmj_NAS_health', 'tmj_nwk_facmgmt', 'tmj_NAS_facmgmt',\n",
    "       'GodivaJobs', 'tmj_RAM_acct', 'tmj_roc_health', 'tmj_nyc_itdb',\n",
    "       'tmj_nwk_transp', 'tmj_nwk_edu', 'tmj_RAM_retail', 'tmj_RAM_mgmt',\n",
    "       'tmj_NAS_socsci', 'tmj_nwk_prod', 'tmj_nyc_realest', 'tmj_NJ_facmgmt', 'tmj_njn_retail',\n",
    "       'tmj_roc_nursing', 'tmj_nwk_finance', 'Fly_Sistah', 'tmj_NYC_skltrd',\n",
    "       'tmj_nya_nursing', 'tmj_nwk_web', 'tmj_roc_cstsrv', 'tmj_nys_jobs',\n",
    "       'tmj_njc_hrta', 'tmj_NAS_retail', 'tmj_roc_hrta',\n",
    "       'ChurchCathy', 'tmj_NAS_secure', 'tmj_RAM_art', 'tmj_NAS_labor',\n",
    "       'tmj_NAS_physici', 'tmj_nwk_skltrd', 'tmj_roc_sales', 'tmj_nwk_purch',\n",
    "       'tmj_NYS_NURSING', 'tmj_nwk_physici', 'tmj_njn_hrta', 'Mezikenyc',\n",
    "       'JCI_Jobs', 'tmj_NAS_acct', 'tmj_NYC_gensci', 'tmj_nya_eng',\n",
    "       'tmj_nwk_nonprft', 'tmj_roc_manuf', 'nwkitsupport', 'tmj_NY_LABOR',\n",
    "       'tmj_ny_mgmt', 'tmj_njn_health',\n",
    "       'tmj_nj_hrta', 'tmj_NAS_cstsrv', 'tmj_nwk_it', 'tmj_nya_transp',\n",
    "       'tmj_ct_nursing', 'tmj_NJ_sales', 'tmj_nya_acct',\n",
    "       'nwkmanuf', 'tmj_nys_cstsrv', 'tmj_njn_nursing',\n",
    "       'tmj_njn_mgmt', 'cbwaszak', 'tmj_NAS_cler', 'tmj_RAM_auto',\n",
    "       'tmj_nwk_art'\n",
    "    'WWEWomenMatter',\n",
    "    #photos\n",
    "    'ThomGambino', 'Xsanthemum', 'francesco212', 'Empressjurnee',\n",
    "       'andrerivera801', 'janice830', 'Ingridebap', 'StevieSoFetch_',\n",
    "       'EstebanDaHost', 'graceyhanderson', 'bccdny', 'brian_wood_'\n",
    "       ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
