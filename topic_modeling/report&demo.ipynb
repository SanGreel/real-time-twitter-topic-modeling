{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement \n",
    "**TODO: describe the problem**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML problem\n",
    "**TODO: describel our ml approach**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data description\n",
    "In order to collect data in a natural way:\n",
    "<br>- we registered Twitter Developer account;\n",
    "<br>- using credentials from Twitter Developer account we run script that collected tweets by the geolocation and saved them in mongodb;\n",
    "<br>\n",
    "<br><b>As a result:</b>\n",
    "<br>- we collected  332548 tweets (10Gb in mongodb, ~100Mb in csv) from New-York geolocation since 30 of May up to 15 of June;\n",
    "<br>- we collected  6617029 tweets (~1.69Gb in csv) from USA geolocation since 15 of June up to now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's start our demo ride"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import all libs needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T13:34:48.681195Z",
     "start_time": "2019-07-05T13:34:48.670618Z"
    }
   },
   "outputs": [],
   "source": [
    "# crucial thing\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T13:34:49.159490Z",
     "start_time": "2019-07-05T13:34:49.142765Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# essential pyspark\n",
    "import pyspark\n",
    "import operator\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import ArrayType, FloatType, StringType, IntegerType, StructField, StructType\n",
    "from pyspark.sql.functions import udf, row_number,column\n",
    "\n",
    "# vectorizer\n",
    "from pyspark.ml.feature import CountVectorizer, StopWordsRemover, HashingTF, IDF, Tokenizer\n",
    "\n",
    "# staff for LDA\n",
    "from pyspark.mllib.clustering import LDA, LDAModel\n",
    "from pyspark.mllib.linalg import Vector as oldVector, Vectors as oldVectors\n",
    "from pyspark.ml.linalg import Vector as newVector, Vectors as newVectors\n",
    "\n",
    "# pytrends for acquiring google trends\n",
    "from pytrends.pytrends.request import TrendReq\n",
    "\n",
    "# import hardcoded variables\n",
    "from variables import channels_not_to_consider\n",
    "\n",
    "# custom text preprocessing\n",
    "from text_preprocessing import *\n",
    "\n",
    "# custom tools to work with google trends \n",
    "from trends import *\n",
    "\n",
    "# handy functions\n",
    "from utils import *\n",
    "\n",
    "# datetime handling\n",
    "from datetime import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Global variables definition**  \n",
    "We picked some time frames to get data from to check if our topic model can extract info about events that occured during this period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final of league championship \n",
    "lc_final_start_datetime = \"Sat Jun 01 00:00:00 +0000 2019\"\n",
    "lc_finish_finish_datetime = \"Sat Jun 01 23:59:59 +0000 2019\"\n",
    "\n",
    "# Stanley cup final\n",
    "stanley_final_start_datetime = \"Wed Jun 12 00:00:00 +0000 2019\"\n",
    "stanley_finish_finish_datetime = \"Wed Jun 12 23:59:59 +0000 2019\"\n",
    "\n",
    "# Draft NBA\n",
    "nba_final_start_datetime = \"Thu Jun 20 00:00:00 +0000 2019\"\n",
    "nba_finish_finish_datetime = \"Sun Jun 23 23:59:59 +0000 2019\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**User-specific variables**  \n",
    "Please feel free to tweak those variables as you wish. For example, you can set number of last hours to get hottest topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T13:41:05.605577Z",
     "start_time": "2019-07-05T13:41:05.601192Z"
    }
   },
   "outputs": [],
   "source": [
    "# if True locations from locations_to_consider will be used to filter\n",
    "get_from_location = True\n",
    "\n",
    "# locations to filter relevant tweets\n",
    "locations_to_consider = [\n",
    "                         'Manhattan, NY', \n",
    "                         'Brooklyn, NY', \n",
    "                         'Queens, NY', \n",
    "                         'Bronx, NY', \n",
    "                         'Staten Island, NY'\n",
    "                         'New York, USA'\n",
    "                        ]\n",
    "\n",
    "# choose US if you want to consider whole US area\n",
    "# used by google trends\n",
    "geo = \"US-NY\" \n",
    "\n",
    "# hyperparams\n",
    "number_of_hours_to_get_topics = 2\n",
    "num_of_top_interest = 15\n",
    "\n",
    "# set window time for interesting\n",
    "frame_start_datetime = str_tweet_to_datetime(stanley_final_start_datetime)\n",
    "frame_finish_datetime = str_tweet_to_datetime(stanley_finish_finish_datetime)\n",
    "\n",
    "assert (frame_finish_datetime - frame_start_datetime).days <= 3, \"Date interval should not be bigger than 3 days\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Technical variables**  \n",
    "Those variables are needed to connect to db and other technical stuff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T13:38:36.620763Z",
     "start_time": "2019-07-05T13:38:36.617592Z"
    }
   },
   "outputs": [],
   "source": [
    "# LDA params\n",
    "num_of_topics_LDA = 10\n",
    "max_iterations_LDA = 100\n",
    "number_of_words_for_topic = 15  # number of words per topic\n",
    "\n",
    "# path to CSV\n",
    "historical_tweets_data = '../get-tweets-by-geolocation/data/new_york_training_tweets_15_06.csv'\n",
    "# historical_tweets_data = './get-tweets-by-geolocation/training_tweets.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T13:38:37.435833Z",
     "start_time": "2019-07-05T13:38:37.427311Z"
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"pipeline\").getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Google Trends data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T13:38:43.580633Z",
     "start_time": "2019-07-05T13:38:43.526264Z"
    }
   },
   "outputs": [],
   "source": [
    "google_trends_search_queries_us = spark.read.csv('../data/google-trends/google-trends-search-queries-US.csv', inferSchema=True, header=True)\n",
    "google_trends_search_topics_us = spark.read.csv('../data/google-trends/google-trends-search-topics-US.csv', inferSchema=True, header=True)\n",
    "google_trends_search_queries_us_ny = spark.read.csv('../data/google-trends/google-trends-search-queries-US-NY.csv', inferSchema=True, header=True)\n",
    "google_trends_search_topics_us_ny = spark.read.csv('../data/google-trends/google-trends-search-topics-US-NY.csv', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the historical data, it can take a while**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T13:38:45.189768Z",
     "start_time": "2019-07-05T13:38:45.184190Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time range to be extracted from  ../get-tweets-by-geolocation/data/new_york_training_tweets_15_06.csv 2019-06-12 00:00:00+00:00 2019-06-12 23:59:59+00:00\n",
      "Range for collected data (history):  2019-06-12 00:00:00+00:00 2019-06-12 23:59:59+00:00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "29538"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "times = (frame_start_datetime, frame_finish_datetime)\n",
    "print(\"Time range to be extracted from \", historical_tweets_data, times[0], times[1])\n",
    "selected_df = get_historical_df(historical_tweets_data=historical_tweets_data, historical_start_time=times[0], historical_finish_time=times[1], spark=spark)\n",
    "assert selected_df != None, \"Something goes wrong with selecting data from recent data/history data\"\n",
    "selected_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweets preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text cleaning is crucial for any text modelling process, especially for topic modelling. In our case it consists from those steps:  \n",
    "1) Lowercase all words  \n",
    "2) Filter words with non-letters at the beginning (mainly for mentions, e.g. \"@some_user\")  \n",
    "3) Filter http/https  \n",
    "4) Filter all non-letters (crucial to remove emoji)  \n",
    "5) Remove multiply whitespaces  \n",
    "6) Remove repeated chars (e.g. \"greeeeat\" -> \"great\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = selected_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter nans\n",
    "df = df.rdd.filter(lambda x: x[0] != None and x[1] != None and x[2] != None and x[4] != None)\n",
    "\n",
    "# filter out channels not to consider\n",
    "df = df.filter(lambda x: x[4] not in channels_not_to_consider)\n",
    "\n",
    "# filter by country\n",
    "df = df.filter(lambda x: x[1] in 'US')\n",
    "\n",
    "# filter by precise location\n",
    "if get_from_location:\n",
    "    df = df.filter(lambda x: x[2] in locations_to_consider)\n",
    "\n",
    "# filter tweet itself\n",
    "df = df.filter(lambda x: filter_tweet(x[0], channels_not_to_consider=channels_not_to_consider))\n",
    "\n",
    "# process tweet\n",
    "df = df.map(lambda x: process_tweet(x[0]))\n",
    "\n",
    "# final preprocesssing\n",
    "df = df.filter(lambda x: len(x) > 0)\n",
    "\n",
    "# make dataframes great again\n",
    "df = df.map(lambda x: [x])\n",
    "\n",
    "# schema for df\n",
    "schema = StructType([StructField('tokens', ArrayType(StringType()), True)])\n",
    "df = df.toDF(schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|              tokens|\n",
      "+--------------------+\n",
      "|[sad, news, noise...|\n",
      "|[first, thing, ge...|\n",
      "|               [lit]|\n",
      "|[ironically, star...|\n",
      "|   [thank, bro, bro]|\n",
      "|         [yeah, sir]|\n",
      "|[indeed, stan, fo...|\n",
      "|[good, draping, e...|\n",
      "|[leftover, tuesda...|\n",
      "|[anyone, want, em...|\n",
      "+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17398"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic modeling/Latent Dirichlet allocation(LDA)\n",
    "**TODO: describe ml solution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2.show(10, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07172019 22:31:29\n",
      "07172019 22:32:00\n"
     ]
    }
   ],
   "source": [
    "print(time.strftime('%m%d%Y %H:%M:%S'))\n",
    "\n",
    "cv = CountVectorizer(inputCol=\"tokens\", outputCol=\"raw_features\", vocabSize=10000, minDF=2.0)\n",
    "cvmodel = cv.fit(df)\n",
    "\n",
    "print(time.strftime('%m%d%Y %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07172019 22:32:00\n",
      "07172019 22:32:00\n"
     ]
    }
   ],
   "source": [
    "print(time.strftime('%m%d%Y %H:%M:%S'))\n",
    "df = cvmodel.transform(df)\n",
    "print(time.strftime('%m%d%Y %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"tf_idf_features\", minDocFreq=2)\n",
    "idfModel = idf.fit(df)\n",
    "\n",
    "df = idfModel.transform(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|              tokens|        raw_features|     tf_idf_features|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|[sad, news, noise...|(7583,[287,369,63...|(7583,[287,369,63...|\n",
      "|[first, thing, ge...|(7583,[2,21,36,37...|(7583,[2,21,36,37...|\n",
      "|               [lit]|  (7583,[753],[1.0])|(7583,[753],[6.54...|\n",
      "|[ironically, star...|(7583,[403,862,11...|(7583,[403,862,11...|\n",
      "|   [thank, bro, bro]|(7583,[18,150],[1...|(7583,[18,150],[3...|\n",
      "|         [yeah, sir]|(7583,[120,675],[...|(7583,[120,675],[...|\n",
      "|[indeed, stan, fo...|(7583,[141,259,42...|(7583,[141,259,42...|\n",
      "|[good, draping, e...|(7583,[14,79,679,...|(7583,[14,79,679,...|\n",
      "|[leftover, tuesda...|(7583,[154,1093,1...|(7583,[154,1093,1...|\n",
      "|[anyone, want, em...|(7583,[20,108,706...|(7583,[20,108,706...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.drop(\"name\")\n",
    "#df.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = Window().orderBy(column(\"tokens\"))\n",
    "df = df.withColumn(\"id\", row_number().over(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+---+\n",
      "|              tokens|        raw_features|     tf_idf_features| id|\n",
      "+--------------------+--------------------+--------------------+---+\n",
      "|[aapl, strong, da...|(7583,[0,10,17,21...|(7583,[0,10,17,21...|  1|\n",
      "|[aaron, coming, b...|(7583,[32,148,144...|(7583,[32,148,144...|  2|\n",
      "|[aaron, got, full...|(7583,[12,30,106,...|(7583,[12,30,106,...|  3|\n",
      "|[abandoned, novel...|(7583,[16,115,174...|(7583,[16,115,174...|  4|\n",
      "|[abingdon, market...|(7583,[1,4,24,708...|(7583,[1,4,24,708...|  5|\n",
      "|[able, convert, s...|(7583,[11,525,657...|(7583,[11,525,657...|  6|\n",
      "|[able, cop, gener...|(7583,[54,118,222...|(7583,[54,118,222...|  7|\n",
      "|           [able, w]|(7583,[76,657],[1...|(7583,[76,657],[4...|  8|\n",
      "| [aboard, tug, boat]|(7583,[3111,4114]...|(7583,[3111,4114]...|  9|\n",
      "|[abortion, listen...| (7583,[1494],[1.0])|(7583,[1494],[7.1...| 10|\n",
      "+--------------------+--------------------+--------------------+---+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = df.rdd.map(lambda x: (x[3], oldVectors.fromML(x[2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|_1 |_2                                                                                                                                                                                                                                                                                                 |\n",
      "+---+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|1  |(7583,[0,10,17,21,35,132,161,233,252,620,637,957,1235],[2.971823584815907,3.6199823781610703,3.86153467888535,3.906234857803257,4.17318103177586,5.082036785162496,5.2533085057698665,5.501488135245401,5.605284928927045,6.362970630624561,6.362970630624561,6.819729033120276,7.056117811184506])|\n",
      "|2  |(7583,[32,148,1441,1837],[4.1546962171017565,5.149047495445457,7.199218654825179,7.46158291929267])                                                                                                                                                                                                |\n",
      "|3  |(7583,[12,30,106,309,657,685,837,1837,2523],[3.7310817904879148,4.165746053288341,4.968377466689975,5.721116744452166,6.396872182300243,6.468331146282387,6.719645574563293,7.46158291929267,7.818257863231403])                                                                                   |\n",
      "|4  |(7583,[16,115,174,538,1251,1626,2044,5083],[3.8345788688968216,5.019235883923466,5.275531642554577,6.208819950797302,7.056117811184506,7.366272739488346,7.566943434950497,8.377873651166826])                                                                                                     |\n",
      "|5  |(7583,[1,4,24,708],[3.1454290287694966,3.5945572797952603,3.9803428299569785,6.506071474265235])                                                                                                                                                                                                   |\n",
      "|6  |(7583,[11,525,657,4710],[3.7167958332404383,6.208819950797302,6.396872182300243,8.377873651166826])                                                                                                                                                                                                |\n",
      "|7  |(7583,[54,118,222,231,657,1052,1425,6214],[4.496309853223388,5.019235883923466,5.446679898750406,5.487501893270661,6.396872182300243,6.873796254390552,7.199218654825179,8.665555723618606])                                                                                                       |\n",
      "|8  |(7583,[76,657],[4.753532718190461,6.396872182300243])                                                                                                                                                                                                                                              |\n",
      "|9  |(7583,[3111,4114],[8.154730099852616,8.377873651166826])                                                                                                                                                                                                                                           |\n",
      "|10 |(7583,[1494],[7.199218654825179])                                                                                                                                                                                                                                                                  |\n",
      "+---+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rs_df = rs.toDF()\n",
    "rs_df.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07172019 22:34:04\n",
      "07172019 22:35:18\n"
     ]
    }
   ],
   "source": [
    "# Run the LDA Topic Modeler\n",
    "# Note the time before and after is printed in order to find out how much time it takes to process x number of records\n",
    "\n",
    "print(time.strftime('%m%d%Y %H:%M:%S'))\n",
    "lda_model = LDA.train(rs_df['_1', '_2'].rdd.map(list), k=10, maxIterations=100)\n",
    "print(time.strftime('%m%d%Y %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07172019 22:35:18\n"
     ]
    }
   ],
   "source": [
    "print(time.strftime('%m%d%Y %H:%M:%S'))\n",
    "topics = lda_model.topicsMatrix()\n",
    "vocabArray = cvmodel.vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07172019 22:35:20\n"
     ]
    }
   ],
   "source": [
    "wordNumbers = 15\n",
    "\n",
    "topicIndices = sc.parallelize(lda_model.describeTopics(maxTermsPerTopic = wordNumbers))\n",
    "\n",
    "def topic_render(topic):  # specify vector id of words to actual words\n",
    "    terms = topic[0]\n",
    "    prob = topic[1]\n",
    "    \n",
    "    result = []\n",
    "    for i in range(number_of_words_for_topic):\n",
    "        term = str(round(prob[i],3))+\"  \"+vocabArray[terms[i]]\n",
    "        result.append(term)\n",
    "    return result\n",
    "print(time.strftime('%m%d%Y %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07172019 22:35:20\n",
      "07172019 22:35:20\n"
     ]
    }
   ],
   "source": [
    "print(time.strftime('%m%d%Y %H:%M:%S'))\n",
    "topics_final = topicIndices.map(lambda topic:topic_render(topic)).collect()\n",
    "print(time.strftime('%m%d%Y %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #1\n",
      "0.032  u\n",
      "0.032  love\n",
      "0.024  thank\n",
      "0.02  work\n",
      "0.02  right\n",
      "0.019  back\n",
      "0.017  let\n",
      "0.016  always\n",
      "0.014  fuck\n",
      "0.013  also\n",
      "0.011  big\n",
      "0.01  looking\n",
      "0.01  trying\n",
      "0.009  watch\n",
      "0.009  lot\n",
      "\n",
      "\n",
      "Topic #2\n",
      "0.03  one\n",
      "0.029  day\n",
      "0.026  good\n",
      "0.026  got\n",
      "0.022  today\n",
      "0.021  year\n",
      "0.018  look\n",
      "0.018  thing\n",
      "0.016  much\n",
      "0.015  last\n",
      "0.015  feel\n",
      "0.013  guy\n",
      "0.012  gonna\n",
      "0.011  night\n",
      "0.011  week\n",
      "\n",
      "\n",
      "Topic #3\n",
      "0.028  see\n",
      "0.027  like\n",
      "0.026  lol\n",
      "0.015  first\n",
      "0.014  said\n",
      "0.013  real\n",
      "0.013  nigga\n",
      "0.012  job\n",
      "0.011  wait\n",
      "0.011  tell\n",
      "0.011  many\n",
      "0.01  wanna\n",
      "0.009  white\n",
      "0.009  ok\n",
      "0.008  team\n",
      "\n",
      "\n",
      "Topic #4\n",
      "0.048  new\n",
      "0.036  york\n",
      "0.031  time\n",
      "0.025  go\n",
      "0.022  think\n",
      "0.015  come\n",
      "0.014  oh\n",
      "0.013  city\n",
      "0.012  give\n",
      "0.011  as\n",
      "0.01  ya\n",
      "0.01  world\n",
      "0.01  park\n",
      "0.009  try\n",
      "0.008  manhattan\n",
      "\n",
      "\n",
      "Topic #5\n",
      "0.022  make\n",
      "0.021  would\n",
      "0.019  lmao\n",
      "0.018  even\n",
      "0.017  life\n",
      "0.015  happy\n",
      "0.015  best\n",
      "0.012  someone\n",
      "0.012  w\n",
      "0.01  thought\n",
      "0.01  mean\n",
      "0.009  b\n",
      "0.009  bitch\n",
      "0.009  hate\n",
      "0.009  person\n",
      "\n",
      "\n",
      "Topic #6\n",
      "0.039  get\n",
      "0.028  need\n",
      "0.025  want\n",
      "0.013  please\n",
      "0.013  girl\n",
      "0.013  woman\n",
      "0.012  baby\n",
      "0.012  trump\n",
      "0.01  god\n",
      "0.01  home\n",
      "0.01  sure\n",
      "0.01  talk\n",
      "0.009  money\n",
      "0.009  like\n",
      "0.009  long\n",
      "\n",
      "\n",
      "Topic #7\n",
      "0.031  know\n",
      "0.022  shit\n",
      "0.02  say\n",
      "0.016  man\n",
      "0.016  people\n",
      "0.015  friend\n",
      "0.015  yes\n",
      "0.014  well\n",
      "0.013  im\n",
      "0.012  stop\n",
      "0.012  lmfao\n",
      "0.011  getting\n",
      "0.011  fucking\n",
      "0.01  hope\n",
      "0.01  everyone\n",
      "\n",
      "\n",
      "Topic #8\n",
      "0.025  really\n",
      "0.02  never\n",
      "0.02  going\n",
      "0.018  take\n",
      "0.018  show\n",
      "0.017  every\n",
      "0.014  ever\n",
      "0.012  could\n",
      "0.011  morning\n",
      "0.011  keep\n",
      "0.01  old\n",
      "0.01  actually\n",
      "0.009  wow\n",
      "0.009  bro\n",
      "0.009  play\n",
      "\n",
      "\n",
      "Topic #9\n",
      "0.029  amp\n",
      "0.022  ny\n",
      "0.022  nyc\n",
      "0.019  great\n",
      "0.017  th\n",
      "0.014  brooklyn\n",
      "0.014  better\n",
      "0.013  next\n",
      "0.012  tonight\n",
      "0.012  pm\n",
      "0.012  made\n",
      "0.01  june\n",
      "0.009  street\n",
      "0.008  open\n",
      "0.008  place\n",
      "\n",
      "\n",
      "Topic #10\n",
      "0.019  still\n",
      "0.017  way\n",
      "0.013  game\n",
      "0.012  summer\n",
      "0.011  amazing\n",
      "0.011  anyone\n",
      "0.01  yeah\n",
      "0.01  video\n",
      "0.009  music\n",
      "0.009  black\n",
      "0.008  check\n",
      "0.008  making\n",
      "0.008  part\n",
      "0.008  case\n",
      "0.008  miss\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# based on the simple vectors(+number of words)\n",
    "\n",
    "for topic in range(len(topics_final)):\n",
    "    print (\"Topic #\" + str(topic+1) + \"\")\n",
    "    for term in topics_final[topic]:\n",
    "        print (term)\n",
    "    print ('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hot topics in the USA from [Google trends](https://trends.google.com/trends/explore?geo=US)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = frame_start_datetime #str_tweet_to_datetime(frame_start_datetime)\n",
    "finish_date = frame_finish_datetime #str_tweet_to_datetime(frame_finish_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_trends_topics, google_trends_queries = get_google_trends_by_geo(geo) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Google trends search queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "interesting_google_topics = google_trends_topics.filter(\n",
    "    (google_trends_topics.Date >= start_date) & (google_trends_topics.Date <= finish_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Google trends Search topics in New York during 2019-06-12 - 2019-06-12\n",
      "+----------+----------------------+----------------------------------------+\n",
      "|Date      |Search topics - rising|Search topics - top                     |\n",
      "+----------+----------------------+----------------------------------------+\n",
      "|2019-06-12|Download - Topic      |New York - City in New York             |\n",
      "|2019-06-12|null                  |New York - US State                     |\n",
      "|2019-06-12|null                  |Google Search - Topic                   |\n",
      "|2019-06-12|null                  |Google - Technology company             |\n",
      "|2019-06-12|null                  |2019 - Topic                            |\n",
      "|2019-06-12|null                  |Weather - Topic                         |\n",
      "|2019-06-12|null                  |YouTube - Video sharing company         |\n",
      "|2019-06-12|null                  |Facebook, Inc. - Social network company |\n",
      "|2019-06-12|null                  |Facebook - Social networking service    |\n",
      "|2019-06-12|null                  |Amazon.com - E-commerce company         |\n",
      "|2019-06-12|null                  |Film - Topic                            |\n",
      "|2019-06-12|null                  |Definition - Topic                      |\n",
      "|2019-06-12|null                  |Brooklyn - New York City borough        |\n",
      "|2019-06-12|null                  |United States - Country in North America|\n",
      "|2019-06-12|null                  |New Jersey - US State                   |\n",
      "+----------+----------------------+----------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_google_trend_title(start_date, finish_date, \"Search topics\")\n",
    "interest_google_topics = convert_datetime_in_interesting_google(interesting_google_topics)\n",
    "interest_google_topics.select(\"Date\",\"Search topics - rising\", \"Search topics - top\").show(num_of_top_interest, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case when timeframe is more than 1 day, filter correctly this google-trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interesing_google_topics_unique= unique_google_trends_by_time_frame(interesting_google_topics)\n",
    "# print_google_trend_title(start_date, finish_date, \"Search topics\")\n",
    "# interesing_google_topics_unique.select(\"Search topics - rising\", \"Search topics - top\").show(num_of_top_interest, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Google trends search queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "interesting_google_queries = google_trends_queries.filter(\n",
    "    (google_trends_queries.Date >= start_date) & (google_trends_queries.Date <= finish_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Google trends Search queries in New York during 2019-06-12 - 2019-06-12\n",
      "+-----------------------+--------------------+------+---+-----+\n",
      "|Search queries - rising|Search queries - top|Rising|Top|geo  |\n",
      "+-----------------------+--------------------+------+---+-----+\n",
      "|sudanese massacre      |google              |+300% |100|US-NY|\n",
      "|hong kong              |weather             |+250% |79 |US-NY|\n",
      "|ny yankees score       |facebook            |+180% |59 |US-NY|\n",
      "|iready                 |youtube             |+150% |54 |US-NY|\n",
      "|boston bruins          |amazon              |+150% |49 |US-NY|\n",
      "|raz kids               |news                |+110% |46 |US-NY|\n",
      "|bruins                 |definition          |+110% |33 |US-NY|\n",
      "|cool math games        |craigslist          |+100% |28 |US-NY|\n",
      "|verizon wireless       |gmail               |+90%  |24 |US-NY|\n",
      "|bitcoin price          |translate           |+90%  |23 |US-NY|\n",
      "|scratch                |nba                 |+90%  |23 |US-NY|\n",
      "|max landis             |instagram           |+90%  |22 |US-NY|\n",
      "|sudan                  |yankees             |+80%  |21 |US-NY|\n",
      "|nhl playoffs           |people              |+80%  |21 |US-NY|\n",
      "|trump news             |yahoo               |+70%  |19 |US-NY|\n",
      "+-----------------------+--------------------+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "interesing_google_queries_unique= unique_google_trends_by_time_frame(interesting_google_queries)\n",
    "print_google_trend_title(start_date, finish_date, \"Search queries\")\n",
    "interesing_google_queries_unique.show(num_of_top_interest, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print_google_trend_title(start_date, finish_date, \"Search queries\")\n",
    "# interest_google_queries = convert_datetime_in_interesting_google(interesting_google_queries)\n",
    "# interest_google_queries.select(\"Date\", \"Search queries - rising\", \"Search queries - top\").show(num_of_top_interest, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hot topics - google trends (directly) (probably this will be removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date_str = start_date.strftime(\"%Y-%m-%d\")\n",
    "finish_date_str = finish_date.strftime(\"%Y-%m-%d\")\n",
    "pytrend = TrendReq()\n",
    "pytrend.build_payload(kw_list=[' '], geo=geo, timeframe=f\"{start_date_str} {finish_date_str}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Search topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_df = pytrend.related_top_search_topics(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Google trends Search topics in New York during 2019-06-12 - 2019-06-12\n",
      "+-------------------------+----------------------------------------+\n",
      "|Search topics - rising   |Search topics - top                     |\n",
      "+-------------------------+----------------------------------------+\n",
      "|Ice - Topic              |New York - City in New York             |\n",
      "|Mathematical game - Topic|New York - US State                     |\n",
      "|Design - Topic           |Google - Technology company             |\n",
      "|nan                      |Google Search - Topic                   |\n",
      "|nan                      |2019 - Topic                            |\n",
      "|nan                      |Weather - Topic                         |\n",
      "|nan                      |YouTube - Video sharing company         |\n",
      "|nan                      |Facebook - Social networking service    |\n",
      "|nan                      |Facebook, Inc. - Social network company |\n",
      "|nan                      |Amazon.com - E-commerce company         |\n",
      "|nan                      |Definition - Topic                      |\n",
      "|nan                      |Film - Topic                            |\n",
      "|nan                      |United States - Country in North America|\n",
      "|nan                      |Brooklyn - New York City borough        |\n",
      "|nan                      |Car - Transportation mode               |\n",
      "+-------------------------+----------------------------------------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_google_trend_title(start_date, finish_date, \"Search topics\")\n",
    "topics_df.select(\"Search topics - rising\", \"Search topics - top\").show(num_of_top_interest, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Search queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_df = pytrend.related_top_search_queries(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Google trends Search queries in New York during 2019-06-12 - 2019-06-12\n",
      "+-----------------------+--------------------+------+---+-----+\n",
      "|Search queries - rising|Search queries - top|Rising|Top|geo  |\n",
      "+-----------------------+--------------------+------+---+-----+\n",
      "|hong kong              |google              |+350% |100|US-NY|\n",
      "|courtney stodden       |weather             |+300% |80 |US-NY|\n",
      "|sudanese massacre      |facebook            |+250% |60 |US-NY|\n",
      "|khloe kardashian       |youtube             |+250% |56 |US-NY|\n",
      "|cricinfo               |amazon              |+190% |53 |US-NY|\n",
      "|iready                 |news                |+180% |42 |US-NY|\n",
      "|neiman marcus          |craigslist          |+110% |27 |US-NY|\n",
      "|jupiter ed             |world cup           |+100% |25 |US-NY|\n",
      "|ny lottery post        |instagram           |+90%  |25 |US-NY|\n",
      "|adidas                 |nba                 |+90%  |25 |US-NY|\n",
      "|alex morgan            |gmail               |+80%  |22 |US-NY|\n",
      "|pbs kids               |map                 |+80%  |22 |US-NY|\n",
      "|jon stewart            |drive               |+80%  |21 |US-NY|\n",
      "|mega millions          |apple               |+80%  |21 |US-NY|\n",
      "|chinese food           |translate           |+70%  |20 |US-NY|\n",
      "+-----------------------+--------------------+------+---+-----+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_google_trend_title(start_date, finish_date, \"Search queries\")\n",
    "queries_df.show(num_of_top_interest, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
