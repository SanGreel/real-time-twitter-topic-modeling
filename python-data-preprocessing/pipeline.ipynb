{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels_not_to_consider = [#traffic\n",
    "        '511NY', '511NYC', 'TotalTrafficNYC', '511nyNJ', '511NYMidHudson',\n",
    "       'Reported_NYC', '511ny456', '511nyAlbany', '511nyACE', '511ny123', \n",
    "        '511nyBDFV', '511nyWNY', '511nyRochester',\n",
    "       '511nyLongIsland', 'AllKindsWeather', \n",
    "       '511ny7',\n",
    "    #jobs\n",
    "    'tmj_nyc_jobs', 'tmj_nyc_adv', 'CalvaryHospJobs',\n",
    "       'tmj_RAM_cstsrv', 'tmj_nyc_mgmt', 'tmj_roc_cler',\n",
    "       'tmj_NAS_edu', 'tmj_nyc_it', 'tmj_NYC_schn', 'tmj_NJN_cstsrv',\n",
    "       'tmj_nwk_retail', 'tmj_nyc_nursing', 'tmj_nyc_sales', 'tmj_nyc_cler',\n",
    "       'USSJobs', 'tmj_nwk_sales', 'tmj_nyc_legal', 'tmj_nwk_eng',\n",
    "       'tmj_nyc_retail', 'tmj_nyc_transp', 'WGPNursingJobs', 'tmj_nwk_socsci',\n",
    "       'tmj_NYC_adm', 'nwkmeddevice', 'tmj_nyc_edu', 'tmj_nwk_secure',\n",
    "       'nwknursing',\n",
    "    'tmj_nyc_finance', 'tmj_NYC_secure', 'tmj_nyc_acct', 'tmj_nwk_cler',\n",
    "       'tmj_nyc_banking', 'tmj_ny_hrta', 'tmj_nwk_mgmt', 'tmj_nwk_auto',\n",
    "       'tmj_nyc_cstsrv', 'tmj_nyc_health', 'nwkhealth', 'tmj_nwk_cstsrv',\n",
    "       'tmj_nyc_manuf', 'tmj_cte_nursing', 'tmj_nyc_eng', 'tmj_nyc_itpm1',\n",
    "       'tmj_nyc_hr', 'tmj_NY_sales', 'CVSHealthJobs', 'tmj_NAS_mgmt',\n",
    "       'CompassJobBoard', 'tmj_nyc_hrta', 'tmj_NAS_nursing', 'BostonMarketJob',\n",
    "       'tmj_nyc_labor', 'MetsAvenue', 'tmj_nwk_schn', 'tmj_RAM_nursing',\n",
    "       'tmj_nwk_acct', 'tmj_nwk_jobs', 'tmj_nyc_art', 'tmj_nwk_labor',\n",
    "       'tmj_roc_eng', 'tmj_NAS_transp', 'CA_ROC_Jobs2', 'tmj_nyc_cosmo',\n",
    "       'tmj_RAM_edu', 'tmj_NAS_health', 'tmj_nwk_facmgmt', 'tmj_NAS_facmgmt',\n",
    "       'GodivaJobs', 'tmj_RAM_acct', 'tmj_roc_health', 'tmj_nyc_itdb',\n",
    "       'tmj_nwk_transp', 'tmj_nwk_edu', 'tmj_RAM_retail', 'tmj_RAM_mgmt',\n",
    "       'tmj_NAS_socsci', 'tmj_nwk_prod', 'tmj_nyc_realest', 'tmj_NJ_facmgmt', 'tmj_njn_retail',\n",
    "       'tmj_roc_nursing', 'tmj_nwk_finance', 'Fly_Sistah', 'tmj_NYC_skltrd',\n",
    "       'tmj_nya_nursing', 'tmj_nwk_web', 'tmj_roc_cstsrv', 'tmj_nys_jobs',\n",
    "       'tmj_njc_hrta', 'tmj_NAS_retail', 'tmj_roc_hrta',\n",
    "       'ChurchCathy', 'tmj_NAS_secure', 'tmj_RAM_art', 'tmj_NAS_labor',\n",
    "       'tmj_NAS_physici', 'tmj_nwk_skltrd', 'tmj_roc_sales', 'tmj_nwk_purch',\n",
    "       'tmj_NYS_NURSING', 'tmj_nwk_physici', 'tmj_njn_hrta', 'Mezikenyc',\n",
    "       'JCI_Jobs', 'tmj_NAS_acct', 'tmj_NYC_gensci', 'tmj_nya_eng',\n",
    "       'tmj_nwk_nonprft', 'tmj_roc_manuf', 'nwkitsupport', 'tmj_NY_LABOR',\n",
    "       'tmj_ny_mgmt', 'tmj_njn_health',\n",
    "       'tmj_nj_hrta', 'tmj_NAS_cstsrv', 'tmj_nwk_it', 'tmj_nya_transp',\n",
    "       'tmj_ct_nursing', 'tmj_NJ_sales', 'tmj_nya_acct',\n",
    "       'nwkmanuf', 'tmj_nys_cstsrv', 'tmj_njn_nursing',\n",
    "       'tmj_njn_mgmt', 'cbwaszak', 'tmj_NAS_cler', 'tmj_RAM_auto',\n",
    "       'tmj_nwk_art'\n",
    "    'WWEWomenMatter',\n",
    "    #photos\n",
    "    'ThomGambino', 'Xsanthemum', 'francesco212', 'Empressjurnee',\n",
    "       'andrerivera801', 'janice830', 'Ingridebap', 'StevieSoFetch_',\n",
    "       'EstebanDaHost', 'graceyhanderson', 'bccdny', 'brian_wood_'\n",
    "       ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"TweetsLDA\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('../get-tweets-by-geolocation/data/usa_training_tweets_23_06.csv', header=True, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df.rdd.filter(lambda x: filter_tweet(x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1177561"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1292100"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = sc.parallelize(df.take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.WordPunctTokenizer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_word_list = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_tweet(tweet):\n",
    "    \n",
    "    if not isinstance(tweet, str):\n",
    "        is_filtered = True\n",
    "    elif len(tweet.split(' ')) < 3:\n",
    "        is_filtered = True\n",
    "    else: \n",
    "        is_filtered = False\n",
    "        \n",
    "    return not is_filtered\n",
    "         \n",
    "def process_tweet(tweet):\n",
    "   \n",
    "    tweet = tweet.lower() # get lowercase\n",
    "    tweet = re.sub(r'@\\w+', '', tweet) # filter words with non-letters at the beginning (mainly for mentions)\n",
    "    tweet = re.sub(r'http://\\S{,280}', '', tweet) # filter http\n",
    "    tweet = re.sub(r'https://\\S{,280}', '', tweet) # filter https\n",
    "    tweet = re.sub(r'[^A-Za-z]', ' ', tweet) # filter all non-letters\n",
    "    tweet = re.sub(r'\\s{2,}', ' ', tweet) # remove multiply whitespaces\n",
    "    tweet = re.sub(r'(.)\\1{2,}', r'\\1', tweet) # remove repeated chars (e.g. \"greeeeat\" -> \"great\")\n",
    "    tweet = tweet.strip() # remove possible whitespaces from both sides of the tweet\n",
    "\n",
    "    # lemmatize, tokenize and conquer\n",
    "    processed_tweet = [lemmatizer.lemmatize(token) for token in tokenizer.tokenize(tweet)\n",
    "                       if token not in stop_word_list]\n",
    "    \n",
    "    return processed_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df = filtered_df.map(lambda x: process_tweet(x[0]))\n",
    "processed_df = processed_df.filter(lambda x: len(x) > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.filter(lambda x: x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['twin'],\n",
       " ['tomorrow',\n",
       "  'come',\n",
       "  'join',\n",
       "  'u',\n",
       "  'ice',\n",
       "  'cream',\n",
       "  'swirll',\n",
       "  'cafe',\n",
       "  'located',\n",
       "  'cedar',\n",
       "  'ave',\n",
       "  'facing',\n",
       "  'baltimore',\n",
       "  'ave',\n",
       "  'look',\n",
       "  'forwa'],\n",
       " ['know'],\n",
       " ['greatest', 'story', 'ever', 'told', 'amen'],\n",
       " ['hahaha', 'bet', 'island', 'landed', 'hopping', 'tho'],\n",
       " ['believe', 'stole', 'tweet', 'smh'],\n",
       " ['new', 'starting', 'quarterback'],\n",
       " ['going', 'must', 'come', 'wall', 'talking', 'bout'],\n",
       " ['yep',\n",
       "  'pop',\n",
       "  'culture',\n",
       "  'standpoint',\n",
       "  'recognized',\n",
       "  'depicted',\n",
       "  'planet',\n",
       "  'due',\n",
       "  'ring'],\n",
       " ['protect', 'cost', 'national', 'treasure']]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.map(lambda x: process_tweet(x[0])).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['twin', '\\U0001f974'],\n",
       " ['tomorrow',\n",
       "  'come',\n",
       "  'join',\n",
       "  'u',\n",
       "  'ice',\n",
       "  'cream',\n",
       "  'üç¶',\n",
       "  'swirll',\n",
       "  'cafe',\n",
       "  'located',\n",
       "  'cedar',\n",
       "  'ave',\n",
       "  'facing',\n",
       "  'baltimore',\n",
       "  'ave',\n",
       "  'look',\n",
       "  'forwa'],\n",
       " ['know', 'üôåüèΩ'],\n",
       " ['livingforjc', 'greatest', '‚ù§Ô∏è', 'story', 'ever', 'told', 'amen'],\n",
       " ['pastor', 'coat', 'hahaha', 'bet', 'island', 'landed', 'hopping', 'tho'],\n",
       " ['uzsagi', 'cant', 'believe', 'stole', 'tweet', 'smh'],\n",
       " ['treenkarli', 'new', 'starting', 'quarterback'],\n",
       " ['dtill', 'abc', 'going', 'must', 'come', 'wall', 'talking', 'bout'],\n",
       " ['killswitch',\n",
       "  'yep',\n",
       "  'pop',\n",
       "  'culture',\n",
       "  'standpoint',\n",
       "  'recognized',\n",
       "  'depicted',\n",
       "  'planet',\n",
       "  'due',\n",
       "  'ring'],\n",
       " ['protect', 'loganmania', 'cost', 'national', 'treasure']]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.map(lambda x: process_tweet(x[0])).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Twins \\U0001f974',\n",
       " 'Tomorrow come join us over ice cream üç¶ at Swirll Cafe located at 4610 Cedar Ave facing Baltimore Ave! We look forwa‚Ä¶ https://t.co/h6UiREy5at',\n",
       " 'You know what üôåüèΩ',\n",
       " '@livingforjc The greatest ‚ù§Ô∏è story ever told amen',\n",
       " '@268pastor @_coat Hahaha I bet that island they landed on is hopping now tho',\n",
       " \"@uzsagi I can't believe they stole your tweet smh\",\n",
       " '@TreenKarli the new starting quarterback',\n",
       " '@dtill96 @ABC What going up Must come down same as that wall you talking bout.',\n",
       " '@kill_switch_0 Yep. and just from a pop culture standpoint, it‚Äôs the most recognized and most depicted planet due to its rings',\n",
       " 'Protect @LoganMania at all costs. He is a national treasure. ']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.map(lambda x: x[0]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
