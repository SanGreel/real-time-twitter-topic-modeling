{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem statement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import all needed libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T09:01:13.049621Z",
     "start_time": "2019-07-05T09:01:13.034697Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/dmytro.babenko/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/dmytro.babenko/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# basically spark\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "import operator\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import ArrayType, FloatType, StringType, IntegerType\n",
    "from pyspark.sql.functions import udf, row_number,column\n",
    "\n",
    "# processing\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "# text preprocessing\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from pyspark.ml.feature import CountVectorizer,StopWordsRemover, HashingTF, IDF, Tokenizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "#staff for LDA\n",
    "from pyspark.mllib.clustering import LDA, LDAModel\n",
    "from pyspark.mllib.linalg import Vector as oldVector, Vectors as oldVectors\n",
    "from pyspark.ml.linalg import Vector as newVector, Vectors as newVectors\n",
    "\n",
    "# import hardcoded variables\n",
    "from variables import channels_not_to_consider\n",
    "\n",
    "#for debug purpose only\n",
    "import time\n",
    "\n",
    "#pytrends - for acquiring google trends\n",
    "from get_google_trends_data.pytrends.pytrends.request import TrendReq\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global variables definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**User-specific variables**  \n",
    "Please feel free to tweak those variables as you wish. For example, you can set number of last hours to get hottest topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T09:01:14.157800Z",
     "start_time": "2019-07-05T09:01:14.153771Z"
    }
   },
   "outputs": [],
   "source": [
    "# if True locations from locations_to_consider will be used to filter\n",
    "get_from_location = True\n",
    "\n",
    "# locations to filter relevant tweets\n",
    "locations_to_consider = [\n",
    "                         'Manhattan, NY', \n",
    "                         'Brooklyn, NY', \n",
    "                         'Queens, NY', \n",
    "                         'Bronx, NY', \n",
    "                         'Staten Island, NY'\n",
    "                         'New York, USA'\n",
    "                        ]\n",
    "\n",
    "number_of_hours_to_get_topics = 2\n",
    "num_of_top_interest = 15\n",
    "\n",
    "geo = \"US-NY\" #US for USA\n",
    "\n",
    "# Set window time for interesting\n",
    "frame_start_datetime = \"Sun Jun 27 00:00:00 +0000 2019\"\n",
    "frame_finish_datetime = \"Sun Jun 30 23:00:00 +0000 2019\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Technical variables**  \n",
    "Those variables are needed to connect to db and other technical stuff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T09:01:14.916672Z",
     "start_time": "2019-07-05T09:01:14.913526Z"
    }
   },
   "outputs": [],
   "source": [
    "# LDA params\n",
    "num_of_topics_LDA = 10\n",
    "max_iterations_LDA = 100\n",
    "nomber_of_words_to_for_topic = 15  # number of words per topic\n",
    "\n",
    "# path to CSV\n",
    "historical_tweets_data = './get-tweets-by-geolocation/training_tweets.csv'\n",
    "# MongoDB table\n",
    "real_time_tweets_table = \"usa_training_tweets_02_07.training_tweets_collection\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext()\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handy functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Text preprocessing and filtering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T09:01:16.014092Z",
     "start_time": "2019-07-05T09:01:16.007448Z"
    }
   },
   "outputs": [],
   "source": [
    "def filter_tweet(tweet, channels_not_to_consider):\n",
    "    \n",
    "    if not isinstance(tweet, str):\n",
    "        is_filtered = True\n",
    "    elif len(tweet.split(' ')) < 3:\n",
    "        is_filtered = True\n",
    "    else: \n",
    "        is_filtered = False\n",
    "        \n",
    "    return not is_filtered\n",
    "         \n",
    "def process_tweet(tweet):\n",
    "   \n",
    "    tweet = tweet.lower() # get lowercase\n",
    "    tweet = re.sub(r'@\\w+', '', tweet) # filter words with non-letters at the beginning (mainly for mentions)\n",
    "    tweet = re.sub(r'http://\\S{,280}', '', tweet) # filter http\n",
    "    tweet = re.sub(r'https://\\S{,280}', '', tweet) # filter https\n",
    "    tweet = re.sub(r'[^A-Za-z]', ' ', tweet) # filter all non-letters\n",
    "    tweet = re.sub(r'\\s{2,}', ' ', tweet) # remove multiply whitespaces\n",
    "    tweet = re.sub(r'(.)\\1{2,}', r'\\1', tweet) # remove repeated chars (e.g. \"greeeeat\" -> \"great\")\n",
    "    tweet = tweet.strip() # remove possible whitespaces from both sides of the tweet\n",
    "\n",
    "    # lemmatize, tokenize and conquer\n",
    "    processed_tweet = [lemmatizer.lemmatize(token) for token in tokenizer.tokenize(tweet)\n",
    "                       if token not in stop_word_list]\n",
    "    \n",
    "    return processed_tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Datetime handling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T09:01:16.957560Z",
     "start_time": "2019-07-05T09:01:16.953752Z"
    }
   },
   "outputs": [],
   "source": [
    "def str_tweet_to_datetime(frame_datetime):\n",
    "    ts = datetime.strptime(frame_datetime,'%a %b %d %H:%M:%S %z %Y')\n",
    "    return ts\n",
    "\n",
    "def datetime_to_tweet_str(frame_datetime):\n",
    "    #print(type(frame_datetime))\n",
    "    ts = datetime.strftime(frame_datetime, '%a %b %d %H:%M:%S %z %Y')\n",
    "    return ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to call this block with functions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T09:01:17.863987Z",
     "start_time": "2019-07-05T09:01:17.857988Z"
    }
   },
   "outputs": [],
   "source": [
    "def tweet2google_timeframe(frame_start_datetime, frame_finish_datetime):\n",
    "    start_date = str_tweet_to_datetime(frame_start_datetime)\n",
    "    end_date = str_tweet_to_datetime(frame_finish_datetime)\n",
    "    tim\n",
    "    \n",
    "def get_google_trends_by_geo(geo):\n",
    "    if geo == 'US':\n",
    "        return google_trends_search_topics_us, google_trends_search_queries_us\n",
    "    elif geo == 'US-NY':\n",
    "        return google_trends_search_topics_us_ny, google_trends_search_queries_us_ny\n",
    "    \n",
    "    return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T09:01:18.327252Z",
     "start_time": "2019-07-05T09:01:18.321760Z"
    }
   },
   "outputs": [],
   "source": [
    "#TODO: move this function to utils\n",
    "def str_rising_to_float(str):\n",
    "    if str is None:\n",
    "        return 0.0\n",
    "    if str == '':\n",
    "        return 0.0\n",
    "    if str == 'Breakout':\n",
    "        return 0.0\n",
    "    \n",
    "    str_value = str.split('%')[0]\n",
    "    if '+' in str_value:\n",
    "        str_value = str_value.split('+')[1]\n",
    "        \n",
    "    if ',' in str_value:\n",
    "        str_value = str_value.replace(',', '.')\n",
    "        value = 1000* float(str_value)\n",
    "        return value\n",
    "    return float(str_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T09:01:18.759489Z",
     "start_time": "2019-07-05T09:01:18.745596Z"
    }
   },
   "outputs": [],
   "source": [
    "#TODO: move this function to utils\n",
    "def unique_google_trends_by_time_frame(df):\n",
    "    data = df.collect()\n",
    "    rising_dict = {}\n",
    "    top_dict = {}\n",
    "    \n",
    "    geo = data[0]['geo']\n",
    "    columns = df.columns\n",
    "\n",
    "    for i in range(0, len(data)):\n",
    "        rising_val = data[i][columns[1]]\n",
    "        top_value = data[i][columns[2]]\n",
    "        \n",
    "        if rising_val in rising_dict:\n",
    "            rising_dict[rising_val][0] += str_rising_to_float(data[i][columns[3]])\n",
    "            rising_dict[rising_val][1] += 1\n",
    "        else:\n",
    "            rising_dict[rising_val] = [str_rising_to_float(data[i][columns[3]]), 1]\n",
    "            \n",
    "        if top_value in top_dict:\n",
    "            top_dict[top_value][0] += float(data[i][columns[4]])\n",
    "            top_dict[top_value][1] += 1\n",
    "        else:\n",
    "            top_dict[top_value] = [float(data[i][columns[4]]), 1]\n",
    "    \n",
    "    \n",
    "    for key in top_dict:\n",
    "        top_dict[key] = round(top_dict[key][0] / top_dict[key][1])\n",
    "        \n",
    "    for key in rising_dict:\n",
    "        rising_dict[key] = round(rising_dict[key][0] / rising_dict[key][1])\n",
    "    \n",
    "    top_dict = sorted(top_dict.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    rising_dict = sorted(rising_dict.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    \n",
    "    \n",
    "    seq = []\n",
    "    len_top = len(top_dict)\n",
    "    len_rising = len(rising_dict)\n",
    "    length = max(len_top, len_rising)\n",
    "    \n",
    "    row = Row(columns[1], columns[2], columns[3], columns[4], columns[5])\n",
    "    \n",
    "    for i in range(0, length):\n",
    "        rising = rising_dict[i][0] if i < len_rising else ''\n",
    "        rising_val = f\"+{rising_dict[i][1]}%\" if i < len_rising else None\n",
    "        \n",
    "        top = top_dict[i][0] if i < len_top else ''\n",
    "        top_val = top_dict[i][1] if i < len_top else None\n",
    "        \n",
    "        seq.append(row(rising, top, rising_val, top_val, geo))\n",
    "    \n",
    "    dframe = spark.createDataFrame(seq)\n",
    "    return dframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T09:01:19.292419Z",
     "start_time": "2019-07-05T09:01:19.287625Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_geo_name(geo):\n",
    "    if geo == \"US-NY\":\n",
    "        return \"New York\"\n",
    "    elif geo == \"US\":\n",
    "        return \"United States\"\n",
    "    return \"\"\n",
    "\n",
    "def print_google_trend_title(start_date, finish_date, name):\n",
    "    start_date_str = start_date.strftime(\"%Y-%m-%d\")\n",
    "    if start_date == finish_date:\n",
    "        print(f\"\\nGoogle trends {name} in {get_geo_name(geo)} during {start_date_str}\")\n",
    "    else:\n",
    "        finish_date_str = finish_date.strftime(\"%Y-%m-%d\")\n",
    "        print(f\"\\nGoogle trends {name} in {get_geo_name(geo)} during {start_date_str} - {finish_date_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_datetime_in_interesting_google(df):\n",
    "    columns = df.columns\n",
    "    converted_df = df.rdd.map(lambda x : (\n",
    "                                          x[\"Date\"].strftime(\"%Y-%m-%d\"), \n",
    "                                          x[columns[1]], \n",
    "                                          x[columns[2]], \n",
    "                                          x[columns[3]],\n",
    "                                          x[columns[4]],\n",
    "                                          x[columns[5]])).toDF([columns[0], columns[1], columns[2], columns[3], columns[4], columns[5]])\n",
    "                                                \n",
    "    return converted_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here should be \"magic IF\" (Yevhen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T09:01:28.065311Z",
     "start_time": "2019-07-05T09:01:28.052941Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_history_and_real_timeframe(requested_start, requested_finish):\n",
    "\n",
    "    requested_start_dt = str_tweet_to_datetime(requested_start)\n",
    "    requested_finish_dt = str_tweet_to_datetime(requested_finish)\n",
    "    \n",
    "    const_end_history_datetime = str_tweet_to_datetime(\"Fri Jul 05 00:00:00 +0000 2019\")\n",
    "\n",
    "    history_start_datetime = None\n",
    "    history_finish_datetime = None\n",
    "    realtime_start_datetime = None\n",
    "    realtime_finish_datetime = None\n",
    "\n",
    "    assert requested_finish_dt > requested_start_dt, \"Finish dataframe MUST be greater than start\"\n",
    "\n",
    "    if (requested_start_dt >= const_end_history_datetime and requested_finish_dt > const_end_history_datetime):\n",
    "        realtime_start_datetime = requested_start_dt\n",
    "        realtime_finish_datetime = requested_finish_dt\n",
    "    elif (requested_start_dt < const_end_history_datetime and requested_finish_dt <= const_end_history_datetime):\n",
    "        history_start_datetime = requested_start_dt\n",
    "        history_finish_datetime = requested_finish_dt\n",
    "    else:\n",
    "        history_start_datetime = requested_start_dt\n",
    "        history_finish_datetime = const_end_history_datetime\n",
    "        realtime_start_datetime = const_end_history_datetime\n",
    "        realtime_finish_datetime = requested_finish_dt\n",
    "        \n",
    "    return (history_start_datetime, history_finish_datetime, realtime_start_datetime, realtime_finish_datetime)\n",
    "\n",
    "print('Example of usage!')\n",
    "times = get_history_and_real_timeframe(requested_start = frame_start_datetime, \n",
    "                                       requested_finish = frame_finish_datetime)\n",
    "\n",
    "print(\"Range for csv: \", times[0], times[1])\n",
    "print(\"Time range for mongodb: \", times[2], times[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T09:01:21.515656Z",
     "start_time": "2019-07-05T09:01:21.458082Z"
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.csv(historical_tweets_data, inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting to MongoDB for the real-time data\n",
    "**Creating Spark connector**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"pipeline\") \\\n",
    "    .config('spark.mongodb.input.uri', 'mongodb://localhost:27017/'+real_time_tweets_table) \\\n",
    "    .config('spark.mongodb.output.uri', 'mongodb://localhost:27017/'+real_time_tweets_table) \\\n",
    "    .config('spark.jars.packages', 'org.mongodb.spark:mongo-spark-connector_2.11:2.3.1') \\\n",
    "    .config('spark.mongodb.input.partitioner', 'MongoPaginateBySizePartitioner') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"recent_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recent_data = spark.sql('SELECT text FROM recent_data WHERE lang=\"en\" AND place.country=\"US\" LIMIT 10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recent_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(historical_tweets_data, header=True, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.WordPunctTokenizer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_word_list = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter nans\n",
    "df = df.rdd.filter(lambda x: x[0] != None and x[1] != None and x[2] != None and x[4] != None)\n",
    "\n",
    "# filter out channels not to consider\n",
    "df = df.filter(lambda x: x[4] not in channels_not_to_consider)\n",
    "\n",
    "# filter by country\n",
    "df = df.filter(lambda x: x[1] in 'US')\n",
    "\n",
    "# filter by precise location\n",
    "if get_from_location:\n",
    "    df = df.filter(lambda x: x[2] in locations_to_consider)\n",
    "\n",
    "# filter tweet itself\n",
    "df = df.filter(lambda x: filter_tweet(x[0]))\n",
    "\n",
    "# process tweet\n",
    "df = df.map(lambda x: process_tweet(x[0]))\n",
    "\n",
    "# final preprocesssing\n",
    "df = df.filter(lambda x: len(x) > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here should be Main dataframe already filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Google Trends data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_trends_search_queries_us = spark.read.csv('data/google-trends/google-trends-search-queries-US.csv', inferSchema=True, header=True)\n",
    "google_trends_search_topics_us = spark.read.csv('data/google-trends/google-trends-search-topics-US.csv', inferSchema=True, header=True)\n",
    "google_trends_search_queries_us_ny = spark.read.csv('data/google-trends/google-trends-search-queries-US-NY.csv', inferSchema=True, header=True)\n",
    "google_trends_search_topics_us_ny = spark.read.csv('data/google-trends/google-trends-search-topics-US-NY.csv', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: move this function to Handy function block \n",
    "def get_google_trends_by_geo(geo):\n",
    "    if geo == 'US':\n",
    "        return google_trends_search_topics_us, google_trends_search_queries_us\n",
    "    elif geo == 'US-NY':\n",
    "        return google_trends_search_topics_us_ny, google_trends_search_queries_us_ny\n",
    "    \n",
    "    return None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data description (Женя)\n",
    "In order to collect data in a natural way:\n",
    "<br>- we registered Twitter Developer account;\n",
    "<br>- using credentials from Twitter Developer account we run script that collected tweets by the geolocation and saved them in mongodb;\n",
    "<br>\n",
    "<br><b>As a result:</b>\n",
    "<br>- we collected  332548 tweets (10Gb in mongodb, ~100Mb in csv) from New-York geolocation since 30 of May up to 15 of June;\n",
    "<br>- we collected  6617029 tweets (~1.69Gb in csv) from USA geolocation since 15 of June up to now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show Raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing\n",
    "# description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "variables": {
     " frame_start_datetime ": {},
     "-frame_start_datetime-": {},
     "frame_finish_datetime": {}
    }
   },
   "source": [
    "#### Selecting data from {{ frame_start_datetime }} to {{frame_finish_datetime}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "StopWords = stopwords.words(\"english\")\n",
    "\n",
    "tokens = df.select('text').rdd \\\n",
    "    .map(lambda x: x[0]) \\\n",
    "    .map( lambda document: re.split(\" \", document))          \\\n",
    "    .map( lambda word: [x for x in word if x.isalpha()])           \\\n",
    "    .map( lambda word: [x for x in word if len(x) > 3] )           \\\n",
    "    .map( lambda word: [x for x in word if x not in StopWords])    \\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show preprocessed data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic modeling/Latent Dirichlet allocation(LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this block can be commented, it's just a mock\n",
    "\n",
    "text_file = 'data/listings.csv'\n",
    "df = spark.read.csv(text_file, inferSchema=True, header=True)\n",
    "df = df.select(\"id\", \"name\").dropna(subset=\"name\")\n",
    "\n",
    "print(time.strftime('%m%d%Y %H:%M:%S'))\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"name\", outputCol=\"tokens\")\n",
    "df = tokenizer.transform(df)\n",
    "print(time.strftime('%m%d%Y %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(time.strftime('%m%d%Y %H:%M:%S'))\n",
    "\n",
    "cv = CountVectorizer(inputCol=\"tokens\", outputCol=\"raw_features\", vocabSize=5000, minDF=3.0)\n",
    "cvmodel = cv.fit(df)\n",
    "\n",
    "print(time.strftime('%m%d%Y %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(time.strftime('%m%d%Y %H:%M:%S'))\n",
    "df = cvmodel.transform(df)\n",
    "print(time.strftime('%m%d%Y %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"tf_idf_features\", minDocFreq=2)\n",
    "idfModel = idf.fit(df)\n",
    "\n",
    "df = idfModel.transform(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(10, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.drop(\"name\")\n",
    "#df.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = Window().orderBy(column(\"id\"))\n",
    "df = df.withColumn(\"id\", row_number().over(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = df.rdd.map(lambda x: (x[0], oldVectors.fromML(x[4])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_df = rs.toDF()\n",
    "rs_df.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the LDA Topic Modeler\n",
    "# Note the time before and after is printed in order to find out how much time it takes to process x number of records\n",
    "\n",
    "print(time.strftime('%m%d%Y %H:%M:%S'))\n",
    "lda_model = LDA.train(rs_df['_1', '_2'].rdd.map(list), k=num_of_topics_LDA, maxIterations=max_iterations_LDA)\n",
    "print(time.strftime('%m%d%Y %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(time.strftime('%m%d%Y %H:%M:%S'))\n",
    "topics = lda_model.topicsMatrix()\n",
    "vocabArray = cvmodel.vocabulary\n",
    "\n",
    "topicIndices = sc.parallelize(lda_model.describeTopics(maxTermsPerTopic = wordNumbers))\n",
    "\n",
    "def topic_render(topic):  # specify vector id of words to actual words\n",
    "    terms = topic[0]\n",
    "    prob = topic[1]\n",
    "    \n",
    "    result = []\n",
    "    for i in range(nomber_of_words_to_for_topic):\n",
    "        term = str(round(prob[i],3))+\"  \"+vocabArray[terms[i]]\n",
    "        result.append(term)\n",
    "    return result\n",
    "print(time.strftime('%m%d%Y %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(time.strftime('%m%d%Y %H:%M:%S'))\n",
    "topics_final = topicIndices.map(lambda topic:topic_render(topic)).collect()\n",
    "print(time.strftime('%m%d%Y %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# based on the simple vectors(+number of words)\n",
    "\n",
    "for topic in range(len(topics_final)):\n",
    "    print (\"Topic #\" + str(topic+1) + \"\")\n",
    "    for term in topics_final[topic]:\n",
    "        print (term)\n",
    "    print ('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#based on the tf-idf\n",
    "\n",
    "for topic in range(len(topics_final)):\n",
    "    print (\"Topic #\" + str(topic+1) + \"\")\n",
    "    for term in topics_final[topic]:\n",
    "        print (term)\n",
    "    print ('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hot topics in the USA from [Google trends](https://trends.google.com/trends/explore?geo=US)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = str_tweet_to_datetime(frame_start_datetime)\n",
    "finish_date = str_tweet_to_datetime(frame_finish_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_trends_topics, google_trends_queries = get_google_trends_by_geo(geo) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Google trends search queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "interesting_google_topics = google_trends_topics.filter(\n",
    "    (google_trends_topics.Date >= start_date) & (google_trends_topics.Date <= finish_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Google trends Search topics in New York during 2019-06-27 - 2019-06-30\n",
      "+----------+--------------------------------------------------------+----------------------------------------+\n",
      "|Date      |Search topics - rising                                  |Search topics - top                     |\n",
      "+----------+--------------------------------------------------------+----------------------------------------+\n",
      "|2019-06-28|Kamala Harris - United States Senator                   |New York - City in New York             |\n",
      "|2019-06-28|Marianne Williamson - American author                   |New York - US State                     |\n",
      "|2019-06-28|United States women's national soccer team - Soccer team|2019 - Topic                            |\n",
      "|2019-06-28|Pete Buttigieg - Mayor of South Bend                    |Weather - Topic                         |\n",
      "|2019-06-28|Joe Biden - Former Vice President of the United States  |Google Search - Topic                   |\n",
      "|2019-06-28|FIFA Women's World Cup - Football competition           |YouTube - Video sharing company         |\n",
      "|2019-06-28|World Cup - Football competition                        |Google - Technology company             |\n",
      "|2019-06-28|2019 FIFA Women's World Cup - Event                     |Facebook - Social networking service    |\n",
      "|2019-06-28|France - Country in Europe                              |Facebook, Inc. - Social network company |\n",
      "|2019-06-28|Colombia - Country in South America                     |United States - Country in North America|\n",
      "|2019-06-28|Copa América - Football championship                    |Film - Topic                            |\n",
      "|2019-06-28|2019 Copa América - Tournament                          |Restaurant - Topic                      |\n",
      "|2019-06-28|Brazil - Country in South America                       |Amazon.com - E-commerce company         |\n",
      "|2019-06-28|Football - Sport                                        |Brooklyn - New York City borough        |\n",
      "|2019-06-28|Pride parade - Topic                                    |Car - Transportation mode               |\n",
      "+----------+--------------------------------------------------------+----------------------------------------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_google_trend_title(start_date, finish_date, \"Search topics\")\n",
    "interest_google_topics = convert_datetime_in_interesting_google(interesting_google_topics)\n",
    "interest_google_topics.select(\"Date\",\"Search topics - rising\", \"Search topics - top\").show(num_of_top_interest, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case when timeframe is more than 1 day, filter correctly this google-trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Google trends Search topics in New York during 2019-06-27 - 2019-06-30\n",
      "+--------------------------------------------------------+----------------------------------------+\n",
      "|Search topics - rising                                  |Search topics - top                     |\n",
      "+--------------------------------------------------------+----------------------------------------+\n",
      "|Kamala Harris - United States Senator                   |New York - City in New York             |\n",
      "|Marianne Williamson - American author                   |New York - US State                     |\n",
      "|Brooklyn Nets - Basketball team                         |2019 - Topic                            |\n",
      "|United States women's national soccer team - Soccer team|Weather - Topic                         |\n",
      "|Pete Buttigieg - Mayor of South Bend                    |YouTube - Video sharing company         |\n",
      "|Joe Biden - Former Vice President of the United States  |Film - Topic                            |\n",
      "|Kevin Durant - American basketball player               |Google Search - Topic                   |\n",
      "|Boston Red Sox - Baseball team                          |Google - Technology company             |\n",
      "|Radar - Topic                                           |Facebook - Social networking service    |\n",
      "|Free agent - Topic                                      |Facebook, Inc. - Social network company |\n",
      "|FIFA Women's World Cup - Football competition           |United States - Country in North America|\n",
      "|World Cup - Football competition                        |Restaurant - Topic                      |\n",
      "|2019 FIFA Women's World Cup - Event                     |The Weather Channel - Television channel|\n",
      "|France - Country in Europe                              |Amazon.com - E-commerce company         |\n",
      "|Colombia - Country in South America                     |Brooklyn - New York City borough        |\n",
      "+--------------------------------------------------------+----------------------------------------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "interesing_google_topics_unique= unique_google_trends_by_time_frame(interesting_google_topics)\n",
    "print_google_trend_title(start_date, finish_date, \"Search topics\")\n",
    "interesing_google_topics_unique.select(\"Search topics - rising\", \"Search topics - top\").show(num_of_top_interest, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Google trends search queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "interesting_google_queries = google_trends_queries.filter(\n",
    "    (google_trends_queries.Date >= start_date) & (google_trends_queries.Date <= finish_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Google trends Search queries in New York during 2019-06-27 - 2019-06-30\n",
      "+----------+---------------------------+--------------------+\n",
      "|Date      |Search queries - rising    |Search queries - top|\n",
      "+----------+---------------------------+--------------------+\n",
      "|2019-06-28|shay mitchell              |weather             |\n",
      "|2019-06-28|marianne williamson        |google              |\n",
      "|2019-06-28|kamala harris              |facebook            |\n",
      "|2019-06-28|argentina vs venezuela 2019|youtube             |\n",
      "|2019-06-28|usa france                 |world cup           |\n",
      "|2019-06-28|brazil vs paraguay         |news                |\n",
      "|2019-06-28|usa vs france              |amazon              |\n",
      "|2019-06-28|colombia vs chile          |copa america        |\n",
      "|2019-06-28|alex morgan                |debate              |\n",
      "|2019-06-28|michael bennet             |instagram           |\n",
      "|2019-06-28|argentina vs venezuela     |craigslist          |\n",
      "|2019-06-28|megan rapinoe              |walmart             |\n",
      "|2019-06-28|argentina                  |yahoo               |\n",
      "|2019-06-28|eric swalwell              |map                 |\n",
      "|2019-06-28|andrew yang                |lottery             |\n",
      "+----------+---------------------------+--------------------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_google_trend_title(start_date, finish_date, \"Search queries\")\n",
    "interest_google_queries = convert_datetime_in_interesting_google(interesting_google_queries)\n",
    "interest_google_queries.select(\"Date\", \"Search queries - rising\", \"Search queries - top\").show(num_of_top_interest, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Google trends Search queries in New York during 2019-06-27 - 2019-06-30\n",
      "+---------------------------+--------------------+------+---+-----+\n",
      "|Search queries - rising    |Search queries - top|Rising|Top|geo  |\n",
      "+---------------------------+--------------------+------+---+-----+\n",
      "|yy                         |weather             |+4950%|100|US-NY|\n",
      "|shay mitchell              |pride               |+3700%|62 |US-NY|\n",
      "|darren collison            |facebook            |+2950%|59 |US-NY|\n",
      "|marianne williamson        |google              |+2750%|55 |US-NY|\n",
      "|kamala harris              |youtube             |+2400%|48 |US-NY|\n",
      "|deandre jordan             |news                |+2400%|44 |US-NY|\n",
      "|india vs england           |amazon              |+1050%|44 |US-NY|\n",
      "|argentina vs venezuela 2019|world cup           |+900% |42 |US-NY|\n",
      "|usa france                 |debate              |+900% |35 |US-NY|\n",
      "|brazil vs paraguay         |yankees             |+900% |34 |US-NY|\n",
      "|usa vs france              |pride parade        |+900% |30 |US-NY|\n",
      "|colombia vs chile          |yahoo               |+800% |28 |US-NY|\n",
      "|kevin durant               |nba                 |+800% |27 |US-NY|\n",
      "|alex morgan                |copa america        |+750% |26 |US-NY|\n",
      "|michael bennet             |instagram           |+700% |26 |US-NY|\n",
      "+---------------------------+--------------------+------+---+-----+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "interesing_google_queries_unique= unique_google_trends_by_time_frame(interesting_google_queries)\n",
    "print_google_trend_title(start_date, finish_date, \"Search queries\")\n",
    "interesing_google_queries_unique.show(num_of_top_interest, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hot topics - google trends (directly) (probably this will be removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date_str = start_date.strftime(\"%Y-%m-%d\")\n",
    "finish_date_str = finish_date.strftime(\"%Y-%m-%d\")\n",
    "pytrend = TrendReq()\n",
    "pytrend.build_payload(kw_list=[' '], geo=geo, timeframe=f\"{start_date_str} {finish_date_str}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Search topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_df = pytrend.related_top_search_topics(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Google trends Search topics in New York during 2019-06-27 - 2019-06-30\n",
      "+-------------------------------------------------------------+---------------------------------------+\n",
      "|Search topics - rising                                       |Search topics - top                    |\n",
      "+-------------------------------------------------------------+---------------------------------------+\n",
      "|Pride parade - Topic                                         |New York - City in New York            |\n",
      "|Parade - Topic                                               |New York - US State                    |\n",
      "|Gay pride - Topic                                            |2019 - Topic                           |\n",
      "|Debate - Topic                                               |Weather - Topic                        |\n",
      "|2016 Democratic Party presidential debates and forums - Topic|YouTube - Video sharing company        |\n",
      "|Fireworks - Topic                                            |Google - Technology company            |\n",
      "|London - Capital of England                                  |Google Search - Topic                  |\n",
      "|Software - Software grouping                                 |Facebook, Inc. - Social network company|\n",
      "|New York Knicks - Basketball team                            |Film - Topic                           |\n",
      "|Independence Day - United States                             |Facebook - Social networking service   |\n",
      "|France - Country in Europe                                   |Hotel - Building function              |\n",
      "|Management - Topic                                           |Restaurant - Topic                     |\n",
      "|Nike - Footwear manufacturing company                        |Amazon.com - E-commerce company        |\n",
      "|nan                                                          |Brooklyn - New York City borough       |\n",
      "|nan                                                          |Car - Transportation mode              |\n",
      "+-------------------------------------------------------------+---------------------------------------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_google_trend_title(start_date, finish_date, \"Search topics\")\n",
    "topics_df.select(\"Search topics - rising\", \"Search topics - top\").show(num_of_top_interest, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Search queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_df = pytrend.related_top_search_queries(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Google trends Search queries in New York during 2019-06-27 - 2019-06-30\n",
      "+-------------------------+--------------------+-------+---+-----+\n",
      "|Search queries - rising  |Search queries - top|Rising |Top|geo  |\n",
      "+-------------------------+--------------------+-------+---+-----+\n",
      "|marianne williamson      |weather             |+1,500%|100|US-NY|\n",
      "|kamala harris            |facebook            |+1,450%|61 |US-NY|\n",
      "|yankees vs red sox       |google              |+1,200%|60 |US-NY|\n",
      "|yy                       |youtube             |+1,150%|52 |US-NY|\n",
      "|tulsi gabbard            |amazon              |+1,050%|46 |US-NY|\n",
      "|mexico vs costa rica     |news                |+950%  |46 |US-NY|\n",
      "|kemba walker             |world cup           |+850%  |38 |US-NY|\n",
      "|usa vs france            |craigslist          |+850%  |27 |US-NY|\n",
      "|mackenzie lueck          |instagram           |+500%  |27 |US-NY|\n",
      "|pride parade 2019 nyc    |yankees             |+450%  |25 |US-NY|\n",
      "|pride parade             |movies              |+400%  |24 |US-NY|\n",
      "|gay pride parade nyc 2019|walmart             |+400%  |23 |US-NY|\n",
      "|andrew yang              |mlb                 |+350%  |20 |US-NY|\n",
      "|bobby debarge            |copa america        |+350%  |19 |US-NY|\n",
      "|stonewall inn            |home depot          |+300%  |18 |US-NY|\n",
      "+-------------------------+--------------------+-------+---+-----+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_google_trend_title(start_date, finish_date, \"Search queries\")\n",
    "queries_df.show(num_of_top_interest, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
