{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data description\n",
    "In order to collect data in a natural way:\n",
    "<br>- we registered Twitter Developer account;\n",
    "<br>- using credentials from Twitter Developer account we run script that collected tweets by the geolocation and saved them in mongodb;\n",
    "<br>\n",
    "<br><b>As a result:</b>\n",
    "<br>- we collected  332548 tweets (10Gb in mongodb, ~100Mb in csv) from New-York geolocation since 30 of May up to 15 of June;\n",
    "<br>- we collected  6617029 tweets (~1.69Gb in csv) from USA geolocation since 15 of June up to now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import all needed libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T13:34:48.681195Z",
     "start_time": "2019-07-05T13:34:48.670618Z"
    }
   },
   "outputs": [],
   "source": [
    "# Do not move\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T13:34:49.159490Z",
     "start_time": "2019-07-05T13:34:49.142765Z"
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import operator\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import ArrayType, FloatType, StringType, IntegerType, StructField, StructType\n",
    "from pyspark.sql.functions import udf, row_number,column\n",
    "\n",
    "# processing\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "# text preprocessing\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from pyspark.ml.feature import CountVectorizer,StopWordsRemover, HashingTF, IDF, Tokenizer\n",
    "\n",
    "#staff for LDA\n",
    "from pyspark.mllib.clustering import LDA, LDAModel\n",
    "from pyspark.mllib.linalg import Vector as oldVector, Vectors as oldVectors\n",
    "from pyspark.ml.linalg import Vector as newVector, Vectors as newVectors\n",
    "\n",
    "# import hardcoded variables\n",
    "from variables import channels_not_to_consider\n",
    "\n",
    "#for debug purpose only\n",
    "import time\n",
    "\n",
    "#pytrends - for acquiring google trends\n",
    "from get_google_trends_data.pytrends.pytrends.request import TrendReq\n",
    "\n",
    "# basically spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datetype functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T13:36:32.794382Z",
     "start_time": "2019-07-05T13:36:32.789149Z"
    }
   },
   "outputs": [],
   "source": [
    "wrong_date = datetime.strptime(\"Mon Jun 03 00:00:00 +0000 2000\", '%a %b %d %H:%M:%S %z %Y')\n",
    "\n",
    "def validate(date_text):\n",
    "    try:\n",
    "        if date_text != datetime.strptime(date_text, '%a %b %d %H:%M:%S %z %Y').strftime('%a %b %d %H:%M:%S %z %Y'):\n",
    "            raise ValueError\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "def str_tweet_to_datetime(frame_datetime):\n",
    "    if (validate(frame_datetime) == True):\n",
    "        return datetime.strptime(frame_datetime,'%a %b %d %H:%M:%S %z %Y')\n",
    "    else:\n",
    "        return wrong_date\n",
    "\n",
    "def datetime_to_tweet_str(frame_datetime):\n",
    "    #print(type(frame_datetime))\n",
    "    ts = datetime.strftime(frame_datetime, '%a %b %d %H:%M:%S %z %Y')\n",
    "    return ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T13:36:51.284907Z",
     "start_time": "2019-07-05T13:36:51.276749Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_history_and_real_timeframe(requested_start_dt, requested_finish_dt):\n",
    "    \n",
    "    const_end_history_datetime = str_tweet_to_datetime(\"Fri Jul 05 00:00:00 +0000 2019\")\n",
    "\n",
    "    history_start_datetime = None\n",
    "    history_finish_datetime = None\n",
    "    realtime_start_datetime = None\n",
    "    realtime_finish_datetime = None\n",
    "\n",
    "    assert requested_finish_dt > requested_start_dt, \"Finish dataframe MUST be greater than start\"\n",
    "\n",
    "    if (requested_start_dt >= const_end_history_datetime and requested_finish_dt > const_end_history_datetime):\n",
    "        realtime_start_datetime = requested_start_dt\n",
    "        realtime_finish_datetime = requested_finish_dt\n",
    "    elif (requested_start_dt < const_end_history_datetime and requested_finish_dt <= const_end_history_datetime):\n",
    "        history_start_datetime = requested_start_dt\n",
    "        history_finish_datetime = requested_finish_dt\n",
    "    else:\n",
    "        history_start_datetime = requested_start_dt\n",
    "        history_finish_datetime = const_end_history_datetime\n",
    "        realtime_start_datetime = const_end_history_datetime\n",
    "        realtime_finish_datetime = requested_finish_dt\n",
    "        \n",
    "    return (history_start_datetime, history_finish_datetime, realtime_start_datetime, realtime_finish_datetime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global variables definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final of league championship \n",
    "lc_final_start_datetime = \"Sat Jun 01 00:00:00 +0000 2019\"\n",
    "lc_finish_finish_datetime = \"Sat Jun 01 23:59:59 +0000 2019\"\n",
    "\n",
    "#Stanley cup final\n",
    "stanley_final_start_datetime = \"Wed Jun 12 00:00:00 +0000 2019\"\n",
    "stanley_finish_finish_datetime = \"Wed Jun 12 23:59:59 +0000 2019\"\n",
    "\n",
    "#Draft NBA\n",
    "nba_final_start_datetime = \"Thu Jun 20 00:00:00 +0000 2019\"\n",
    "nba_finish_finish_datetime = \"Sun Jun 23 23:59:59 +0000 2019\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**User-specific variables**  \n",
    "Please feel free to tweak those variables as you wish. For example, you can set number of last hours to get hottest topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T13:41:05.605577Z",
     "start_time": "2019-07-05T13:41:05.601192Z"
    }
   },
   "outputs": [],
   "source": [
    "# if True locations from locations_to_consider will be used to filter\n",
    "get_from_location = True\n",
    "\n",
    "# locations to filter relevant tweets\n",
    "locations_to_consider = [\n",
    "                         'Manhattan, NY', \n",
    "                         'Brooklyn, NY', \n",
    "                         'Queens, NY', \n",
    "                         'Bronx, NY', \n",
    "                         'Staten Island, NY'\n",
    "                         'New York, USA'\n",
    "                        ]\n",
    "\n",
    "geo = \"US-NY\" #US for USA\n",
    "\n",
    "number_of_hours_to_get_topics = 2\n",
    "num_of_top_interest = 15\n",
    "\n",
    "# Set window time for interesting\n",
    "frame_start_datetime = str_tweet_to_datetime(stanley_final_start_datetime)\n",
    "frame_finish_datetime = str_tweet_to_datetime(stanley_finish_finish_datetime)\n",
    "\n",
    "assert (frame_finish_datetime - frame_start_datetime).days <= 3, \"Date interval should not be bigger than 3 days\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Technical variables**  \n",
    "Those variables are needed to connect to db and other technical stuff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T13:38:36.620763Z",
     "start_time": "2019-07-05T13:38:36.617592Z"
    }
   },
   "outputs": [],
   "source": [
    "# LDA params\n",
    "num_of_topics_LDA = 10\n",
    "max_iterations_LDA = 100\n",
    "nomber_of_words_to_for_topic = 15  # number of words per topic\n",
    "\n",
    "# path to CSV\n",
    "historical_tweets_data = './get-tweets-by-geolocation/data/new_york_training_tweets_15_06.csv'\n",
    "# historical_tweets_data = './get-tweets-by-geolocation/training_tweets.csv'\n",
    "# MongoDB table\n",
    "real_time_tweets_table = \"usa_training_tweets_04_07.training_tweets_collection\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T13:38:37.435833Z",
     "start_time": "2019-07-05T13:38:37.427311Z"
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"pipeline\") \\\n",
    "    .config('spark.mongodb.input.uri', 'mongodb://localhost:27017/'+real_time_tweets_table) \\\n",
    "    .config('spark.mongodb.output.uri', 'mongodb://localhost:27017/'+real_time_tweets_table) \\\n",
    "    .config('spark.jars.packages', 'org.mongodb.spark:mongo-spark-connector_2.11:2.3.1') \\\n",
    "    .config('spark.mongodb.input.partitioner', 'MongoPaginateBySizePartitioner') \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handy functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Text preprocessing and filtering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T13:38:37.931399Z",
     "start_time": "2019-07-05T13:38:37.917377Z"
    }
   },
   "outputs": [],
   "source": [
    "def filter_tweet(tweet, channels_not_to_consider):\n",
    "    \n",
    "    if not isinstance(tweet, str):\n",
    "        is_filtered = True\n",
    "    elif len(tweet.split(' ')) < 3:\n",
    "        is_filtered = True\n",
    "    else: \n",
    "        is_filtered = False\n",
    "        \n",
    "    return not is_filtered\n",
    "         \n",
    "def process_tweet(tweet):\n",
    "   \n",
    "    tweet = tweet.lower() # get lowercase\n",
    "    tweet = re.sub(r'@\\w+', '', tweet) # filter words with non-letters at the beginning (mainly for mentions)\n",
    "    tweet = re.sub(r'http://\\S{,280}', '', tweet) # filter http\n",
    "    tweet = re.sub(r'https://\\S{,280}', '', tweet) # filter https\n",
    "    tweet = re.sub(r'[^A-Za-z]', ' ', tweet) # filter all non-letters\n",
    "    tweet = re.sub(r'\\s{2,}', ' ', tweet) # remove multiply whitespaces\n",
    "    tweet = re.sub(r'(.)\\1{2,}', r'\\1', tweet) # remove repeated chars (e.g. \"greeeeat\" -> \"great\")\n",
    "    tweet = tweet.strip() # remove possible whitespaces from both sides of the tweet\n",
    "\n",
    "    # lemmatize, tokenize and conquer\n",
    "    processed_tweet = [lemmatizer.lemmatize(token) for token in tokenizer.tokenize(tweet)\n",
    "                       if token not in stop_word_list]\n",
    "    \n",
    "    return processed_tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Google trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T13:38:38.372038Z",
     "start_time": "2019-07-05T13:38:38.368658Z"
    }
   },
   "outputs": [],
   "source": [
    "#TODO: move this function to Handy function block \n",
    "def get_google_trends_by_geo(geo):\n",
    "    if geo == 'US':\n",
    "        return google_trends_search_topics_us, google_trends_search_queries_us\n",
    "    elif geo == 'US-NY':\n",
    "        return google_trends_search_topics_us_ny, google_trends_search_queries_us_ny\n",
    "    \n",
    "    return None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to call this block with functions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T13:38:38.953666Z",
     "start_time": "2019-07-05T13:38:38.949385Z"
    }
   },
   "outputs": [],
   "source": [
    "def tweet2google_timeframe(frame_start_datetime, frame_finish_datetime):\n",
    "    start_date = str_tweet_to_datetime(frame_start_datetime)\n",
    "    end_date = str_tweet_to_datetime(frame_finish_datetime)\n",
    "    tim\n",
    "    \n",
    "def get_google_trends_by_geo(geo):\n",
    "    if geo == 'US':\n",
    "        return google_trends_search_topics_us, google_trends_search_queries_us\n",
    "    elif geo == 'US-NY':\n",
    "        return google_trends_search_topics_us_ny, google_trends_search_queries_us_ny\n",
    "    \n",
    "    return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T13:38:39.324015Z",
     "start_time": "2019-07-05T13:38:39.319039Z"
    }
   },
   "outputs": [],
   "source": [
    "#TODO: move this function to utils\n",
    "def str_rising_to_float(str):\n",
    "    if str is None:\n",
    "        return 0.0\n",
    "    if str == '':\n",
    "        return 0.0\n",
    "    if str == 'Breakout':\n",
    "        return 0.0\n",
    "    \n",
    "    str_value = str.split('%')[0]\n",
    "    if '+' in str_value:\n",
    "        str_value = str_value.split('+')[1]\n",
    "        \n",
    "    if ',' in str_value:\n",
    "        str_value = str_value.replace(',', '.')\n",
    "        value = 1000* float(str_value)\n",
    "        return value\n",
    "    return float(str_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T13:38:39.747760Z",
     "start_time": "2019-07-05T13:38:39.733886Z"
    }
   },
   "outputs": [],
   "source": [
    "#TODO: move this function to utils\n",
    "def unique_google_trends_by_time_frame(df):\n",
    "    data = df.collect()\n",
    "    rising_dict = {}\n",
    "    top_dict = {}\n",
    "    \n",
    "    geo = data[0]['geo']\n",
    "    columns = df.columns\n",
    "\n",
    "    for i in range(0, len(data)):\n",
    "        rising_val = data[i][columns[1]]\n",
    "        top_value = data[i][columns[2]]\n",
    "        \n",
    "        if rising_val in rising_dict:\n",
    "            rising_dict[rising_val][0] += str_rising_to_float(data[i][columns[3]])\n",
    "            rising_dict[rising_val][1] += 1\n",
    "        else:\n",
    "            rising_dict[rising_val] = [str_rising_to_float(data[i][columns[3]]), 1]\n",
    "            \n",
    "        if top_value in top_dict:\n",
    "            top_dict[top_value][0] += float(data[i][columns[4]])\n",
    "            top_dict[top_value][1] += 1\n",
    "        else:\n",
    "            top_dict[top_value] = [float(data[i][columns[4]]), 1]\n",
    "    \n",
    "    \n",
    "    for key in top_dict:\n",
    "        top_dict[key] = round(top_dict[key][0] / top_dict[key][1])\n",
    "        \n",
    "    for key in rising_dict:\n",
    "        rising_dict[key] = round(rising_dict[key][0] / rising_dict[key][1])\n",
    "    \n",
    "    top_dict = sorted(top_dict.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    rising_dict = sorted(rising_dict.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    \n",
    "    \n",
    "    seq = []\n",
    "    len_top = len(top_dict)\n",
    "    len_rising = len(rising_dict)\n",
    "    length = max(len_top, len_rising)\n",
    "    \n",
    "    row = Row(columns[1], columns[2], columns[3], columns[4], columns[5])\n",
    "    \n",
    "    for i in range(0, length):\n",
    "        rising = rising_dict[i][0] if i < len_rising else ''\n",
    "        rising_val = f\"+{rising_dict[i][1]}%\" if i < len_rising else None\n",
    "        \n",
    "        top = top_dict[i][0] if i < len_top else ''\n",
    "        top_val = top_dict[i][1] if i < len_top else None\n",
    "        \n",
    "        seq.append(row(rising, top, rising_val, top_val, geo))\n",
    "    \n",
    "    dframe = spark.createDataFrame(seq)\n",
    "    return dframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T13:38:40.258348Z",
     "start_time": "2019-07-05T13:38:40.253386Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_geo_name(geo):\n",
    "    if geo == \"US-NY\":\n",
    "        return \"New York\"\n",
    "    elif geo == \"US\":\n",
    "        return \"United States\"\n",
    "    return \"\"\n",
    "\n",
    "def print_google_trend_title(start_date, finish_date, name):\n",
    "    start_date_str = start_date.strftime(\"%Y-%m-%d\")\n",
    "    if start_date == finish_date:\n",
    "        print(f\"\\nGoogle trends {name} in {get_geo_name(geo)} during {start_date_str}\")\n",
    "    else:\n",
    "        finish_date_str = finish_date.strftime(\"%Y-%m-%d\")\n",
    "        print(f\"\\nGoogle trends {name} in {get_geo_name(geo)} during {start_date_str} - {finish_date_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T13:38:40.982464Z",
     "start_time": "2019-07-05T13:38:40.976980Z"
    }
   },
   "outputs": [],
   "source": [
    "def convert_datetime_in_interesting_google(df):\n",
    "    columns = df.columns\n",
    "    converted_df = df.rdd.map(lambda x : (\n",
    "                                          x[\"Date\"].strftime(\"%Y-%m-%d\"), \n",
    "                                          x[columns[1]], \n",
    "                                          x[columns[2]], \n",
    "                                          x[columns[3]],\n",
    "                                          x[columns[4]],\n",
    "                                          x[columns[5]])).toDF([columns[0], columns[1], columns[2], columns[3], columns[4], columns[5]])\n",
    "                                                \n",
    "    return converted_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Google Trends data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T13:38:43.580633Z",
     "start_time": "2019-07-05T13:38:43.526264Z"
    }
   },
   "outputs": [],
   "source": [
    "google_trends_search_queries_us = spark.read.csv('data/google-trends/google-trends-search-queries-US.csv', inferSchema=True, header=True)\n",
    "google_trends_search_topics_us = spark.read.csv('data/google-trends/google-trends-search-topics-US.csv', inferSchema=True, header=True)\n",
    "google_trends_search_queries_us_ny = spark.read.csv('data/google-trends/google-trends-search-queries-US-NY.csv', inferSchema=True, header=True)\n",
    "google_trends_search_topics_us_ny = spark.read.csv('data/google-trends/google-trends-search-topics-US-NY.csv', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the historical data, it can take a while"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Range for csv:  2019-06-12 00:00:00+00:00 2019-06-12 23:59:59+00:00\n",
      "Time range for mongodb:  None None\n"
     ]
    }
   ],
   "source": [
    "times = get_history_and_real_timeframe(requested_start_dt = frame_start_datetime, \n",
    "                                       requested_finish_dt = frame_finish_datetime)\n",
    "\n",
    "print(\"Range for csv: \", times[0], times[1])\n",
    "print(\"Time range for mongodb: \", times[2], times[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T13:38:45.189768Z",
     "start_time": "2019-07-05T13:38:45.184190Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_historical_df(historical_start_time, historical_finish_time):\n",
    "    print(\"Range for collected data (history): \", historical_start_time, historical_finish_time)\n",
    "    \n",
    "    df = spark.read.csv(historical_tweets_data, inferSchema=True, header=True)\n",
    "    # remove records with no date\n",
    "    df = df.na.drop(subset=[\"created_at\"])\n",
    "    \n",
    "    # convert string to desired date format\n",
    "    from datetime import datetime\n",
    "    from pyspark.sql.functions import col, udf\n",
    "    from pyspark.sql.types import DateType, TimestampType\n",
    "\n",
    "    func =  udf (lambda x: str_tweet_to_datetime(x), TimestampType())\n",
    "\n",
    "    df = df.withColumn('created_at', func(col('created_at')))\n",
    "\n",
    "    selected_history = df.filter((df.created_at > historical_start_time) & (df.created_at < historical_finish_time))\n",
    "\n",
    "    return selected_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mongo DB Staff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2 = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df2.createOrReplaceTempView(\"recent_data\")\n",
    "# df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recent_data = spark.sql('SELECT text, screen_name, place.location FROM recent_data WHERE lang=\"en\" AND place.country=\"US\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# recent_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data filtering and merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T13:38:47.040177Z",
     "start_time": "2019-07-05T13:38:45.812859Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Range for collected data (history):  2019-06-12 00:00:00+00:00 2019-06-12 23:59:59+00:00\n"
     ]
    }
   ],
   "source": [
    "selected_history = None\n",
    "\n",
    "if times[0] != None and times[1] != None:\n",
    "    selected_history = get_historical_df(historical_start_time = times[0], historical_finish_time = times[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29538"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_history.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T10:21:41.712358Z",
     "start_time": "2019-07-05T10:21:41.706636Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Range for recent data (mongodb):  None None\n"
     ]
    }
   ],
   "source": [
    "# select recent data according to user's time request\n",
    "\n",
    "times = get_history_and_real_timeframe(requested_start_dt = frame_start_datetime, \n",
    "                                       requested_finish_dt = frame_finish_datetime)\n",
    "recent_start_time = times[2]\n",
    "recent_finish_time = times[3]\n",
    "\n",
    "print(\"Range for recent data (mongodb): \", recent_start_time, recent_finish_time)\n",
    "\n",
    "selected_recent = None\n",
    "\n",
    "if recent_start_time != None and recent_finish_time != None:\n",
    "    selected_recent = df.filter((recent_data.created_at > historical_start_time) \n",
    "                                & (recent_data.created_at < historical_finish_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T11:16:35.087709Z",
     "start_time": "2019-07-05T11:16:35.083709Z"
    }
   },
   "outputs": [],
   "source": [
    "# merge together selected_recent and selected_history\n",
    "selected_df = None\n",
    "\n",
    "if selected_history != None and selected_recent != None:\n",
    "    selected_df = selected_history.union(selected_recent)\n",
    "elif selected_history != None and selected_recent == None:\n",
    "    selected_df = selected_history\n",
    "elif selected_history != None and selected_recent == None:\n",
    "    selected_df = selected_recent\n",
    "\n",
    "assert selected_df != None, \"Something goes wrong with selecting data from recent data/history data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29538"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweets preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text cleaning is crucial for any text modelling process, especially for topic modelling. In our case it consists from those steps:  \n",
    "1) Lowercase all words  \n",
    "2) Filter words with non-letters at the beginning (mainly for mentions, e.g. \"@some_user\")  \n",
    "3) Filter http/https  \n",
    "4) Filter all non-letters (crucial to remove emoji)  \n",
    "5) Remove multiply whitespaces  \n",
    "6) Remove repeated chars (e.g. \"greeeeat\" -> \"great\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = selected_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.WordPunctTokenizer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_word_list = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter nans\n",
    "df = df.rdd.filter(lambda x: x[0] != None and x[1] != None and x[2] != None and x[4] != None)\n",
    "\n",
    "# filter out channels not to consider\n",
    "df = df.filter(lambda x: x[4] not in channels_not_to_consider)\n",
    "\n",
    "# filter by country\n",
    "df = df.filter(lambda x: x[1] in 'US')\n",
    "\n",
    "# filter by precise location\n",
    "if get_from_location:\n",
    "    df = df.filter(lambda x: x[2] in locations_to_consider)\n",
    "\n",
    "# filter tweet itself\n",
    "df = df.filter(lambda x: filter_tweet(x[0], channels_not_to_consider=channels_not_to_consider))\n",
    "\n",
    "# process tweet\n",
    "df = df.map(lambda x: process_tweet(x[0]))\n",
    "\n",
    "# final preprocesssing\n",
    "df = df.filter(lambda x: len(x) > 0)\n",
    "\n",
    "# make dataframes great again\n",
    "df = df.map(lambda x: [x])\n",
    "\n",
    "# schema for df\n",
    "schema = StructType([StructField('tokens', ArrayType(StringType()), True)])\n",
    "df = df.toDF(schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|              tokens|\n",
      "+--------------------+\n",
      "|[sad, news, noise...|\n",
      "|[first, thing, ge...|\n",
      "|               [lit]|\n",
      "|[ironically, star...|\n",
      "|   [thank, bro, bro]|\n",
      "|         [yeah, sir]|\n",
      "|[indeed, stan, fo...|\n",
      "|[good, draping, e...|\n",
      "|[leftover, tuesda...|\n",
      "|[anyone, want, em...|\n",
      "+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17398"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic modeling/Latent Dirichlet allocation(LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2.show(10, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07172019 22:31:29\n",
      "07172019 22:32:00\n"
     ]
    }
   ],
   "source": [
    "print(time.strftime('%m%d%Y %H:%M:%S'))\n",
    "\n",
    "cv = CountVectorizer(inputCol=\"tokens\", outputCol=\"raw_features\", vocabSize=10000, minDF=2.0)\n",
    "cvmodel = cv.fit(df)\n",
    "\n",
    "print(time.strftime('%m%d%Y %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07172019 22:32:00\n",
      "07172019 22:32:00\n"
     ]
    }
   ],
   "source": [
    "print(time.strftime('%m%d%Y %H:%M:%S'))\n",
    "df = cvmodel.transform(df)\n",
    "print(time.strftime('%m%d%Y %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"tf_idf_features\", minDocFreq=2)\n",
    "idfModel = idf.fit(df)\n",
    "\n",
    "df = idfModel.transform(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|              tokens|        raw_features|     tf_idf_features|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|[sad, news, noise...|(7583,[287,369,63...|(7583,[287,369,63...|\n",
      "|[first, thing, ge...|(7583,[2,21,36,37...|(7583,[2,21,36,37...|\n",
      "|               [lit]|  (7583,[753],[1.0])|(7583,[753],[6.54...|\n",
      "|[ironically, star...|(7583,[403,862,11...|(7583,[403,862,11...|\n",
      "|   [thank, bro, bro]|(7583,[18,150],[1...|(7583,[18,150],[3...|\n",
      "|         [yeah, sir]|(7583,[120,675],[...|(7583,[120,675],[...|\n",
      "|[indeed, stan, fo...|(7583,[141,259,42...|(7583,[141,259,42...|\n",
      "|[good, draping, e...|(7583,[14,79,679,...|(7583,[14,79,679,...|\n",
      "|[leftover, tuesda...|(7583,[154,1093,1...|(7583,[154,1093,1...|\n",
      "|[anyone, want, em...|(7583,[20,108,706...|(7583,[20,108,706...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.drop(\"name\")\n",
    "#df.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = Window().orderBy(column(\"tokens\"))\n",
    "df = df.withColumn(\"id\", row_number().over(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+---+\n",
      "|              tokens|        raw_features|     tf_idf_features| id|\n",
      "+--------------------+--------------------+--------------------+---+\n",
      "|[aapl, strong, da...|(7583,[0,10,17,21...|(7583,[0,10,17,21...|  1|\n",
      "|[aaron, coming, b...|(7583,[32,148,144...|(7583,[32,148,144...|  2|\n",
      "|[aaron, got, full...|(7583,[12,30,106,...|(7583,[12,30,106,...|  3|\n",
      "|[abandoned, novel...|(7583,[16,115,174...|(7583,[16,115,174...|  4|\n",
      "|[abingdon, market...|(7583,[1,4,24,708...|(7583,[1,4,24,708...|  5|\n",
      "|[able, convert, s...|(7583,[11,525,657...|(7583,[11,525,657...|  6|\n",
      "|[able, cop, gener...|(7583,[54,118,222...|(7583,[54,118,222...|  7|\n",
      "|           [able, w]|(7583,[76,657],[1...|(7583,[76,657],[4...|  8|\n",
      "| [aboard, tug, boat]|(7583,[3111,4114]...|(7583,[3111,4114]...|  9|\n",
      "|[abortion, listen...| (7583,[1494],[1.0])|(7583,[1494],[7.1...| 10|\n",
      "+--------------------+--------------------+--------------------+---+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = df.rdd.map(lambda x: (x[3], oldVectors.fromML(x[2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|_1 |_2                                                                                                                                                                                                                                                                                                 |\n",
      "+---+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|1  |(7583,[0,10,17,21,35,132,161,233,252,620,637,957,1235],[2.971823584815907,3.6199823781610703,3.86153467888535,3.906234857803257,4.17318103177586,5.082036785162496,5.2533085057698665,5.501488135245401,5.605284928927045,6.362970630624561,6.362970630624561,6.819729033120276,7.056117811184506])|\n",
      "|2  |(7583,[32,148,1441,1837],[4.1546962171017565,5.149047495445457,7.199218654825179,7.46158291929267])                                                                                                                                                                                                |\n",
      "|3  |(7583,[12,30,106,309,657,685,837,1837,2523],[3.7310817904879148,4.165746053288341,4.968377466689975,5.721116744452166,6.396872182300243,6.468331146282387,6.719645574563293,7.46158291929267,7.818257863231403])                                                                                   |\n",
      "|4  |(7583,[16,115,174,538,1251,1626,2044,5083],[3.8345788688968216,5.019235883923466,5.275531642554577,6.208819950797302,7.056117811184506,7.366272739488346,7.566943434950497,8.377873651166826])                                                                                                     |\n",
      "|5  |(7583,[1,4,24,708],[3.1454290287694966,3.5945572797952603,3.9803428299569785,6.506071474265235])                                                                                                                                                                                                   |\n",
      "|6  |(7583,[11,525,657,4710],[3.7167958332404383,6.208819950797302,6.396872182300243,8.377873651166826])                                                                                                                                                                                                |\n",
      "|7  |(7583,[54,118,222,231,657,1052,1425,6214],[4.496309853223388,5.019235883923466,5.446679898750406,5.487501893270661,6.396872182300243,6.873796254390552,7.199218654825179,8.665555723618606])                                                                                                       |\n",
      "|8  |(7583,[76,657],[4.753532718190461,6.396872182300243])                                                                                                                                                                                                                                              |\n",
      "|9  |(7583,[3111,4114],[8.154730099852616,8.377873651166826])                                                                                                                                                                                                                                           |\n",
      "|10 |(7583,[1494],[7.199218654825179])                                                                                                                                                                                                                                                                  |\n",
      "+---+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rs_df = rs.toDF()\n",
    "rs_df.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07172019 22:34:04\n",
      "07172019 22:35:18\n"
     ]
    }
   ],
   "source": [
    "# Run the LDA Topic Modeler\n",
    "# Note the time before and after is printed in order to find out how much time it takes to process x number of records\n",
    "\n",
    "print(time.strftime('%m%d%Y %H:%M:%S'))\n",
    "lda_model = LDA.train(rs_df['_1', '_2'].rdd.map(list), k=10, maxIterations=100)\n",
    "print(time.strftime('%m%d%Y %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07172019 22:35:18\n"
     ]
    }
   ],
   "source": [
    "print(time.strftime('%m%d%Y %H:%M:%S'))\n",
    "topics = lda_model.topicsMatrix()\n",
    "vocabArray = cvmodel.vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07172019 22:35:20\n"
     ]
    }
   ],
   "source": [
    "wordNumbers = 15\n",
    "\n",
    "topicIndices = sc.parallelize(lda_model.describeTopics(maxTermsPerTopic = wordNumbers))\n",
    "\n",
    "def topic_render(topic):  # specify vector id of words to actual words\n",
    "    terms = topic[0]\n",
    "    prob = topic[1]\n",
    "    \n",
    "    result = []\n",
    "    for i in range(nomber_of_words_to_for_topic):\n",
    "        term = str(round(prob[i],3))+\"  \"+vocabArray[terms[i]]\n",
    "        result.append(term)\n",
    "    return result\n",
    "print(time.strftime('%m%d%Y %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07172019 22:35:20\n",
      "07172019 22:35:20\n"
     ]
    }
   ],
   "source": [
    "print(time.strftime('%m%d%Y %H:%M:%S'))\n",
    "topics_final = topicIndices.map(lambda topic:topic_render(topic)).collect()\n",
    "print(time.strftime('%m%d%Y %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #1\n",
      "0.032  u\n",
      "0.032  love\n",
      "0.024  thank\n",
      "0.02  work\n",
      "0.02  right\n",
      "0.019  back\n",
      "0.017  let\n",
      "0.016  always\n",
      "0.014  fuck\n",
      "0.013  also\n",
      "0.011  big\n",
      "0.01  looking\n",
      "0.01  trying\n",
      "0.009  watch\n",
      "0.009  lot\n",
      "\n",
      "\n",
      "Topic #2\n",
      "0.03  one\n",
      "0.029  day\n",
      "0.026  good\n",
      "0.026  got\n",
      "0.022  today\n",
      "0.021  year\n",
      "0.018  look\n",
      "0.018  thing\n",
      "0.016  much\n",
      "0.015  last\n",
      "0.015  feel\n",
      "0.013  guy\n",
      "0.012  gonna\n",
      "0.011  night\n",
      "0.011  week\n",
      "\n",
      "\n",
      "Topic #3\n",
      "0.028  see\n",
      "0.027  like\n",
      "0.026  lol\n",
      "0.015  first\n",
      "0.014  said\n",
      "0.013  real\n",
      "0.013  nigga\n",
      "0.012  job\n",
      "0.011  wait\n",
      "0.011  tell\n",
      "0.011  many\n",
      "0.01  wanna\n",
      "0.009  white\n",
      "0.009  ok\n",
      "0.008  team\n",
      "\n",
      "\n",
      "Topic #4\n",
      "0.048  new\n",
      "0.036  york\n",
      "0.031  time\n",
      "0.025  go\n",
      "0.022  think\n",
      "0.015  come\n",
      "0.014  oh\n",
      "0.013  city\n",
      "0.012  give\n",
      "0.011  as\n",
      "0.01  ya\n",
      "0.01  world\n",
      "0.01  park\n",
      "0.009  try\n",
      "0.008  manhattan\n",
      "\n",
      "\n",
      "Topic #5\n",
      "0.022  make\n",
      "0.021  would\n",
      "0.019  lmao\n",
      "0.018  even\n",
      "0.017  life\n",
      "0.015  happy\n",
      "0.015  best\n",
      "0.012  someone\n",
      "0.012  w\n",
      "0.01  thought\n",
      "0.01  mean\n",
      "0.009  b\n",
      "0.009  bitch\n",
      "0.009  hate\n",
      "0.009  person\n",
      "\n",
      "\n",
      "Topic #6\n",
      "0.039  get\n",
      "0.028  need\n",
      "0.025  want\n",
      "0.013  please\n",
      "0.013  girl\n",
      "0.013  woman\n",
      "0.012  baby\n",
      "0.012  trump\n",
      "0.01  god\n",
      "0.01  home\n",
      "0.01  sure\n",
      "0.01  talk\n",
      "0.009  money\n",
      "0.009  like\n",
      "0.009  long\n",
      "\n",
      "\n",
      "Topic #7\n",
      "0.031  know\n",
      "0.022  shit\n",
      "0.02  say\n",
      "0.016  man\n",
      "0.016  people\n",
      "0.015  friend\n",
      "0.015  yes\n",
      "0.014  well\n",
      "0.013  im\n",
      "0.012  stop\n",
      "0.012  lmfao\n",
      "0.011  getting\n",
      "0.011  fucking\n",
      "0.01  hope\n",
      "0.01  everyone\n",
      "\n",
      "\n",
      "Topic #8\n",
      "0.025  really\n",
      "0.02  never\n",
      "0.02  going\n",
      "0.018  take\n",
      "0.018  show\n",
      "0.017  every\n",
      "0.014  ever\n",
      "0.012  could\n",
      "0.011  morning\n",
      "0.011  keep\n",
      "0.01  old\n",
      "0.01  actually\n",
      "0.009  wow\n",
      "0.009  bro\n",
      "0.009  play\n",
      "\n",
      "\n",
      "Topic #9\n",
      "0.029  amp\n",
      "0.022  ny\n",
      "0.022  nyc\n",
      "0.019  great\n",
      "0.017  th\n",
      "0.014  brooklyn\n",
      "0.014  better\n",
      "0.013  next\n",
      "0.012  tonight\n",
      "0.012  pm\n",
      "0.012  made\n",
      "0.01  june\n",
      "0.009  street\n",
      "0.008  open\n",
      "0.008  place\n",
      "\n",
      "\n",
      "Topic #10\n",
      "0.019  still\n",
      "0.017  way\n",
      "0.013  game\n",
      "0.012  summer\n",
      "0.011  amazing\n",
      "0.011  anyone\n",
      "0.01  yeah\n",
      "0.01  video\n",
      "0.009  music\n",
      "0.009  black\n",
      "0.008  check\n",
      "0.008  making\n",
      "0.008  part\n",
      "0.008  case\n",
      "0.008  miss\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# based on the simple vectors(+number of words)\n",
    "\n",
    "for topic in range(len(topics_final)):\n",
    "    print (\"Topic #\" + str(topic+1) + \"\")\n",
    "    for term in topics_final[topic]:\n",
    "        print (term)\n",
    "    print ('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hot topics in the USA from [Google trends](https://trends.google.com/trends/explore?geo=US)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = frame_start_datetime #str_tweet_to_datetime(frame_start_datetime)\n",
    "finish_date = frame_finish_datetime #str_tweet_to_datetime(frame_finish_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_trends_topics, google_trends_queries = get_google_trends_by_geo(geo) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Google trends search queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "interesting_google_topics = google_trends_topics.filter(\n",
    "    (google_trends_topics.Date >= start_date) & (google_trends_topics.Date <= finish_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Google trends Search topics in New York during 2019-06-12 - 2019-06-12\n",
      "+----------+----------------------+----------------------------------------+\n",
      "|Date      |Search topics - rising|Search topics - top                     |\n",
      "+----------+----------------------+----------------------------------------+\n",
      "|2019-06-12|Download - Topic      |New York - City in New York             |\n",
      "|2019-06-12|null                  |New York - US State                     |\n",
      "|2019-06-12|null                  |Google Search - Topic                   |\n",
      "|2019-06-12|null                  |Google - Technology company             |\n",
      "|2019-06-12|null                  |2019 - Topic                            |\n",
      "|2019-06-12|null                  |Weather - Topic                         |\n",
      "|2019-06-12|null                  |YouTube - Video sharing company         |\n",
      "|2019-06-12|null                  |Facebook, Inc. - Social network company |\n",
      "|2019-06-12|null                  |Facebook - Social networking service    |\n",
      "|2019-06-12|null                  |Amazon.com - E-commerce company         |\n",
      "|2019-06-12|null                  |Film - Topic                            |\n",
      "|2019-06-12|null                  |Definition - Topic                      |\n",
      "|2019-06-12|null                  |Brooklyn - New York City borough        |\n",
      "|2019-06-12|null                  |United States - Country in North America|\n",
      "|2019-06-12|null                  |New Jersey - US State                   |\n",
      "+----------+----------------------+----------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_google_trend_title(start_date, finish_date, \"Search topics\")\n",
    "interest_google_topics = convert_datetime_in_interesting_google(interesting_google_topics)\n",
    "interest_google_topics.select(\"Date\",\"Search topics - rising\", \"Search topics - top\").show(num_of_top_interest, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case when timeframe is more than 1 day, filter correctly this google-trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interesing_google_topics_unique= unique_google_trends_by_time_frame(interesting_google_topics)\n",
    "# print_google_trend_title(start_date, finish_date, \"Search topics\")\n",
    "# interesing_google_topics_unique.select(\"Search topics - rising\", \"Search topics - top\").show(num_of_top_interest, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Google trends search queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "interesting_google_queries = google_trends_queries.filter(\n",
    "    (google_trends_queries.Date >= start_date) & (google_trends_queries.Date <= finish_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Google trends Search queries in New York during 2019-06-12 - 2019-06-12\n",
      "+-----------------------+--------------------+------+---+-----+\n",
      "|Search queries - rising|Search queries - top|Rising|Top|geo  |\n",
      "+-----------------------+--------------------+------+---+-----+\n",
      "|sudanese massacre      |google              |+300% |100|US-NY|\n",
      "|hong kong              |weather             |+250% |79 |US-NY|\n",
      "|ny yankees score       |facebook            |+180% |59 |US-NY|\n",
      "|iready                 |youtube             |+150% |54 |US-NY|\n",
      "|boston bruins          |amazon              |+150% |49 |US-NY|\n",
      "|raz kids               |news                |+110% |46 |US-NY|\n",
      "|bruins                 |definition          |+110% |33 |US-NY|\n",
      "|cool math games        |craigslist          |+100% |28 |US-NY|\n",
      "|verizon wireless       |gmail               |+90%  |24 |US-NY|\n",
      "|bitcoin price          |translate           |+90%  |23 |US-NY|\n",
      "|scratch                |nba                 |+90%  |23 |US-NY|\n",
      "|max landis             |instagram           |+90%  |22 |US-NY|\n",
      "|sudan                  |yankees             |+80%  |21 |US-NY|\n",
      "|nhl playoffs           |people              |+80%  |21 |US-NY|\n",
      "|trump news             |yahoo               |+70%  |19 |US-NY|\n",
      "+-----------------------+--------------------+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "interesing_google_queries_unique= unique_google_trends_by_time_frame(interesting_google_queries)\n",
    "print_google_trend_title(start_date, finish_date, \"Search queries\")\n",
    "interesing_google_queries_unique.show(num_of_top_interest, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print_google_trend_title(start_date, finish_date, \"Search queries\")\n",
    "# interest_google_queries = convert_datetime_in_interesting_google(interesting_google_queries)\n",
    "# interest_google_queries.select(\"Date\", \"Search queries - rising\", \"Search queries - top\").show(num_of_top_interest, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hot topics - google trends (directly) (probably this will be removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date_str = start_date.strftime(\"%Y-%m-%d\")\n",
    "finish_date_str = finish_date.strftime(\"%Y-%m-%d\")\n",
    "pytrend = TrendReq()\n",
    "pytrend.build_payload(kw_list=[' '], geo=geo, timeframe=f\"{start_date_str} {finish_date_str}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Search topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_df = pytrend.related_top_search_topics(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Google trends Search topics in New York during 2019-06-12 - 2019-06-12\n",
      "+-------------------------+----------------------------------------+\n",
      "|Search topics - rising   |Search topics - top                     |\n",
      "+-------------------------+----------------------------------------+\n",
      "|Ice - Topic              |New York - City in New York             |\n",
      "|Mathematical game - Topic|New York - US State                     |\n",
      "|Design - Topic           |Google - Technology company             |\n",
      "|nan                      |Google Search - Topic                   |\n",
      "|nan                      |2019 - Topic                            |\n",
      "|nan                      |Weather - Topic                         |\n",
      "|nan                      |YouTube - Video sharing company         |\n",
      "|nan                      |Facebook - Social networking service    |\n",
      "|nan                      |Facebook, Inc. - Social network company |\n",
      "|nan                      |Amazon.com - E-commerce company         |\n",
      "|nan                      |Definition - Topic                      |\n",
      "|nan                      |Film - Topic                            |\n",
      "|nan                      |United States - Country in North America|\n",
      "|nan                      |Brooklyn - New York City borough        |\n",
      "|nan                      |Car - Transportation mode               |\n",
      "+-------------------------+----------------------------------------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_google_trend_title(start_date, finish_date, \"Search topics\")\n",
    "topics_df.select(\"Search topics - rising\", \"Search topics - top\").show(num_of_top_interest, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Search queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_df = pytrend.related_top_search_queries(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Google trends Search queries in New York during 2019-06-12 - 2019-06-12\n",
      "+-----------------------+--------------------+------+---+-----+\n",
      "|Search queries - rising|Search queries - top|Rising|Top|geo  |\n",
      "+-----------------------+--------------------+------+---+-----+\n",
      "|hong kong              |google              |+350% |100|US-NY|\n",
      "|courtney stodden       |weather             |+300% |80 |US-NY|\n",
      "|sudanese massacre      |facebook            |+250% |60 |US-NY|\n",
      "|khloe kardashian       |youtube             |+250% |56 |US-NY|\n",
      "|cricinfo               |amazon              |+190% |53 |US-NY|\n",
      "|iready                 |news                |+180% |42 |US-NY|\n",
      "|neiman marcus          |craigslist          |+110% |27 |US-NY|\n",
      "|jupiter ed             |world cup           |+100% |25 |US-NY|\n",
      "|ny lottery post        |instagram           |+90%  |25 |US-NY|\n",
      "|adidas                 |nba                 |+90%  |25 |US-NY|\n",
      "|alex morgan            |gmail               |+80%  |22 |US-NY|\n",
      "|pbs kids               |map                 |+80%  |22 |US-NY|\n",
      "|jon stewart            |drive               |+80%  |21 |US-NY|\n",
      "|mega millions          |apple               |+80%  |21 |US-NY|\n",
      "|chinese food           |translate           |+70%  |20 |US-NY|\n",
      "+-----------------------+--------------------+------+---+-----+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_google_trend_title(start_date, finish_date, \"Search queries\")\n",
    "queries_df.show(num_of_top_interest, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
