{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outline\n",
    "1. [Problem statement](#Problem-statement)\n",
    "2. [ML problem](#ML-problem)\n",
    "3. [Data description](#Data-description)\n",
    "4. [Demo](#Let's-start-our-demo-ride)\n",
    "    * [Imports](#Import-all-libs-needed)\n",
    "    * [Define variables](#Define-variables)\n",
    "    * [Create Spark session](#Create-Spark-session)\n",
    "    * [Read data](#Read-data)\n",
    "    * [Tweets preprocessing](#Tweets-preprocessing)\n",
    "    * [Topic modeling via Latent Dirichlet Allocation](#Topic-modeling-via-Latent-Dirichlet-Allocation)\n",
    "    * [Term frequency-inverse document frequency (TF-IDF)](#TF-IDF)\n",
    "    * [Run the LDA Topic Modeler](#Run-the-LDA-Topic-Modeler)\n",
    "    * [Hot topics in the USA from Google trends](#Hot-topics-in-the-USA-from-Google-trends)\n",
    "5. [Evaluation and justification](#Evaluation-and-justification)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem statement "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of our project is to identify the topics under active discussion at the moment in a certain area.\n",
    "<br> The motivation for choosing this topic is, firstly, the published news articles can be distorted. Secondly, there is a delay between the actual event and publication in the news. Both of these factors are critical for market players, and those who have instant access to reliable information have a clear advantage.\n",
    "<br>We decided to organize this advantage for ourselves and for everyone (since this project is open source), highlighting the topics discussed in real-time on Twitter.\n",
    "<br>As a source, we chose Twitter because of its popularity and prevalence throughout the world, legitimate access to real-time data and the many topics discussed in it.\n",
    "<br>As an example, there are many cases when companies use Twitter to identify vulnerability in their security systems since information about it often comes in social networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, exist a lot of algorithms that solve the problem topic-modeling. Some of them are based on classical mathematical approaches as matrix decomposition, some use probabilistic methods, some are based on deep learning. In the context of our task, we have considered 3 potential methods for solution: <b>LSA</b>, <b>pLSA</b> and <b>LDA</b>. Each of them has its advantages and disadvantages.\n",
    "<br> <b>LSA</b> - Latent Semantic Analysis - is based on a singular matrix decomposition, under the assumption that words that are close in meaning will occur in similar pieces of text. This method is simple to implement, but for reliable results requires a large amount of data. \n",
    "<br><b>pLSA</b> - Latent Semantic Analysis - is based on probabilistic methods and finding hidden variables - topics. But in this approach, the number of parameters increases linearly with the number of documents. \n",
    "<br>That is why we settled on the <b>LDA</b> - Latent Dirichlet Allocation - unsupervised learning algorithm, which assumes that the topics of the documents have a Dirichlet distribution and the words in the topics also have a Dirichlet distribution. The technical part of the algorithm will be described below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to collect data in a natural way:\n",
    "<br>- we registered Twitter Developer account;\n",
    "<br>- using credentials from Twitter Developer account we run script that collected tweets by the geolocation and saved them in mongodb;\n",
    "<br>\n",
    "<br><b>As a result:</b>\n",
    "<br>- we collected  332548 tweets (10Gb in mongodb, ~100Mb in csv) from New-York geolocation since 30 of May up to 15 of June;\n",
    "<br>- we collected  6617029 tweets (~1.69Gb in csv) from USA geolocation since 15 of June up to now.\n",
    "<br> All collected data can be downloaded via link: https://drive.google.com/file/d/1QxGI2esat6BnrPv0YE5Ud6uma49Lccty/view?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's start our demo ride"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import all libs needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T13:34:48.681195Z",
     "start_time": "2019-07-05T13:34:48.670618Z"
    }
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T13:34:49.159490Z",
     "start_time": "2019-07-05T13:34:49.142765Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# essential pyspark\n",
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import ArrayType, FloatType, StringType, IntegerType, StructField, StructType\n",
    "from pyspark.sql.functions import udf, row_number,column\n",
    "\n",
    "# vectorizer\n",
    "from pyspark.ml.feature import CountVectorizer, StopWordsRemover, HashingTF, IDF, Tokenizer\n",
    "\n",
    "# staff for LDA\n",
    "from pyspark.mllib.clustering import LDA, LDAModel\n",
    "from pyspark.mllib.linalg import Vector as oldVector, Vectors as oldVectors\n",
    "from pyspark.ml.linalg import Vector as newVector, Vectors as newVectors\n",
    "\n",
    "# pytrends for acquiring google trends\n",
    "from pytrends.pytrends.request import TrendReq\n",
    "\n",
    "# import hardcoded variables\n",
    "from utils.channels_to_filter import channels_not_to_consider\n",
    "\n",
    "# custom text preprocessing\n",
    "from utils.text_preprocessing import *\n",
    "\n",
    "# custom tools to work with google trends \n",
    "from utils.trends import *\n",
    "\n",
    "# handy functions for data merging\n",
    "from utils.data_merge import *\n",
    "\n",
    "# handy functions for topic modeling result handeling\n",
    "from utils.topic_modeling import *\n",
    "\n",
    "# datetime handling\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "# lib to download file with tweets from google drive\n",
    "from google_drive_downloader import GoogleDriveDownloader as gdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Connecting to data**  \n",
    "File with tweets will be downloaded automatically.\n",
    "<br> In case of any troubles it's possible to download file via https://drive.google.com/file/d/1gDdlNGaNZYrBg6IWirMy7lAj6e5tqcYk/view?usp=sharing and put in folder './data'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to CSV\n",
    "historical_tweets_data = 'data/tweets/new_york_training_tweets_15_06.csv'\n",
    "\n",
    "gdd.download_file_from_google_drive(file_id='1gDdlNGaNZYrBg6IWirMy7lAj6e5tqcYk',\n",
    "                                    dest_path=historical_tweets_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Time frames to pick data from**  \n",
    "We picked some time frames to get data from to check if our topic model can extract info about events that occured during this period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final of league championship \n",
    "lc_final_start_datetime = \"Sat Jun 01 00:00:00 +0000 2019\"\n",
    "lc_final_finish_datetime = \"Sat Jun 01 23:59:59 +0000 2019\"\n",
    "\n",
    "# Stanley cup final\n",
    "stanley_final_start_datetime = \"Wed Jun 12 00:00:00 +0000 2019\"\n",
    "stanley_final_finish_datetime = \"Wed Jun 12 23:59:59 +0000 2019\"\n",
    "\n",
    "# Draft NBA\n",
    "nba_final_start_datetime = \"Thu Jun 20 00:00:00 +0000 2019\"\n",
    "nba_finish_final_datetime = \"Sun Jun 23 23:59:59 +0000 2019\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or you can specify your own dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_datetime = ''\n",
    "finish_datetime = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use one of the dates defined above right here (e.g., we use Stanely cup final dates)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_start_datetime = str_tweet_to_datetime(lc_final_start_datetime)\n",
    "frame_finish_datetime = str_tweet_to_datetime(lc_final_finish_datetime)\n",
    "\n",
    "assert (frame_finish_datetime - frame_start_datetime).days <= 3, \"Date interval should not be bigger than 3 days\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Location related variables**  \n",
    "Here you can specify exact location from which you want to get tweets for topic modeling. Here we have example for NYC (local approach) and whole US (global approach). You can explore raw data to find more locations for filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T13:41:05.605577Z",
     "start_time": "2019-07-05T13:41:05.601192Z"
    }
   },
   "outputs": [],
   "source": [
    "# if True locations from locations_to_consider will be used to filter\n",
    "get_from_location = True\n",
    "\n",
    "# locations to filter relevant tweets\n",
    "locations_to_consider = [\n",
    "                         'Manhattan, NY', \n",
    "                         'Brooklyn, NY', \n",
    "                         'Queens, NY', \n",
    "                         'Bronx, NY', \n",
    "                         'Staten Island, NY'\n",
    "                         'New York, USA'\n",
    "                        ]\n",
    "\n",
    "# used to extract google trends\n",
    "if get_from_location:\n",
    "    geo = 'US-NY' \n",
    "else:\n",
    "    geo = 'US'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LDA parameters**  \n",
    "We have tuned parameters of LDA in order to obtain reliable results. The evaluation will be discussed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T13:38:36.620763Z",
     "start_time": "2019-07-05T13:38:36.617592Z"
    }
   },
   "outputs": [],
   "source": [
    "# LDA params\n",
    "lda_seed = 42\n",
    "num_of_topics_LDA = 15\n",
    "max_iterations_LDA = 120\n",
    "\n",
    "number_of_words_per_topic = 15  # number of words per topic\n",
    "num_of_top_interest = 15 # number of topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T13:38:37.435833Z",
     "start_time": "2019-07-05T13:38:37.427311Z"
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"pipeline\").getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the historical data, it can take a while**  \n",
    "Here we load data and filter by dates you chose above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T13:38:45.189768Z",
     "start_time": "2019-07-05T13:38:45.184190Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time range to be extracted from  data/tweets/new_york_training_tweets_15_06.csv 2019-06-01 00:00:00+00:00 2019-06-01 23:59:59+00:00\n",
      "Range for collected data (history):  2019-06-01 00:00:00+00:00 2019-06-01 23:59:59+00:00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "58318"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "times = (frame_start_datetime, frame_finish_datetime)\n",
    "print(\"Time range to be extracted from \", historical_tweets_data, times[0], times[1])\n",
    "selected_df = get_historical_df(historical_tweets_data=historical_tweets_data, historical_start_time=times[0], historical_finish_time=times[1], spark=spark)\n",
    "assert selected_df != None, \"Something goes wrong with selecting data from recent data/history data\"\n",
    "selected_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweets preprocessing  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Basic filtering**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we do basic filtering based on null values. We filter out channels that are not important in topic modeling. We defined it by applying LDA on different dates and find out that there are a lot of channels which specialized on particular topics (for example, weather, traffic in the city, photos, hiring people for a job). These topics were always distinguished. But, since they don't provide any information about important events, we have removed specialized channels from consideration.\n",
    " Also, we do filtering based on global and local location (if local filtering is enabled). Finally, we check tweet itself for the length of the message. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = selected_df\n",
    "\n",
    "# filter nans\n",
    "df = df.rdd.filter(lambda x: x[0] != None and x[1] != None and x[2] != None and x[4] != None)\n",
    "\n",
    "# filter out channels not to consider\n",
    "df = df.filter(lambda x: x[4] not in channels_not_to_consider)\n",
    "\n",
    "# filter by country\n",
    "df = df.filter(lambda x: x[1] in 'US')\n",
    "\n",
    "# filter by precise location\n",
    "if get_from_location:\n",
    "    df = df.filter(lambda x: x[2] in locations_to_consider)\n",
    "\n",
    "# filter tweet itself\n",
    "df = df.filter(lambda x: filter_tweet(x[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now the most interesting - tweet cleaning**  \n",
    "Text cleaning is crucial for any text modelling process, especially for topic modelling. We tried three different approaches: classic, using only hashtags, using only urls. In classic approach we delete all non-words (including urls, hashtags, emojis and mentions), filter out common stop words, so only plain text information is left. But still such data has a lot of noise and uninformative words, so we tried another approaches with using only hashtags (which should code the most important information) and urls."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's start from classic approach**  \n",
    "In our case it consists from those steps:  \n",
    "1) Lowercase all words  \n",
    "2) Filter words with non-letters at the beginning (mainly for mentions, e.g. \"@some_user\")  \n",
    "3) Filter http/https  \n",
    "4) Filter all non-letters (crucial to remove emoji)  \n",
    "5) Remove multiply whitespaces  \n",
    "6) Remove repeated chars (e.g. \"greeeeat\" -> \"great\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hashtags & URLs**  \n",
    "Those approaches are pretty clear. We simply get hashtags and urls from tweet with regex. In case of urls we query it **(one query can take up to 500ms, so it can take a while)** and get html response, then we parse meta for keywords and description. Some basic preprocessing and tokenization is applied to both, keywords and description. Finally, we merge keywords and description tokens together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Basically applying one of the approaches discussed above**  \n",
    "Now let's apply one of the aproaches. But first specify which one you want to use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_type = 'hashtags' # 'just-text', 'hashtags' or 'urls'\n",
    "\n",
    "if preprocessing_type == 'just-text':\n",
    "    process_tweet = process_text\n",
    "elif preprocessing_type == 'hashtags':\n",
    "    process_tweet = process_hashtags\n",
    "elif preprocessing_type == 'urls':\n",
    "    process_tweet = process_urls\n",
    "\n",
    "# process tweet\n",
    "df = df.map(lambda x: process_tweet(x[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Final postprocessing**  \n",
    "Here we make sure that we don't have entries without tokens at all, also we change the structure suitable for pyspark LDA class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final preprocesssing\n",
    "df = df.filter(lambda x: len(x) > 0)\n",
    "\n",
    "# make dataframes great again\n",
    "df = df.map(lambda x: [x])\n",
    "\n",
    "# schema for df\n",
    "schema = StructType([StructField('tokens', ArrayType(StringType()), True)])\n",
    "df = df.toDF(schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|              tokens|\n",
      "+--------------------+\n",
      "|[nationalsmileday...|\n",
      "|      [whoraisedyou]|\n",
      "|          [godzilla]|\n",
      "| [hero, rachelhauck]|\n",
      "|[latinosforice, r...|\n",
      "| [blackmendontcheat]|\n",
      "|[curiousincidento...|\n",
      "|   [abstinencegoals]|\n",
      "|              [wplj]|\n",
      "|[farewellplj, plj...|\n",
      "+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4060"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic modeling via Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic Model is a type of statistical model used for tagging abstract “topics” that occur in a collection of documents that best represents the information in them.<br/>\n",
    "The basic idea in the LDA is that documents are represented as a random mixture of latent topics, where each topic is characterized by a distribution of words.<br/>\n",
    "\n",
    "<img src=\"http://chdoig.github.io/pytexas2015-topic-modeling/images/lda-4.png\" width=600/>\n",
    "*http://chdoig.github.io/pytexas2015-topic-modeling/?source=post_page---------------------------#/3/4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### CountVectorizer helps to convert a collection of text documents to vectors of token counts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07202019 14:23:39\n",
      "07202019 14:24:43\n"
     ]
    }
   ],
   "source": [
    "print(time.strftime('%m%d%Y %H:%M:%S'))\n",
    "\n",
    "cv = CountVectorizer(inputCol=\"tokens\", outputCol=\"raw_features\", vocabSize=10000, minDF=2.0)\n",
    "cvmodel = cv.fit(df)\n",
    "\n",
    "print(time.strftime('%m%d%Y %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07202019 14:24:43\n",
      "07202019 14:24:44\n"
     ]
    }
   ],
   "source": [
    "print(time.strftime('%m%d%Y %H:%M:%S'))\n",
    "df = cvmodel.transform(df)\n",
    "print(time.strftime('%m%d%Y %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Process text mining to reflect the importance of a term to a document in the corpus. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"tf_idf_features\", minDocFreq=2)\n",
    "idfModel = idf.fit(df)\n",
    "\n",
    "df = idfModel.transform(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|              tokens|        raw_features|     tf_idf_features|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|[nationalsmileday...|(1096,[145,801],[...|(1096,[145,801],[...|\n",
      "|      [whoraisedyou]|        (1096,[],[])|        (1096,[],[])|\n",
      "|          [godzilla]|  (1096,[150],[1.0])|(1096,[150],[6.36...|\n",
      "| [hero, rachelhauck]|        (1096,[],[])|        (1096,[],[])|\n",
      "|[latinosforice, r...|        (1096,[],[])|        (1096,[],[])|\n",
      "| [blackmendontcheat]|  (1096,[871],[1.0])|(1096,[871],[7.21...|\n",
      "|[curiousincidento...|  (1096,[306],[1.0])|(1096,[306],[6.69...|\n",
      "|   [abstinencegoals]|        (1096,[],[])|        (1096,[],[])|\n",
      "|              [wplj]|  (1096,[366],[1.0])|(1096,[366],[6.92...|\n",
      "|[farewellplj, plj...|  (1096,[893],[1.0])|(1096,[893],[7.21...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Add id field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = Window().orderBy(column(\"tokens\"))\n",
    "df = df.withColumn(\"id\", row_number().over(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+---+\n",
      "|              tokens|        raw_features|     tf_idf_features| id|\n",
      "+--------------------+--------------------+--------------------+---+\n",
      "|                 [0]|        (1096,[],[])|        (1096,[],[])|  1|\n",
      "|[00, freeship, ma...|(1096,[2,513,906]...|(1096,[2,513,906]...|  2|\n",
      "|[000, freeship, m...|(1096,[2,513],[1....|(1096,[2,513],[3....|  3|\n",
      "|                 [1]|   (1096,[39],[1.0])|(1096,[39],[5.364...|  4|\n",
      "|                 [1]|   (1096,[39],[1.0])|(1096,[39],[5.364...|  5|\n",
      "|                 [1]|   (1096,[39],[1.0])|(1096,[39],[5.364...|  6|\n",
      "|                 [1]|   (1096,[39],[1.0])|(1096,[39],[5.364...|  7|\n",
      "|                 [1]|   (1096,[39],[1.0])|(1096,[39],[5.364...|  8|\n",
      "|                 [1]|   (1096,[39],[1.0])|(1096,[39],[5.364...|  9|\n",
      "|                 [1]|   (1096,[39],[1.0])|(1096,[39],[5.364...| 10|\n",
      "+--------------------+--------------------+--------------------+---+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = df.rdd.map(lambda x: (x[3], oldVectors.fromML(x[2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_df = rs.toDF()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the LDA Topic Modeler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07202019 14:27:55\n",
      "07202019 14:29:12\n"
     ]
    }
   ],
   "source": [
    "print(time.strftime('%m%d%Y %H:%M:%S'))\n",
    "lda_model = LDA.train(rs_df['_1', '_2'].rdd.map(list), k=num_of_topics_LDA, maxIterations=max_iterations_LDA, seed=lda_seed)\n",
    "print(time.strftime('%m%d%Y %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Now we prepare output of LDA  to be shown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07202019 14:29:12\n",
      "07202019 14:29:13\n"
     ]
    }
   ],
   "source": [
    "print(time.strftime('%m%d%Y %H:%M:%S'))\n",
    "topics = lda_model.topicsMatrix()\n",
    "vocabArray = cvmodel.vocabulary\n",
    "print(time.strftime('%m%d%Y %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07202019 14:29:13\n",
      "07202019 14:29:13\n"
     ]
    }
   ],
   "source": [
    "print(time.strftime('%m%d%Y %H:%M:%S'))\n",
    "topicIndices = sc.parallelize(lda_model.describeTopics(maxTermsPerTopic = number_of_words_per_topic))\n",
    "print(time.strftime('%m%d%Y %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07202019 14:29:13\n",
      "07202019 14:29:13\n"
     ]
    }
   ],
   "source": [
    "print(time.strftime('%m%d%Y %H:%M:%S'))\n",
    "topics_final = topicIndices.map(lambda topic:topic_render(topic, number_of_words_per_topic, vocabArray)).collect()\n",
    "print(time.strftime('%m%d%Y %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Topics based on tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #1\n",
      "0.053  love\n",
      "0.035  saturday\n",
      "0.031  photooftheday\n",
      "0.031  sunset\n",
      "0.027  family\n",
      "0.026  nofilter\n",
      "0.026  loveislove\n",
      "0.024  gay\n",
      "0.024  fun\n",
      "0.024  weekend\n",
      "0.023  fashion\n",
      "0.021  june\n",
      "0.018  streetwear\n",
      "0.018  instagood\n",
      "0.018  3\n",
      "\n",
      "\n",
      "Topic #2\n",
      "0.255  nyc\n",
      "0.153  newyork\n",
      "0.09  newyorkcity\n",
      "0.029  lgbtq\n",
      "0.024  fashion\n",
      "0.023  kevinshahroozi\n",
      "0.019  party\n",
      "0.019  lifestylesoftherichanddysfuntional\n",
      "0.018  guncontrolnow\n",
      "0.017  event\n",
      "0.015  robertachiarellajewelry\n",
      "0.014  home\n",
      "0.012  handmadeintheusa\n",
      "0.012  robertachiarella\n",
      "0.012  style\n",
      "\n",
      "\n",
      "Topic #3\n",
      "0.067  joshuaruiz\n",
      "0.061  bookcon19\n",
      "0.043  boxing\n",
      "0.03  smirk\n",
      "0.03  comedy\n",
      "0.03  goodomens\n",
      "0.03  smile\n",
      "0.023  humor\n",
      "0.018  chuckle\n",
      "0.018  laughter\n",
      "0.018  lol\n",
      "0.015  mom\n",
      "0.015  grin\n",
      "0.015  mirth\n",
      "0.015  tee\n",
      "\n",
      "\n",
      "Topic #4\n",
      "0.129  yankee\n",
      "0.094  mlb\n",
      "0.077  ynwa\n",
      "0.056  mets\n",
      "0.035  deadwoodmovie\n",
      "0.03  yankeestadium\n",
      "0.025  dbacks\n",
      "0.025  championsleague2019\n",
      "0.025  nypd\n",
      "0.022  phillies\n",
      "0.022  newmusic\n",
      "0.019  red\n",
      "0.017  redsox\n",
      "0.016  amjoy\n",
      "0.016  summer\n",
      "\n",
      "\n",
      "Topic #5\n",
      "0.062  ufcstockholm\n",
      "0.049  mma\n",
      "0.047  ufc\n",
      "0.044  happyhour\n",
      "0.044  art\n",
      "0.036  foj\n",
      "0.017  poem\n",
      "0.017  artist\n",
      "0.017  figmentnyc\n",
      "0.017  bitcoin\n",
      "0.017  plastic\n",
      "0.014  lighthousepark\n",
      "0.014  rooseveltisland\n",
      "0.014  classic\n",
      "0.014  life\n",
      "\n",
      "\n",
      "Topic #6\n",
      "0.173  whentheyseeus\n",
      "0.047  falcon4\n",
      "0.025  foodie\n",
      "0.022  phantomthread\n",
      "0.022  foodporn\n",
      "0.022  uws\n",
      "0.019  nevercursed\n",
      "0.019  godzilla\n",
      "0.019  legend\n",
      "0.019  photography\n",
      "0.016  jfk\n",
      "0.016  godzillakingofthemonsters\n",
      "0.016  author\n",
      "0.014  lovelife\n",
      "0.013  rip\n",
      "\n",
      "\n",
      "Topic #7\n",
      "0.061  manhattan\n",
      "0.061  ny\n",
      "0.044  centralpark\n",
      "0.032  liverpool\n",
      "0.032  pridemonth2019\n",
      "0.029  championsleague\n",
      "0.024  notmeus\n",
      "0.024  mytwitteranniversary\n",
      "0.022  spring\n",
      "0.019  cbd\n",
      "0.019  mta\n",
      "0.019  blessed\n",
      "0.019  happybirthday\n",
      "0.019  uclfinal2019\n",
      "0.016  stanleycup\n",
      "\n",
      "\n",
      "Topic #8\n",
      "0.104  alwaysbemymaybe\n",
      "0.059  coys\n",
      "0.044  repost\n",
      "0.035  govball2019\n",
      "0.035  youredoinggreat\n",
      "0.032  championsleaguefinal\n",
      "0.026  uclfinal19\n",
      "0.023  dunkout\n",
      "0.021  nowplaying\n",
      "0.018  model\n",
      "0.017  msg\n",
      "0.017  brunch\n",
      "0.015  mrjordanbc\n",
      "0.015  womeninventors\n",
      "0.015  bellaabzugpark\n",
      "\n",
      "\n",
      "Topic #9\n",
      "0.073  govballnyc\n",
      "0.052  beermenus\n",
      "0.033  medium\n",
      "0.028  streetart\n",
      "0.027  gh\n",
      "0.027  virginiabeachshooting\n",
      "0.027  timessquare\n",
      "0.025  governorsball\n",
      "0.019  nxttakeoverxxv\n",
      "0.019  travel\n",
      "0.018  nike\n",
      "0.017  festival\n",
      "0.017  graffiti\n",
      "0.017  cdr\n",
      "0.016  5\n",
      "\n",
      "\n",
      "Topic #10\n",
      "0.099  food\n",
      "0.088  hookah\n",
      "0.088  sugardaddysfridays\n",
      "0.088  valet\n",
      "0.074  sexyentertainers\n",
      "0.068  nycnightlife\n",
      "0.051  pride2019\n",
      "0.019  saturdaythoughts\n",
      "0.019  classof2019\n",
      "0.016  rbny\n",
      "0.014  amwriting\n",
      "0.014  saturdaymorning\n",
      "0.014  100reviews\n",
      "0.014  deadwood\n",
      "0.012  batman\n",
      "\n",
      "\n",
      "Topic #11\n",
      "0.135  uclfinal\n",
      "0.059  lfc\n",
      "0.025  broadway\n",
      "0.025  shakespeareinthepark\n",
      "0.023  rocketman\n",
      "0.023  fbf\n",
      "0.02  ucl\n",
      "0.02  govball\n",
      "0.018  free\n",
      "0.018  fan\n",
      "0.018  hadestown\n",
      "0.015  neverreallyover\n",
      "0.012  coffee\n",
      "0.012  netflix\n",
      "0.012  press\n",
      "\n",
      "\n",
      "Topic #12\n",
      "0.151  freeship\n",
      "0.071  bookcon\n",
      "0.07  case\n",
      "0.056  generic\n",
      "0.043  dvd\n",
      "0.028  cd\n",
      "0.028  jewel\n",
      "0.026  harlem\n",
      "0.021  sleeve\n",
      "0.019  latergram\n",
      "0.017  fitness\n",
      "0.015  808mafia\n",
      "0.014  graduation\n",
      "0.014  mylife\n",
      "0.012  storage\n",
      "\n",
      "\n",
      "Topic #13\n",
      "0.125  pride\n",
      "0.05  1\n",
      "0.045  queen\n",
      "0.033  2\n",
      "0.03  happypride\n",
      "0.026  stonewall50\n",
      "0.025  worldpride\n",
      "0.022  subway\n",
      "0.022  dog\n",
      "0.022  theother5th\n",
      "0.022  godzillamovie\n",
      "0.02  beautiful\n",
      "0.017  newprofilepic\n",
      "0.017  kelseysarahbinge\n",
      "0.017  championsleaguefinal2019\n",
      "\n",
      "\n",
      "Topic #14\n",
      "0.113  brooklyn\n",
      "0.047  music\n",
      "0.043  bookcon2019\n",
      "0.041  hiphoped\n",
      "0.031  hiphop\n",
      "0.026  bronx\n",
      "0.026  btsatwembley\n",
      "0.022  birthday\n",
      "0.022  mood\n",
      "0.019  bk\n",
      "0.017  rose\n",
      "0.017  2019\n",
      "0.017  friend\n",
      "0.017  saturdayvibes\n",
      "0.014  youtube\n",
      "\n",
      "\n",
      "Topic #15\n",
      "0.144  pridemonth\n",
      "0.047  nxttakeover\n",
      "0.032  lga\n",
      "0.027  flightdelay\n",
      "0.025  impact\n",
      "0.025  pinstripepride\n",
      "0.023  impactontwitch\n",
      "0.022  lgbt\n",
      "0.02  wwe\n",
      "0.02  boston\n",
      "0.018  travelingmusician\n",
      "0.015  oistrakhsymphony\n",
      "0.015  stonewall\n",
      "0.015  portrait\n",
      "0.015  bushwick\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for topic in range(len(topics_final)):\n",
    "    print (\"Topic #\" + str(topic+1) + \"\")\n",
    "    for term in topics_final[topic]:\n",
    "        print (term)\n",
    "    print ('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Result of topic modelling, actual topics are highlighted\n",
    "###### Topic #1 - BookCon - conference in NY (1-2 June)\n",
    "0.072  bookcon19<br/>\n",
    "0.053  1<br/>\n",
    "0.045  repost<br/>\n",
    "0.038  deadwoodmovie<br/>\n",
    "0.035  2<br/>\n",
    "0.027  sleeve<br/>\n",
    "0.027  mood<br/>\n",
    "0.021  legend<br/>\n",
    "0.021  blessed<br/>\n",
    "0.019  nike<br/>\n",
    "0.018  plastic<br/>\n",
    "0.018  model<br/>\n",
    "0.018  5<br/>\n",
    "0.018  deadwood<br/>\n",
    "0.018  streetphotography\n",
    "<br/>\n",
    "<br/>\n",
    "Topic #2<br/>\n",
    "0.082  bookcon<br/>\n",
    "0.053  music<br/>\n",
    "0.051  beermenus<br/>\n",
    "0.035  lga<br/>\n",
    "0.032  youredoinggreat<br/>\n",
    "0.032  govball2019<br/>\n",
    "0.03  flightdelay<br/>\n",
    "0.027  nofilter<br/>\n",
    "0.022  ucl<br/>\n",
    "0.019  uclfinal2019<br/>\n",
    "0.016  bushwick<br/>\n",
    "0.016  peace<br/>\n",
    "0.014  mrjordanbc<br/>\n",
    "0.014  womeninventors<br/>\n",
    "0.014  bellaabzugpark\n",
    "<br/>\n",
    "<br/>\n",
    "###### Topic #3 - June - month of pride\n",
    "0.131  pridemonth<br/>\n",
    "0.05  pride2019<br/>\n",
    "0.046  coys<br/>\n",
    "0.033  lgbtq<br/>\n",
    "0.029  hiphop<br/>\n",
    "0.027  smirk<br/>\n",
    "0.027  comedy<br/>\n",
    "0.024  happypride<br/>\n",
    "0.023  loveislove<br/>\n",
    "0.022  virginiabeachshooting<br/>\n",
    "0.02  humor<br/>\n",
    "0.02  uclfinal19<br/>\n",
    "0.02  lgbt<br/>\n",
    "0.016  chuckle<br/>\n",
    "0.016  laughter\n",
    "<br/>\n",
    "<br/>\n",
    "Topic #4<br/>\n",
    "0.106  brooklyn<br/>\n",
    "0.096  food<br/>\n",
    "0.086  sugardaddysfridays<br/>\n",
    "0.086  hookah<br/>\n",
    "0.086  valet<br/>\n",
    "0.072  sexyentertainers<br/>\n",
    "0.066  nycnightlife<br/>\n",
    "0.027  medium<br/>\n",
    "0.021  stonewall50<br/>\n",
    "0.021  worldpride<br/>\n",
    "0.02  championsleague2019<br/>\n",
    "0.018  author<br/>\n",
    "0.018  bk<br/>\n",
    "0.018  uws<br/>\n",
    "0.014  cdr\n",
    "<br/>\n",
    "<br/>\n",
    "###### Topic #5 - UFC Fight Night: Gustafsson vs. Smith - 1 June\n",
    "0.068  ufcstockholm<br/>\n",
    "0.053  mma<br/>\n",
    "0.051  ufc<br/>\n",
    "0.034  bronx<br/>\n",
    "0.028  nypd<br/>\n",
    "0.028  mytwitteranniversary<br/>\n",
    "0.022  cbd<br/>\n",
    "0.022  rbny<br/>\n",
    "0.022  red<br/>\n",
    "0.019  stanleycup<br/>\n",
    "0.019  youtube<br/>\n",
    "0.015  classic<br/>\n",
    "0.015  stlblues<br/>\n",
    "0.015  vintage<br/>\n",
    "0.015  moandmo\n",
    "<br/>\n",
    "<br/>\n",
    "Topic #6<br/>\n",
    "0.1  mlb<br/>\n",
    "0.06  mets<br/>\n",
    "0.035  pridemonth2019<br/>\n",
    "0.034  smile<br/>\n",
    "0.027  dbacks<br/>\n",
    "0.026  notmeus<br/>\n",
    "0.024  phillies<br/>\n",
    "0.023  govball<br/>\n",
    "0.021  3<br/>\n",
    "0.021  happybirthday<br/>\n",
    "0.018  comic<br/>\n",
    "0.018  jfk<br/>\n",
    "0.017  festival<br/>\n",
    "0.015  vegan<br/>\n",
    "0.014  chernobyl\n",
    "<br/>\n",
    "<br/>\n",
    "###### Topic #7 -  UEFA Champions League Final - 1 June\n",
    "0.116  uclfinal<br/>\n",
    "0.102  yankee<br/>\n",
    "0.06  ynwa<br/>\n",
    "0.051  lfc<br/>\n",
    "0.037  hiphoped<br/>\n",
    "0.026  liverpool<br/>\n",
    "0.024  yankeestadium<br/>\n",
    "0.024  championsleaguefinal<br/>\n",
    "0.022  gh<br/>\n",
    "0.015  nxttakeoverxxv<br/>\n",
    "0.015  photography<br/>\n",
    "0.013  poem<br/>\n",
    "0.013  redsox<br/>\n",
    "0.013  tottenham<br/>\n",
    "0.011  avonbeautybar2015\n",
    "<br/>\n",
    "<br/>\n",
    "Topic #8<br/>\n",
    "0.291  nyc<br/>\n",
    "0.04  falcon4<br/>\n",
    "0.028  sunset<br/>\n",
    "0.024  streetart<br/>\n",
    "0.021  birthday<br/>\n",
    "0.019  dunkout<br/>\n",
    "0.019  godzillamovie<br/>\n",
    "0.016  friend<br/>\n",
    "0.016  2019<br/>\n",
    "0.014  mylife<br/>\n",
    "0.014  portrait<br/>\n",
    "0.014  got<br/>\n",
    "0.014  bitcoin<br/>\n",
    "0.014  graffiti<br/>\n",
    "0.012  improv\n",
    "<br/>\n",
    "<br/>\n",
    "##### Topic #9 - boxing Anthony Joshua vs. Andy Ruiz Jr. - 1 June\n",
    "0.069  joshuaruiz<br/>\n",
    "0.052  love<br/>\n",
    "0.044  boxing<br/>\n",
    "0.028  championsleague<br/>\n",
    "0.026  family<br/>\n",
    "0.023  fun<br/>\n",
    "0.023  weekend<br/>\n",
    "0.021  phantomthread<br/>\n",
    "0.021  latergram<br/>\n",
    "0.021  june<br/>\n",
    "0.019  free<br/>\n",
    "0.018  nevercursed<br/>\n",
    "0.018  nowplaying<br/>\n",
    "0.016  msg<br/>\n",
    "0.015  amwriting\n",
    "<br/>\n",
    "<br/>\n",
    "Topic #10<br/>\n",
    "0.153  freeship<br/>\n",
    "0.071  case<br/>\n",
    "0.064  govballnyc<br/>\n",
    "0.057  generic<br/>\n",
    "0.043  dvd<br/>\n",
    "0.039  centralpark<br/>\n",
    "0.029  cd<br/>\n",
    "0.029  jewel<br/>\n",
    "0.026  btsatwembley<br/>\n",
    "0.024  timessquare<br/>\n",
    "0.022  governorsball<br/>\n",
    "0.019  spring<br/>\n",
    "0.014  figmentnyc<br/>\n",
    "0.014  graduation<br/>\n",
    "0.014  neverreallyover\n",
    "<br/>\n",
    "<br/>\n",
    "Topic #11<br/>\n",
    "0.112  pride<br/>\n",
    "0.046  nxttakeover<br/>\n",
    "0.038  happyhour<br/>\n",
    "0.033  saturday<br/>\n",
    "0.032  foj<br/>\n",
    "0.029  goodomens<br/>\n",
    "0.025  impact<br/>\n",
    "0.023  gay<br/>\n",
    "0.022  impactontwitch<br/>\n",
    "0.022  foodie<br/>\n",
    "0.02  wwe<br/>\n",
    "0.02  foodporn<br/>\n",
    "0.02  newmusic<br/>\n",
    "0.02  theother5th<br/>\n",
    "0.017  hadestown\n",
    "<br/>\n",
    "<br/>\n",
    "Topic #12<br/>\n",
    "0.058  art<br/>\n",
    "0.05  bookcon2019<br/>\n",
    "0.045  queen<br/>\n",
    "0.039  fashion<br/>\n",
    "0.033  photooftheday<br/>\n",
    "0.028  broadway<br/>\n",
    "0.02  streetwear<br/>\n",
    "0.02  instagood<br/>\n",
    "0.017  igers<br/>\n",
    "0.017  newprofilepic<br/>\n",
    "0.017  songswithmonsters<br/>\n",
    "0.014  askannyc<br/>\n",
    "0.014  dope<br/>\n",
    "0.014  instastyle<br/>\n",
    "0.014  custom\n",
    "<br/>\n",
    "<br/>\n",
    "Topic #13<br/>\n",
    "0.18  newyork<br/>\n",
    "0.106  newyorkcity<br/>\n",
    "0.027  kevinshahroozi<br/>\n",
    "0.022  party<br/>\n",
    "0.022  lifestylesoftherichanddysfuntional<br/>\n",
    "0.02  event<br/>\n",
    "0.02  subway<br/>\n",
    "0.019  classof2019<br/>\n",
    "0.017  robertachiarellajewelry<br/>\n",
    "0.017  travelingmusician<br/>\n",
    "0.017  home<br/>\n",
    "0.017  travel<br/>\n",
    "0.015  dj<br/>\n",
    "0.015  fashion<br/>\n",
    "0.015  handmadeintheusa\n",
    "<br/>\n",
    "<br/>\n",
    "Topic #14<br/>\n",
    "0.161  whentheyseeus<br/>\n",
    "0.058  ny<br/>\n",
    "0.058  manhattan<br/>\n",
    "0.02  saturdaythoughts<br/>\n",
    "0.018  rose<br/>\n",
    "0.018  fan<br/>\n",
    "0.018  mta<br/>\n",
    "0.015  flyelyfe<br/>\n",
    "0.015  saturdaymorning<br/>\n",
    "0.015  coneyisland<br/>\n",
    "0.013  rock<br/>\n",
    "0.012  work<br/>\n",
    "0.012  healthcare<br/>\n",
    "0.012  pop<br/>\n",
    "0.012  pizza\n",
    "<br/>\n",
    "<br/>\n",
    "Topic #15<br/>\n",
    "0.105  alwaysbemymaybe<br/>\n",
    "0.032  harlem<br/>\n",
    "0.029  pinstripepride<br/>\n",
    "0.029  shakespeareinthepark<br/>\n",
    "0.027  rocketman<br/>\n",
    "0.027  fbf<br/>\n",
    "0.026  guncontrolnow<br/>\n",
    "0.024  boston<br/>\n",
    "0.024  dog<br/>\n",
    "0.021  beautiful<br/>\n",
    "0.021  godzilla<br/>\n",
    "0.018  808mafia<br/>\n",
    "0.018  summer<br/>\n",
    "0.015  dogsofinstagram<br/>\n",
    "0.014  sesamestreet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "#### Result based on the tweets text without hashtags\n",
    "Topic #1\n",
    "0.038  get\n",
    "0.03  people\n",
    "0.027  need\n",
    "0.027  good\n",
    "0.024  want\n",
    "0.019  never\n",
    "0.017  still\n",
    "0.017  show\n",
    "0.015  first\n",
    "0.014  like\n",
    "0.013  gonna\n",
    "0.011  keep\n",
    "0.011  baby\n",
    "0.011  getting\n",
    "0.01  hope\n",
    "\n",
    "\n",
    "Topic #2\n",
    "0.029  amp\n",
    "0.02  work\n",
    "0.018  great\n",
    "0.017  th\n",
    "0.014  well\n",
    "0.012  tonight\n",
    "0.012  pm\n",
    "0.01  june\n",
    "0.009  another\n",
    "0.009  call\n",
    "0.008  nice\n",
    "0.008  open\n",
    "0.008  hour\n",
    "0.008  making\n",
    "0.008  free\n",
    "\n",
    "\n",
    "Topic #3\n",
    "0.028  got\n",
    "0.023  year\n",
    "0.018  take\n",
    "0.016  last\n",
    "0.016  best\n",
    "0.01  sure\n",
    "0.01  two\n",
    "0.009  already\n",
    "0.009  try\n",
    "0.009  wow\n",
    "0.009  literally\n",
    "0.009  read\n",
    "0.009  song\n",
    "0.008  point\n",
    "0.008  black\n",
    "\n",
    "\n",
    "Topic #4\n",
    "0.032  love\n",
    "0.03  day\n",
    "0.024  really\n",
    "0.022  make\n",
    "0.022  shit\n",
    "0.02  say\n",
    "0.02  right\n",
    "0.02  back\n",
    "0.019  lmao\n",
    "0.016  every\n",
    "0.015  happy\n",
    "0.015  come\n",
    "0.015  friend\n",
    "0.013  said\n",
    "0.012  im\n",
    "\n",
    "\n",
    "Topic #5\n",
    "0.025  thank\n",
    "0.02  thing\n",
    "0.016  feel\n",
    "0.015  yes\n",
    "0.015  ever\n",
    "0.013  real\n",
    "0.013  woman\n",
    "0.011  fucking\n",
    "0.011  amazing\n",
    "0.011  something\n",
    "0.011  god\n",
    "0.01  watch\n",
    "0.009  like\n",
    "0.009  white\n",
    "0.009  men\n",
    "\n",
    "\n",
    "Topic #6\n",
    "0.032  time\n",
    "0.032  u\n",
    "0.03  know\n",
    "0.026  see\n",
    "0.019  going\n",
    "0.018  even\n",
    "0.017  let\n",
    "0.017  much\n",
    "0.014  fuck\n",
    "0.012  made\n",
    "0.011  could\n",
    "0.01  thought\n",
    "0.01  trying\n",
    "0.01  old\n",
    "0.009  park\n",
    "\n",
    "\n",
    "Topic #7\n",
    "0.035  one\n",
    "0.022  think\n",
    "0.021  would\n",
    "0.014  also\n",
    "0.013  someone\n",
    "0.013  next\n",
    "0.011  trump\n",
    "0.011  week\n",
    "0.011  job\n",
    "0.01  tell\n",
    "0.01  mean\n",
    "0.01  many\n",
    "0.01  anyone\n",
    "0.01  talk\n",
    "0.01  everyone\n",
    "\n",
    "\n",
    "Topic #8\n",
    "0.025  lol\n",
    "0.018  life\n",
    "0.017  way\n",
    "0.016  man\n",
    "0.014  better\n",
    "0.013  nigga\n",
    "0.013  stop\n",
    "0.012  lmfao\n",
    "0.01  bad\n",
    "0.01  wanna\n",
    "0.01  start\n",
    "0.009  gotta\n",
    "0.009  find\n",
    "0.009  hard\n",
    "0.009  help\n",
    "\n",
    "\n",
    "Topic #9\n",
    "0.048  new\n",
    "0.036  york\n",
    "0.023  today\n",
    "0.022  ny\n",
    "0.02  look\n",
    "0.013  please\n",
    "0.013  city\n",
    "0.013  w\n",
    "0.012  game\n",
    "0.012  summer\n",
    "0.01  like\n",
    "0.01  video\n",
    "0.01  put\n",
    "0.009  hate\n",
    "0.009  whole\n",
    "\n",
    "\n",
    "Topic #10\n",
    "0.022  nyc\n",
    "0.02  go\n",
    "0.015  brooklyn\n",
    "0.014  guy\n",
    "0.014  oh\n",
    "0.013  girl\n",
    "0.012  thanks\n",
    "0.011  wait\n",
    "0.011  night\n",
    "0.01  yeah\n",
    "0.01  b\n",
    "0.009  always\n",
    "0.009  street\n",
    "0.008  manhattan\n",
    "0.008  photo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### As we can see, results are better when we use hashtags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hot topics in the USA from [Google trends](https://trends.google.com/trends/explore?geo=US)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check actual result of tweets data, we acquire google trends data by the specific location and the same period of time. So, send request to trends.google.com and got responce which contains top search topics and top search queries. \n",
    "There are used modified pytrends API to get get this data. We modified a little bit (add fuinctions to interface related_top_search_topics, related_top_search_queries) pytrends API to get google trends date by specifi date and timeframe, because there are not such functional in public API.\n",
    "<br>\n",
    "<br>\n",
    "pytrnds API: https://github.com/GeneralMills/pytrends\n",
    "<br>\n",
    "This repository was forked and changes saves in our public rep: https://github.com/DmytroBabenko/pytrends "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date_str = frame_start_datetime.strftime(\"%Y-%m-%d\")\n",
    "finish_date_str = frame_start_datetime.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "pytrend = TrendReq()\n",
    "pytrend.build_payload(kw_list=[' '], geo=geo, timeframe=f\"{start_date_str} {finish_date_str}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Search topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_df = pytrend.related_top_search_topics(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Google trends Search topics in New York during 2019-06-01\n",
      "+---------------------------------------------+---------------------------------------+\n",
      "|Search topics - rising                       |Search topics - top                    |\n",
      "+---------------------------------------------+---------------------------------------+\n",
      "|Liverpool F.C. - Football club               |New York - City in New York            |\n",
      "|Tottenham Hotspur F.C. - Football club       |New York - US State                    |\n",
      "|2018 UEFA Champions League Final - Tournament|2019 - Topic                           |\n",
      "|UEFA Champions League - Football competition |Weather - Topic                        |\n",
      "|Mega Millions - Topic                        |Film - Topic                           |\n",
      "|Virginia Beach - City in Virginia            |Facebook, Inc. - Social network company|\n",
      "|The Central Park Five - 2012 film            |Facebook - Social networking service   |\n",
      "|Sports league - Topic                        |YouTube - Video sharing company        |\n",
      "|Shooting - Topic                             |Google - Technology company            |\n",
      "|New York Yankees - Baseball team             |Amazon.com - E-commerce company        |\n",
      "|Gay pride - Topic                            |Restaurant - Topic                     |\n",
      "|Football - Sport                             |Google Search - Topic                  |\n",
      "|MLB - Baseball league                        |Brooklyn - New York City borough       |\n",
      "|Festival - Topic                             |Sales - Topic                          |\n",
      "|Month - Unit of time                         |New York Yankees - Baseball team       |\n",
      "+---------------------------------------------+---------------------------------------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_google_trend_title(frame_start_datetime, frame_start_datetime, \"Search topics\", geo)\n",
    "topics_df.select(\"Search topics - rising\", \"Search topics - top\").show(num_of_top_interest, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Search queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_df = pytrend.related_top_search_queries(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Google trends Search queries in New York during 2019-06-01\n",
      "+-----------------------+--------------------+--------+---+-----+\n",
      "|Search queries - rising|Search queries - top|Rising  |Top|geo  |\n",
      "+-----------------------+--------------------+--------+---+-----+\n",
      "|dewayne craddock       |you                 |Breakout|100|US-NY|\n",
      "|jose antonio reyes     |weather             |+3,350% |97 |US-NY|\n",
      "|dwayne craddock        |facebook            |+2,550% |63 |US-NY|\n",
      "|nelson figueroa        |google              |+1,750% |51 |US-NY|\n",
      "|liverpool vs tottenham |youtube             |+1,450% |46 |US-NY|\n",
      "|champions league final |amazon              |+1,100% |40 |US-NY|\n",
      "|tottenham              |news                |+1,000% |39 |US-NY|\n",
      "|champions league       |yankees             |+850%   |30 |US-NY|\n",
      "|liverpool              |champions           |+850%   |29 |US-NY|\n",
      "|champions              |lottery             |+650%   |27 |US-NY|\n",
      "|ucl final              |movies              |+600%   |26 |US-NY|\n",
      "|virginia beach shooter |craigslist          |+550%   |25 |US-NY|\n",
      "|mega millions          |champions league    |+500%   |24 |US-NY|\n",
      "|red sox vs yankees     |walmart             |+500%   |23 |US-NY|\n",
      "|eminem                 |liverpool           |+450%   |23 |US-NY|\n",
      "+-----------------------+--------------------+--------+---+-----+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_google_trend_title(frame_start_datetime, frame_start_datetime, \"Search queries\", geo)\n",
    "queries_df.show(num_of_top_interest, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation and justification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we considered the date 1 June, there was the final of Champions Leaque. Runnig LDA algorithm on only ** hashtags extracted from tweets **, we got several topics based on tweets, which are described above. In these topics we can find posts as **uclfinal2019**, **ucl**, ***championsleague2019***, ***liverpool***, ***championsleaguefinal***, ***tottenham***. Futhemore, google trends top search topics tell us about ***Liverpool F.C. - Football club***, ***Tottenham Hotspur F.C. - Football club***, ***UEFA Champions League - Football competition***, ***2018 UEFA Champions League Final - Tournament***, which so similar to tweets. Moreover, google seach queries for this date has ***liverpool vs tottenham***, ***tottenham***, ***liverpool***, ***champions league final***, ***uefa champions league final 2019***. It is clear to notice, that LDA tweets result are relevant to google trends for 1 June, when there was the final of Chamion League. <br>\n",
    "Besides, the day before 1 June, there was Virginia Beach shooting. And, LDA gives us some relevant tweet to this event - ***virginiabeachshooting***, which is acceptbale by google trends results: ***Virginia Beach - City in Virginia*** and ***virginia beach shooting suspect***. <br>\n",
    "Also, there are a lot of coincides about Gay Prode 2019, which was in June in New York. So, for this event we have sevaral tweets: ***pridemonth***, ***pride2019***, ***lgbtq***, ***loveislove***, ***gay***, ***love***, ***worldpride***. And relevant topics/queries from google trends - ***Gay pride - Topic***, ***pride month***.\n",
    "\n",
    "Overall, the tweets results are relevant to some popular events which are close to specific date."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating the results of the topic modelling, we can see that our approach highlights many of the current events of the day (01-Jun-2019) - competitions in boxing, mix fights competition, UEFA final; book conference in New York; beginning of the pride in New York. Comparing the results with the Google-trends, we can see overlaping. So we can say that the pipeline we developed is a working solution for the topic modeling problem.\n",
    "<br>As an option to improve the performance of our algorithm, we see the purchase of the full amount of Twitter data on the territory, and their processing on a full-fledged cluster. This will remove the effect of a random selection of tweets and increase the amount of data, which should make the result even better."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
