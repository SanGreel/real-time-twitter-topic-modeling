{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem statement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import all needed libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T11:07:06.153513Z",
     "start_time": "2019-07-05T11:07:06.137348Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/yevhenp/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/yevhenp/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'get_google_trends_data.pytrends.pytrends'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-85-4656a88656fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m#pytrends - for acquiring google trends\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mget_google_trends_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpytrends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpytrends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTrendReq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;31m# basically spark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'get_google_trends_data.pytrends.pytrends'"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "import operator\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import ArrayType, FloatType, StringType, IntegerType\n",
    "from pyspark.sql.functions import udf, row_number,column\n",
    "\n",
    "# processing\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "# text preprocessing\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from pyspark.ml.feature import CountVectorizer,StopWordsRemover, HashingTF, IDF, Tokenizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "#staff for LDA\n",
    "from pyspark.mllib.clustering import LDA, LDAModel\n",
    "from pyspark.mllib.linalg import Vector as oldVector, Vectors as oldVectors\n",
    "from pyspark.ml.linalg import Vector as newVector, Vectors as newVectors\n",
    "\n",
    "# import hardcoded variables\n",
    "from variables import channels_not_to_consider\n",
    "\n",
    "#for debug purpose only\n",
    "import time\n",
    "\n",
    "#pytrends - for acquiring google trends\n",
    "from get_google_trends_data.pytrends.pytrends.request import TrendReq\n",
    "\n",
    "# basically spark\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global variables definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**User-specific variables**  \n",
    "Please feel free to tweak those variables as you wish. For example, you can set number of last hours to get hottest topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T11:07:19.777989Z",
     "start_time": "2019-07-05T11:07:19.774380Z"
    }
   },
   "outputs": [],
   "source": [
    "# if True locations from locations_to_consider will be used to filter\n",
    "get_from_location = True\n",
    "\n",
    "# locations to filter relevant tweets\n",
    "locations_to_consider = [\n",
    "                         'Manhattan, NY', \n",
    "                         'Brooklyn, NY', \n",
    "                         'Queens, NY', \n",
    "                         'Bronx, NY', \n",
    "                         'Staten Island, NY'\n",
    "                         'New York, USA'\n",
    "                        ]\n",
    "\n",
    "number_of_hours_to_get_topics = 2\n",
    "num_of_top_interest = 15\n",
    "\n",
    "geo = \"US-NY\" #US for USA\n",
    "\n",
    "# Set window time for interesting\n",
    "frame_start_datetime = \"Mon Jun 03 00:00:00 +0000 2019\"\n",
    "frame_finish_datetime = \"Mon Jun 17 23:00:00 +0000 2019\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Technical variables**  \n",
    "Those variables are needed to connect to db and other technical stuff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T11:07:59.784011Z",
     "start_time": "2019-07-05T11:07:59.780870Z"
    }
   },
   "outputs": [],
   "source": [
    "# LDA params\n",
    "num_of_topics_LDA = 10\n",
    "max_iterations_LDA = 100\n",
    "nomber_of_words_to_for_topic = 15  # number of words per topic\n",
    "\n",
    "# path to CSV\n",
    "# historical_tweets_data = './new_york_training_tweets_15_06.csv'\n",
    "historical_tweets_data = './get-tweets-by-geolocation/training_tweets.csv'\n",
    "# MongoDB table\n",
    "real_time_tweets_table = \"usa_training_tweets_04_07.training_tweets_collection\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T11:08:00.780762Z",
     "start_time": "2019-07-05T11:08:00.769434Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at <ipython-input-12-73ad000ecbc7>:1 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-99-73ad000ecbc7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    131\u001b[0m                     \" note this option will be removed in Spark 3.0\")\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    330\u001b[0m                         \u001b[0;34m\" created by %s at %s:%s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[0;32m--> 332\u001b[0;31m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[1;32m    333\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at <ipython-input-12-73ad000ecbc7>:1 "
     ]
    }
   ],
   "source": [
    "sc = SparkContext()\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handy functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Text preprocessing and filtering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T11:08:01.993067Z",
     "start_time": "2019-07-05T11:08:01.985747Z"
    }
   },
   "outputs": [],
   "source": [
    "def filter_tweet(tweet, channels_not_to_consider):\n",
    "    \n",
    "    if not isinstance(tweet, str):\n",
    "        is_filtered = True\n",
    "    elif len(tweet.split(' ')) < 3:\n",
    "        is_filtered = True\n",
    "    else: \n",
    "        is_filtered = False\n",
    "        \n",
    "    return not is_filtered\n",
    "         \n",
    "def process_tweet(tweet):\n",
    "   \n",
    "    tweet = tweet.lower() # get lowercase\n",
    "    tweet = re.sub(r'@\\w+', '', tweet) # filter words with non-letters at the beginning (mainly for mentions)\n",
    "    tweet = re.sub(r'http://\\S{,280}', '', tweet) # filter http\n",
    "    tweet = re.sub(r'https://\\S{,280}', '', tweet) # filter https\n",
    "    tweet = re.sub(r'[^A-Za-z]', ' ', tweet) # filter all non-letters\n",
    "    tweet = re.sub(r'\\s{2,}', ' ', tweet) # remove multiply whitespaces\n",
    "    tweet = re.sub(r'(.)\\1{2,}', r'\\1', tweet) # remove repeated chars (e.g. \"greeeeat\" -> \"great\")\n",
    "    tweet = tweet.strip() # remove possible whitespaces from both sides of the tweet\n",
    "\n",
    "    # lemmatize, tokenize and conquer\n",
    "    processed_tweet = [lemmatizer.lemmatize(token) for token in tokenizer.tokenize(tweet)\n",
    "                       if token not in stop_word_list]\n",
    "    \n",
    "    return processed_tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Datetime handling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T11:08:02.679651Z",
     "start_time": "2019-07-05T11:08:02.673631Z"
    }
   },
   "outputs": [],
   "source": [
    "wrong_date = datetime.strptime(\"Mon Jun 03 00:00:00 +0000 2000\", '%a %b %d %H:%M:%S %z %Y')\n",
    "\n",
    "def validate(date_text):\n",
    "    try:\n",
    "        if date_text != datetime.strptime(date_text, '%a %b %d %H:%M:%S %z %Y').strftime('%a %b %d %H:%M:%S %z %Y'):\n",
    "            raise ValueError\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "def str_tweet_to_datetime(frame_datetime):\n",
    "    if (validate(frame_datetime) == True):\n",
    "        return datetime.strptime(frame_datetime,'%a %b %d %H:%M:%S %z %Y')\n",
    "    else:\n",
    "        return wrong_date\n",
    "\n",
    "def datetime_to_tweet_str(frame_datetime):\n",
    "    #print(type(frame_datetime))\n",
    "    ts = datetime.strftime(frame_datetime, '%a %b %d %H:%M:%S %z %Y')\n",
    "    return ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to call this block with functions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T11:08:03.411921Z",
     "start_time": "2019-07-05T11:08:03.407987Z"
    }
   },
   "outputs": [],
   "source": [
    "def tweet2google_timeframe(frame_start_datetime, frame_finish_datetime):\n",
    "    start_date = str_tweet_to_datetime(frame_start_datetime)\n",
    "    end_date = str_tweet_to_datetime(frame_finish_datetime)\n",
    "    tim\n",
    "    \n",
    "def get_google_trends_by_geo(geo):\n",
    "    if geo == 'US':\n",
    "        return google_trends_search_topics_us, google_trends_search_queries_us\n",
    "    elif geo == 'US-NY':\n",
    "        return google_trends_search_topics_us_ny, google_trends_search_queries_us_ny\n",
    "    \n",
    "    return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T11:08:03.838250Z",
     "start_time": "2019-07-05T11:08:03.832579Z"
    }
   },
   "outputs": [],
   "source": [
    "#TODO: move this function to utils\n",
    "def str_rising_to_float(str):\n",
    "    if str is None:\n",
    "        return 0.0\n",
    "    if str == '':\n",
    "        return 0.0\n",
    "    if str == 'Breakout':\n",
    "        return 0.0\n",
    "    \n",
    "    str_value = str.split('%')[0]\n",
    "    if '+' in str_value:\n",
    "        str_value = str_value.split('+')[1]\n",
    "        \n",
    "    if ',' in str_value:\n",
    "        str_value = str_value.replace(',', '.')\n",
    "        value = 1000* float(str_value)\n",
    "        return value\n",
    "    return float(str_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T11:08:04.312224Z",
     "start_time": "2019-07-05T11:08:04.297695Z"
    }
   },
   "outputs": [],
   "source": [
    "#TODO: move this function to utils\n",
    "def unique_google_trends_by_time_frame(df):\n",
    "    data = df.collect()\n",
    "    rising_dict = {}\n",
    "    top_dict = {}\n",
    "    \n",
    "    geo = data[0]['geo']\n",
    "    columns = df.columns\n",
    "\n",
    "    for i in range(0, len(data)):\n",
    "        rising_val = data[i][columns[1]]\n",
    "        top_value = data[i][columns[2]]\n",
    "        \n",
    "        if rising_val in rising_dict:\n",
    "            rising_dict[rising_val][0] += str_rising_to_float(data[i][columns[3]])\n",
    "            rising_dict[rising_val][1] += 1\n",
    "        else:\n",
    "            rising_dict[rising_val] = [str_rising_to_float(data[i][columns[3]]), 1]\n",
    "            \n",
    "        if top_value in top_dict:\n",
    "            top_dict[top_value][0] += float(data[i][columns[4]])\n",
    "            top_dict[top_value][1] += 1\n",
    "        else:\n",
    "            top_dict[top_value] = [float(data[i][columns[4]]), 1]\n",
    "    \n",
    "    \n",
    "    for key in top_dict:\n",
    "        top_dict[key] = round(top_dict[key][0] / top_dict[key][1])\n",
    "        \n",
    "    for key in rising_dict:\n",
    "        rising_dict[key] = round(rising_dict[key][0] / rising_dict[key][1])\n",
    "    \n",
    "    top_dict = sorted(top_dict.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    rising_dict = sorted(rising_dict.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    \n",
    "    \n",
    "    seq = []\n",
    "    len_top = len(top_dict)\n",
    "    len_rising = len(rising_dict)\n",
    "    length = max(len_top, len_rising)\n",
    "    \n",
    "    row = Row(columns[1], columns[2], columns[3], columns[4], columns[5])\n",
    "    \n",
    "    for i in range(0, length):\n",
    "        rising = rising_dict[i][0] if i < len_rising else ''\n",
    "        rising_val = f\"+{rising_dict[i][1]}%\" if i < len_rising else None\n",
    "        \n",
    "        top = top_dict[i][0] if i < len_top else ''\n",
    "        top_val = top_dict[i][1] if i < len_top else None\n",
    "        \n",
    "        seq.append(row(rising, top, rising_val, top_val, geo))\n",
    "    \n",
    "    dframe = spark.createDataFrame(seq)\n",
    "    return dframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T11:08:05.104399Z",
     "start_time": "2019-07-05T11:08:05.099856Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_geo_name(geo):\n",
    "    if geo == \"US-NY\":\n",
    "        return \"New York\"\n",
    "    elif geo == \"US\":\n",
    "        return \"United States\"\n",
    "    return \"\"\n",
    "\n",
    "def print_google_trend_title(start_date, finish_date, name):\n",
    "    start_date_str = start_date.strftime(\"%Y-%m-%d\")\n",
    "    if start_date == finish_date:\n",
    "        print(f\"\\nGoogle trends {name} in {get_geo_name(geo)} during {start_date_str}\")\n",
    "    else:\n",
    "        finish_date_str = finish_date.strftime(\"%Y-%m-%d\")\n",
    "        print(f\"\\nGoogle trends {name} in {get_geo_name(geo)} during {start_date_str} - {finish_date_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T11:08:05.711904Z",
     "start_time": "2019-07-05T11:08:05.706997Z"
    }
   },
   "outputs": [],
   "source": [
    "def convert_datetime_in_interesting_google(df):\n",
    "    columns = df.columns\n",
    "    converted_df = df.rdd.map(lambda x : (\n",
    "                                          x[\"Date\"].strftime(\"%Y-%m-%d\"), \n",
    "                                          x[columns[1]], \n",
    "                                          x[columns[2]], \n",
    "                                          x[columns[3]],\n",
    "                                          x[columns[4]],\n",
    "                                          x[columns[5]])).toDF([columns[0], columns[1], columns[2], columns[3], columns[4], columns[5]])\n",
    "                                                \n",
    "    return converted_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here should be \"magic IF\" (Yevhen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T11:08:07.458614Z",
     "start_time": "2019-07-05T11:08:07.449307Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of usage!\n",
      "Range for csv:  2019-06-03 00:00:00+00:00 2019-06-17 23:00:00+00:00\n",
      "Time range for mongodb:  None None\n"
     ]
    }
   ],
   "source": [
    "def get_history_and_real_timeframe(requested_start, requested_finish):\n",
    "\n",
    "    requested_start_dt = str_tweet_to_datetime(requested_start)\n",
    "    requested_finish_dt = str_tweet_to_datetime(requested_finish)\n",
    "    \n",
    "    const_end_history_datetime = str_tweet_to_datetime(\"Fri Jul 05 00:00:00 +0000 2019\")\n",
    "\n",
    "    history_start_datetime = None\n",
    "    history_finish_datetime = None\n",
    "    realtime_start_datetime = None\n",
    "    realtime_finish_datetime = None\n",
    "\n",
    "    assert requested_finish_dt > requested_start_dt, \"Finish dataframe MUST be greater than start\"\n",
    "\n",
    "    if (requested_start_dt >= const_end_history_datetime and requested_finish_dt > const_end_history_datetime):\n",
    "        realtime_start_datetime = requested_start_dt\n",
    "        realtime_finish_datetime = requested_finish_dt\n",
    "    elif (requested_start_dt < const_end_history_datetime and requested_finish_dt <= const_end_history_datetime):\n",
    "        history_start_datetime = requested_start_dt\n",
    "        history_finish_datetime = requested_finish_dt\n",
    "    else:\n",
    "        history_start_datetime = requested_start_dt\n",
    "        history_finish_datetime = const_end_history_datetime\n",
    "        realtime_start_datetime = const_end_history_datetime\n",
    "        realtime_finish_datetime = requested_finish_dt\n",
    "        \n",
    "    return (history_start_datetime, history_finish_datetime, realtime_start_datetime, realtime_finish_datetime)\n",
    "\n",
    "print('Example of usage!')\n",
    "times = get_history_and_real_timeframe(requested_start = frame_start_datetime, \n",
    "                                       requested_finish = frame_finish_datetime)\n",
    "\n",
    "print(\"Range for csv: \", times[0], times[1])\n",
    "print(\"Time range for mongodb: \", times[2], times[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the historical data, it can take a while"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T11:08:09.999170Z",
     "start_time": "2019-07-05T11:08:09.288960Z"
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.csv(historical_tweets_data, inferSchema=True, header=True)\n",
    "# remove records with no date\n",
    "df = df.na.drop(subset=[\"created_at\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T11:08:12.183890Z",
     "start_time": "2019-07-05T11:08:12.161648Z"
    }
   },
   "outputs": [],
   "source": [
    "# convert string to desired date format\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import DateType, TimestampType\n",
    "\n",
    "func =  udf (lambda x: str_tweet_to_datetime(x), TimestampType())\n",
    "\n",
    "df = df.withColumn('created_at', func(col('created_at')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T11:08:13.033480Z",
     "start_time": "2019-07-05T11:08:12.981145Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Range for collected data (history):  2019-06-03 00:00:00+00:00 2019-06-17 23:00:00+00:00\n"
     ]
    }
   ],
   "source": [
    "# select history according to user's time request\n",
    "\n",
    "times = get_history_and_real_timeframe(requested_start = frame_start_datetime, \n",
    "                                       requested_finish = frame_finish_datetime)\n",
    "historical_start_time = times[0]\n",
    "historical_finish_time = times[1]\n",
    "\n",
    "print(\"Range for collected data (history): \", historical_start_time, historical_finish_time)\n",
    "\n",
    "selected_history = None\n",
    "\n",
    "if historical_start_time != None and historical_finish_time != None:\n",
    "    selected_history = df.filter((df.created_at > historical_start_time) & (df.created_at < historical_finish_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Can we optimize this??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting to MongoDB for the real-time data\n",
    "**Creating Spark connector**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"pipeline\") \\\n",
    "    .config('spark.mongodb.input.uri', 'mongodb://localhost:27017/'+real_time_tweets_table) \\\n",
    "    .config('spark.mongodb.output.uri', 'mongodb://localhost:27017/'+real_time_tweets_table) \\\n",
    "    .config('spark.jars.packages', 'org.mongodb.spark:mongo-spark-connector_2.11:2.3.1') \\\n",
    "    .config('spark.mongodb.input.partitioner', 'MongoPaginateBySizePartitioner') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem №1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o89.load.\n: java.lang.ClassNotFoundException: Failed to find data source: com.mongodb.spark.sql.DefaultSource. Please find packages at http://spark.apache.org/third-party-projects.html\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:657)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:194)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:167)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.ClassNotFoundException: com.mongodb.spark.sql.DefaultSource.DefaultSource\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:382)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$20$$anonfun$apply$12.apply(DataSource.scala:634)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$20$$anonfun$apply$12.apply(DataSource.scala:634)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$20.apply(DataSource.scala:634)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$20.apply(DataSource.scala:634)\n\tat scala.util.Try.orElse(Try.scala:84)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:634)\n\t... 13 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-73397b9f116d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"com.mongodb.spark.sql.DefaultSource\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    170\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o89.load.\n: java.lang.ClassNotFoundException: Failed to find data source: com.mongodb.spark.sql.DefaultSource. Please find packages at http://spark.apache.org/third-party-projects.html\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:657)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:194)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:167)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.ClassNotFoundException: com.mongodb.spark.sql.DefaultSource.DefaultSource\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:382)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$20$$anonfun$apply$12.apply(DataSource.scala:634)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$20$$anonfun$apply$12.apply(DataSource.scala:634)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$20.apply(DataSource.scala:634)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$20.apply(DataSource.scala:634)\n\tat scala.util.Try.orElse(Try.scala:84)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:634)\n\t... 13 more\n"
     ]
    }
   ],
   "source": [
    "df2 = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- tweet: string (nullable = true)\n",
      " |-- country_code: string (nullable = true)\n",
      " |-- geo_location: string (nullable = true)\n",
      " |-- bounding_box: string (nullable = true)\n",
      " |-- screen_name: string (nullable = true)\n",
      " |-- favourites_count: string (nullable = true)\n",
      " |-- followers_count: string (nullable = true)\n",
      " |-- statuses_count: string (nullable = true)\n",
      " |-- friends_count: string (nullable = true)\n",
      " |-- listed_count: string (nullable = true)\n",
      " |-- user_described_location: string (nullable = true)\n",
      " |-- created_at: timestamp (nullable = true)\n",
      " |-- utc_offset: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"recent_data\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "\"cannot resolve '`text`' given input columns: [recent_data.listed_count, recent_data.created_at, recent_data.utc_offset, recent_data.tweet, recent_data.friends_count, recent_data.user_described_location, recent_data.screen_name, recent_data.statuses_count, recent_data.followers_count, recent_data.favourites_count, recent_data.geo_location, recent_data.country_code, recent_data.bounding_box]; line 1 pos 7;\\n'GlobalLimit 10\\n+- 'LocalLimit 10\\n   +- 'Project ['text]\\n      +- SubqueryAlias `recent_data`\\n         +- Project [tweet#10, country_code#11, geo_location#12, bounding_box#13, screen_name#14, favourites_count#15, followers_count#16, statuses_count#17, friends_count#18, listed_count#19, user_described_location#20, <lambda>(created_at#21) AS created_at#50, utc_offset#22]\\n            +- Filter AtLeastNNulls(n, created_at#21)\\n               +- Relation[tweet#10,country_code#11,geo_location#12,bounding_box#13,screen_name#14,favourites_count#15,followers_count#16,statuses_count#17,friends_count#18,listed_count#19,user_described_location#20,created_at#21,utc_offset#22] csv\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o23.sql.\n: org.apache.spark.sql.AnalysisException: cannot resolve '`text`' given input columns: [recent_data.listed_count, recent_data.created_at, recent_data.utc_offset, recent_data.tweet, recent_data.friends_count, recent_data.user_described_location, recent_data.screen_name, recent_data.statuses_count, recent_data.followers_count, recent_data.favourites_count, recent_data.geo_location, recent_data.country_code, recent_data.bounding_box]; line 1 pos 7;\n'GlobalLimit 10\n+- 'LocalLimit 10\n   +- 'Project ['text]\n      +- SubqueryAlias `recent_data`\n         +- Project [tweet#10, country_code#11, geo_location#12, bounding_box#13, screen_name#14, favourites_count#15, followers_count#16, statuses_count#17, friends_count#18, listed_count#19, user_described_location#20, <lambda>(created_at#21) AS created_at#50, utc_offset#22]\n            +- Filter AtLeastNNulls(n, created_at#21)\n               +- Relation[tweet#10,country_code#11,geo_location#12,bounding_box#13,screen_name#14,favourites_count#15,followers_count#16,statuses_count#17,friends_count#18,listed_count#19,user_described_location#20,created_at#21,utc_offset#22] csv\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:110)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:107)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:278)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:278)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:277)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$2.apply(QueryPlan.scala:121)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.immutable.List.map(List.scala:296)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:121)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:107)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:85)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:85)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:642)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-04b211e7f872>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrecent_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'SELECT text FROM recent_data LIMIT 10'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#WHERE lang=\"en\" AND place.country=\"US\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    708\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m         \"\"\"\n\u001b[0;32m--> 710\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: \"cannot resolve '`text`' given input columns: [recent_data.listed_count, recent_data.created_at, recent_data.utc_offset, recent_data.tweet, recent_data.friends_count, recent_data.user_described_location, recent_data.screen_name, recent_data.statuses_count, recent_data.followers_count, recent_data.favourites_count, recent_data.geo_location, recent_data.country_code, recent_data.bounding_box]; line 1 pos 7;\\n'GlobalLimit 10\\n+- 'LocalLimit 10\\n   +- 'Project ['text]\\n      +- SubqueryAlias `recent_data`\\n         +- Project [tweet#10, country_code#11, geo_location#12, bounding_box#13, screen_name#14, favourites_count#15, followers_count#16, statuses_count#17, friends_count#18, listed_count#19, user_described_location#20, <lambda>(created_at#21) AS created_at#50, utc_offset#22]\\n            +- Filter AtLeastNNulls(n, created_at#21)\\n               +- Relation[tweet#10,country_code#11,geo_location#12,bounding_box#13,screen_name#14,favourites_count#15,followers_count#16,statuses_count#17,friends_count#18,listed_count#19,user_described_location#20,created_at#21,utc_offset#22] csv\\n\""
     ]
    }
   ],
   "source": [
    "recent_data = spark.sql('SELECT text FROM recent_data LIMIT 10') #WHERE lang=\"en\" AND place.country=\"US\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recent_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T10:21:41.712358Z",
     "start_time": "2019-07-05T10:21:41.706636Z"
    }
   },
   "outputs": [],
   "source": [
    "# select recent data according to user's time request\n",
    "\n",
    "times = get_history_and_real_timeframe(requested_start = frame_start_datetime, \n",
    "                                       requested_finish = frame_finish_datetime)\n",
    "recent_start_time = times[2]\n",
    "recent_finish_time = times[3]\n",
    "\n",
    "print(\"Range for recent data (mongodb): \", recent_start_time, recent_finish_time)\n",
    "\n",
    "selected_recent = None\n",
    "\n",
    "if recent_start_time != None and recent_finish_time != None:\n",
    "    selected_recent = df.filter((recent_data.created_at > historical_start_time) \n",
    "                                & (recent_data.created_at < historical_finish_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T11:16:35.087709Z",
     "start_time": "2019-07-05T11:16:35.083709Z"
    }
   },
   "outputs": [],
   "source": [
    "# merge together selected_recent and selected_history\n",
    "\n",
    "selected_df = None\n",
    "\n",
    "if selected_history != None and selected_recent != None:\n",
    "    selected_df = selected_history.union(selected_recent)\n",
    "elif selected_history != None and selected_recent == None:\n",
    "    selected_df = selected_history\n",
    "elif selected_history != None and selected_recent == None:\n",
    "    selected_df = selected_recent\n",
    "\n",
    "assert selected_df != None, \"Something goes wrong with selecting data from recent data/history data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = selected_history #spark.read.csv(historical_tweets_data, header=True, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.WordPunctTokenizer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_word_list = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter nans\n",
    "df = df.rdd.filter(lambda x: x[0] != None and x[1] != None and x[2] != None and x[4] != None)\n",
    "\n",
    "# filter out channels not to consider\n",
    "df = df.filter(lambda x: x[4] not in channels_not_to_consider)\n",
    "\n",
    "# filter by country\n",
    "df = df.filter(lambda x: x[1] in 'US')\n",
    "\n",
    "# filter by precise location\n",
    "if get_from_location:\n",
    "    df = df.filter(lambda x: x[2] in locations_to_consider)\n",
    "\n",
    "# filter tweet itself\n",
    "df = df.filter(lambda x: filter_tweet(x[0]))\n",
    "\n",
    "# process tweet\n",
    "df = df.map(lambda x: process_tweet(x[0]))\n",
    "\n",
    "# final preprocesssing\n",
    "df = df.filter(lambda x: len(x) > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here should be Main dataframe already filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Google Trends data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_trends_search_queries_us = spark.read.csv('data/google-trends/google-trends-search-queries-US.csv', inferSchema=True, header=True)\n",
    "google_trends_search_topics_us = spark.read.csv('data/google-trends/google-trends-search-topics-US.csv', inferSchema=True, header=True)\n",
    "google_trends_search_queries_us_ny = spark.read.csv('data/google-trends/google-trends-search-queries-US-NY.csv', inferSchema=True, header=True)\n",
    "google_trends_search_topics_us_ny = spark.read.csv('data/google-trends/google-trends-search-topics-US-NY.csv', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: move this function to Handy function block \n",
    "def get_google_trends_by_geo(geo):\n",
    "    if geo == 'US':\n",
    "        return google_trends_search_topics_us, google_trends_search_queries_us\n",
    "    elif geo == 'US-NY':\n",
    "        return google_trends_search_topics_us_ny, google_trends_search_queries_us_ny\n",
    "    \n",
    "    return None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WHERE is our data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data description (Женя)\n",
    "In order to collect data in a natural way:\n",
    "<br>- we registered Twitter Developer account;\n",
    "<br>- using credentials from Twitter Developer account we run script that collected tweets by the geolocation and saved them in mongodb;\n",
    "<br>\n",
    "<br><b>As a result:</b>\n",
    "<br>- we collected  332548 tweets (10Gb in mongodb, ~100Mb in csv) from New-York geolocation since 30 of May up to 15 of June;\n",
    "<br>- we collected  6617029 tweets (~1.69Gb in csv) from USA geolocation since 15 of June up to now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show Raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing\n",
    "# description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "variables": {
     " frame_start_datetime ": {},
     "-frame_start_datetime-": {},
     "frame_finish_datetime": {}
    }
   },
   "source": [
    "#### Selecting data from {{ frame_start_datetime }} to {{frame_finish_datetime}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "StopWords = stopwords.words(\"english\")\n",
    "\n",
    "tokens = df.select('text').rdd \\\n",
    "    .map(lambda x: x[0]) \\\n",
    "    .map( lambda document: re.split(\" \", document))          \\\n",
    "    .map( lambda word: [x for x in word if x.isalpha()])           \\\n",
    "    .map( lambda word: [x for x in word if len(x) > 3] )           \\\n",
    "    .map( lambda word: [x for x in word if x not in StopWords])    \\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show preprocessed data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic modeling/Latent Dirichlet allocation(LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this block can be commented, it's just a mock\n",
    "\n",
    "text_file = 'data/listings.csv'\n",
    "df = spark.read.csv(text_file, inferSchema=True, header=True)\n",
    "df = df.select(\"id\", \"name\").dropna(subset=\"name\")\n",
    "\n",
    "print(time.strftime('%m%d%Y %H:%M:%S'))\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"name\", outputCol=\"tokens\")\n",
    "df = tokenizer.transform(df)\n",
    "print(time.strftime('%m%d%Y %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(time.strftime('%m%d%Y %H:%M:%S'))\n",
    "\n",
    "cv = CountVectorizer(inputCol=\"tokens\", outputCol=\"raw_features\", vocabSize=5000, minDF=3.0)\n",
    "cvmodel = cv.fit(df)\n",
    "\n",
    "print(time.strftime('%m%d%Y %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(time.strftime('%m%d%Y %H:%M:%S'))\n",
    "df = cvmodel.transform(df)\n",
    "print(time.strftime('%m%d%Y %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"tf_idf_features\", minDocFreq=2)\n",
    "idfModel = idf.fit(df)\n",
    "\n",
    "df = idfModel.transform(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(10, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.drop(\"name\")\n",
    "#df.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = Window().orderBy(column(\"id\"))\n",
    "df = df.withColumn(\"id\", row_number().over(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = df.rdd.map(lambda x: (x[0], oldVectors.fromML(x[4])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_df = rs.toDF()\n",
    "rs_df.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the LDA Topic Modeler\n",
    "# Note the time before and after is printed in order to find out how much time it takes to process x number of records\n",
    "\n",
    "print(time.strftime('%m%d%Y %H:%M:%S'))\n",
    "lda_model = LDA.train(rs_df['_1', '_2'].rdd.map(list), k=num_of_topics_LDA, maxIterations=max_iterations_LDA)\n",
    "print(time.strftime('%m%d%Y %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(time.strftime('%m%d%Y %H:%M:%S'))\n",
    "topics = lda_model.topicsMatrix()\n",
    "vocabArray = cvmodel.vocabulary\n",
    "\n",
    "topicIndices = sc.parallelize(lda_model.describeTopics(maxTermsPerTopic = wordNumbers))\n",
    "\n",
    "def topic_render(topic):  # specify vector id of words to actual words\n",
    "    terms = topic[0]\n",
    "    prob = topic[1]\n",
    "    \n",
    "    result = []\n",
    "    for i in range(nomber_of_words_to_for_topic):\n",
    "        term = str(round(prob[i],3))+\"  \"+vocabArray[terms[i]]\n",
    "        result.append(term)\n",
    "    return result\n",
    "print(time.strftime('%m%d%Y %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(time.strftime('%m%d%Y %H:%M:%S'))\n",
    "topics_final = topicIndices.map(lambda topic:topic_render(topic)).collect()\n",
    "print(time.strftime('%m%d%Y %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# based on the simple vectors(+number of words)\n",
    "\n",
    "for topic in range(len(topics_final)):\n",
    "    print (\"Topic #\" + str(topic+1) + \"\")\n",
    "    for term in topics_final[topic]:\n",
    "        print (term)\n",
    "    print ('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#based on the tf-idf\n",
    "\n",
    "for topic in range(len(topics_final)):\n",
    "    print (\"Topic #\" + str(topic+1) + \"\")\n",
    "    for term in topics_final[topic]:\n",
    "        print (term)\n",
    "    print ('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hot topics in the USA from [Google trends](https://trends.google.com/trends/explore?geo=US)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = str_tweet_to_datetime(frame_start_datetime)\n",
    "finish_date = str_tweet_to_datetime(frame_finish_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_trends_topics, google_trends_queries = get_google_trends_by_geo(geo) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Google trends search queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "interesting_google_topics = google_trends_topics.filter(\n",
    "    (google_trends_topics.Date >= start_date) & (google_trends_topics.Date <= finish_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Google trends Search topics in New York during 2019-06-27 - 2019-06-30\n",
      "+----------+--------------------------------------------------------+----------------------------------------+\n",
      "|Date      |Search topics - rising                                  |Search topics - top                     |\n",
      "+----------+--------------------------------------------------------+----------------------------------------+\n",
      "|2019-06-28|Kamala Harris - United States Senator                   |New York - City in New York             |\n",
      "|2019-06-28|Marianne Williamson - American author                   |New York - US State                     |\n",
      "|2019-06-28|United States women's national soccer team - Soccer team|2019 - Topic                            |\n",
      "|2019-06-28|Pete Buttigieg - Mayor of South Bend                    |Weather - Topic                         |\n",
      "|2019-06-28|Joe Biden - Former Vice President of the United States  |Google Search - Topic                   |\n",
      "|2019-06-28|FIFA Women's World Cup - Football competition           |YouTube - Video sharing company         |\n",
      "|2019-06-28|World Cup - Football competition                        |Google - Technology company             |\n",
      "|2019-06-28|2019 FIFA Women's World Cup - Event                     |Facebook - Social networking service    |\n",
      "|2019-06-28|France - Country in Europe                              |Facebook, Inc. - Social network company |\n",
      "|2019-06-28|Colombia - Country in South America                     |United States - Country in North America|\n",
      "|2019-06-28|Copa América - Football championship                    |Film - Topic                            |\n",
      "|2019-06-28|2019 Copa América - Tournament                          |Restaurant - Topic                      |\n",
      "|2019-06-28|Brazil - Country in South America                       |Amazon.com - E-commerce company         |\n",
      "|2019-06-28|Football - Sport                                        |Brooklyn - New York City borough        |\n",
      "|2019-06-28|Pride parade - Topic                                    |Car - Transportation mode               |\n",
      "+----------+--------------------------------------------------------+----------------------------------------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_google_trend_title(start_date, finish_date, \"Search topics\")\n",
    "interest_google_topics = convert_datetime_in_interesting_google(interesting_google_topics)\n",
    "interest_google_topics.select(\"Date\",\"Search topics - rising\", \"Search topics - top\").show(num_of_top_interest, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case when timeframe is more than 1 day, filter correctly this google-trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Google trends Search topics in New York during 2019-06-27 - 2019-06-30\n",
      "+--------------------------------------------------------+----------------------------------------+\n",
      "|Search topics - rising                                  |Search topics - top                     |\n",
      "+--------------------------------------------------------+----------------------------------------+\n",
      "|Kamala Harris - United States Senator                   |New York - City in New York             |\n",
      "|Marianne Williamson - American author                   |New York - US State                     |\n",
      "|Brooklyn Nets - Basketball team                         |2019 - Topic                            |\n",
      "|United States women's national soccer team - Soccer team|Weather - Topic                         |\n",
      "|Pete Buttigieg - Mayor of South Bend                    |YouTube - Video sharing company         |\n",
      "|Joe Biden - Former Vice President of the United States  |Film - Topic                            |\n",
      "|Kevin Durant - American basketball player               |Google Search - Topic                   |\n",
      "|Boston Red Sox - Baseball team                          |Google - Technology company             |\n",
      "|Radar - Topic                                           |Facebook - Social networking service    |\n",
      "|Free agent - Topic                                      |Facebook, Inc. - Social network company |\n",
      "|FIFA Women's World Cup - Football competition           |United States - Country in North America|\n",
      "|World Cup - Football competition                        |Restaurant - Topic                      |\n",
      "|2019 FIFA Women's World Cup - Event                     |The Weather Channel - Television channel|\n",
      "|France - Country in Europe                              |Amazon.com - E-commerce company         |\n",
      "|Colombia - Country in South America                     |Brooklyn - New York City borough        |\n",
      "+--------------------------------------------------------+----------------------------------------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "interesing_google_topics_unique= unique_google_trends_by_time_frame(interesting_google_topics)\n",
    "print_google_trend_title(start_date, finish_date, \"Search topics\")\n",
    "interesing_google_topics_unique.select(\"Search topics - rising\", \"Search topics - top\").show(num_of_top_interest, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Google trends search queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "interesting_google_queries = google_trends_queries.filter(\n",
    "    (google_trends_queries.Date >= start_date) & (google_trends_queries.Date <= finish_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Google trends Search queries in New York during 2019-06-27 - 2019-06-30\n",
      "+----------+---------------------------+--------------------+\n",
      "|Date      |Search queries - rising    |Search queries - top|\n",
      "+----------+---------------------------+--------------------+\n",
      "|2019-06-28|shay mitchell              |weather             |\n",
      "|2019-06-28|marianne williamson        |google              |\n",
      "|2019-06-28|kamala harris              |facebook            |\n",
      "|2019-06-28|argentina vs venezuela 2019|youtube             |\n",
      "|2019-06-28|usa france                 |world cup           |\n",
      "|2019-06-28|brazil vs paraguay         |news                |\n",
      "|2019-06-28|usa vs france              |amazon              |\n",
      "|2019-06-28|colombia vs chile          |copa america        |\n",
      "|2019-06-28|alex morgan                |debate              |\n",
      "|2019-06-28|michael bennet             |instagram           |\n",
      "|2019-06-28|argentina vs venezuela     |craigslist          |\n",
      "|2019-06-28|megan rapinoe              |walmart             |\n",
      "|2019-06-28|argentina                  |yahoo               |\n",
      "|2019-06-28|eric swalwell              |map                 |\n",
      "|2019-06-28|andrew yang                |lottery             |\n",
      "+----------+---------------------------+--------------------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_google_trend_title(start_date, finish_date, \"Search queries\")\n",
    "interest_google_queries = convert_datetime_in_interesting_google(interesting_google_queries)\n",
    "interest_google_queries.select(\"Date\", \"Search queries - rising\", \"Search queries - top\").show(num_of_top_interest, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Google trends Search queries in New York during 2019-06-27 - 2019-06-30\n",
      "+---------------------------+--------------------+------+---+-----+\n",
      "|Search queries - rising    |Search queries - top|Rising|Top|geo  |\n",
      "+---------------------------+--------------------+------+---+-----+\n",
      "|yy                         |weather             |+4950%|100|US-NY|\n",
      "|shay mitchell              |pride               |+3700%|62 |US-NY|\n",
      "|darren collison            |facebook            |+2950%|59 |US-NY|\n",
      "|marianne williamson        |google              |+2750%|55 |US-NY|\n",
      "|kamala harris              |youtube             |+2400%|48 |US-NY|\n",
      "|deandre jordan             |news                |+2400%|44 |US-NY|\n",
      "|india vs england           |amazon              |+1050%|44 |US-NY|\n",
      "|argentina vs venezuela 2019|world cup           |+900% |42 |US-NY|\n",
      "|usa france                 |debate              |+900% |35 |US-NY|\n",
      "|brazil vs paraguay         |yankees             |+900% |34 |US-NY|\n",
      "|usa vs france              |pride parade        |+900% |30 |US-NY|\n",
      "|colombia vs chile          |yahoo               |+800% |28 |US-NY|\n",
      "|kevin durant               |nba                 |+800% |27 |US-NY|\n",
      "|alex morgan                |copa america        |+750% |26 |US-NY|\n",
      "|michael bennet             |instagram           |+700% |26 |US-NY|\n",
      "+---------------------------+--------------------+------+---+-----+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "interesing_google_queries_unique= unique_google_trends_by_time_frame(interesting_google_queries)\n",
    "print_google_trend_title(start_date, finish_date, \"Search queries\")\n",
    "interesing_google_queries_unique.show(num_of_top_interest, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hot topics - google trends (directly) (probably this will be removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date_str = start_date.strftime(\"%Y-%m-%d\")\n",
    "finish_date_str = finish_date.strftime(\"%Y-%m-%d\")\n",
    "pytrend = TrendReq()\n",
    "pytrend.build_payload(kw_list=[' '], geo=geo, timeframe=f\"{start_date_str} {finish_date_str}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Search topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_df = pytrend.related_top_search_topics(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Google trends Search topics in New York during 2019-06-27 - 2019-06-30\n",
      "+-------------------------------------------------------------+---------------------------------------+\n",
      "|Search topics - rising                                       |Search topics - top                    |\n",
      "+-------------------------------------------------------------+---------------------------------------+\n",
      "|Pride parade - Topic                                         |New York - City in New York            |\n",
      "|Parade - Topic                                               |New York - US State                    |\n",
      "|Gay pride - Topic                                            |2019 - Topic                           |\n",
      "|Debate - Topic                                               |Weather - Topic                        |\n",
      "|2016 Democratic Party presidential debates and forums - Topic|YouTube - Video sharing company        |\n",
      "|Fireworks - Topic                                            |Google - Technology company            |\n",
      "|London - Capital of England                                  |Google Search - Topic                  |\n",
      "|Software - Software grouping                                 |Facebook, Inc. - Social network company|\n",
      "|New York Knicks - Basketball team                            |Film - Topic                           |\n",
      "|Independence Day - United States                             |Facebook - Social networking service   |\n",
      "|France - Country in Europe                                   |Hotel - Building function              |\n",
      "|Management - Topic                                           |Restaurant - Topic                     |\n",
      "|Nike - Footwear manufacturing company                        |Amazon.com - E-commerce company        |\n",
      "|nan                                                          |Brooklyn - New York City borough       |\n",
      "|nan                                                          |Car - Transportation mode              |\n",
      "+-------------------------------------------------------------+---------------------------------------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_google_trend_title(start_date, finish_date, \"Search topics\")\n",
    "topics_df.select(\"Search topics - rising\", \"Search topics - top\").show(num_of_top_interest, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Search queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_df = pytrend.related_top_search_queries(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Google trends Search queries in New York during 2019-06-27 - 2019-06-30\n",
      "+-------------------------+--------------------+-------+---+-----+\n",
      "|Search queries - rising  |Search queries - top|Rising |Top|geo  |\n",
      "+-------------------------+--------------------+-------+---+-----+\n",
      "|marianne williamson      |weather             |+1,500%|100|US-NY|\n",
      "|kamala harris            |facebook            |+1,450%|61 |US-NY|\n",
      "|yankees vs red sox       |google              |+1,200%|60 |US-NY|\n",
      "|yy                       |youtube             |+1,150%|52 |US-NY|\n",
      "|tulsi gabbard            |amazon              |+1,050%|46 |US-NY|\n",
      "|mexico vs costa rica     |news                |+950%  |46 |US-NY|\n",
      "|kemba walker             |world cup           |+850%  |38 |US-NY|\n",
      "|usa vs france            |craigslist          |+850%  |27 |US-NY|\n",
      "|mackenzie lueck          |instagram           |+500%  |27 |US-NY|\n",
      "|pride parade 2019 nyc    |yankees             |+450%  |25 |US-NY|\n",
      "|pride parade             |movies              |+400%  |24 |US-NY|\n",
      "|gay pride parade nyc 2019|walmart             |+400%  |23 |US-NY|\n",
      "|andrew yang              |mlb                 |+350%  |20 |US-NY|\n",
      "|bobby debarge            |copa america        |+350%  |19 |US-NY|\n",
      "|stonewall inn            |home depot          |+300%  |18 |US-NY|\n",
      "+-------------------------+--------------------+-------+---+-----+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_google_trend_title(start_date, finish_date, \"Search queries\")\n",
    "queries_df.show(num_of_top_interest, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
