{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outline\n",
    "1. [Problem statement](#Problem-statement)\n",
    "2. [ML problem](#ML-problem)\n",
    "3. [Data description](#Data-description)\n",
    "4. [Demo](#Let's-start-our-demo-ride)\n",
    "    * [Imports](#Import-all-libs-needed)\n",
    "    * [Define variables](#Define-variables)\n",
    "    * [Create Spark session](#Create-Spark-session)\n",
    "    * [Read data](#Read-data)\n",
    "    * [Tweets preprocessing](#Tweets-preprocessing)\n",
    "    * [Topic modeling via Latent Dirichlet Allocation](#Topic-modeling-via-Latent-Dirichlet-Allocation)\n",
    "    * [Term frequency-inverse document frequency (TF-IDF)](#TF-IDF)\n",
    "    * [Run the LDA Topic Modeler](#Run-the-LDA-Topic-Modeler)\n",
    "    * [Hot topics in the USA from Google trends](#Hot-topics-in-the-USA-from-Google-trends)\n",
    "5. [Evaluation and justification](#Evaluation-and-justification)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem statement "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of our project is to identify the topics under active discussion at the moment in a certain area.\n",
    "<br> The motivation for choosing this topic is, firstly, the published news articles can be distorted. Secondly, there is a delay between the actual event and publication in the news. Both of these factors are critical for market players, and those who have instant access to reliable information have a clear advantage.\n",
    "<br>We decided to organize this advantage for ourselves and for everyone (since this project is open source), highlighting the topics discussed in real-time on Twitter.\n",
    "<br>As a source, we chose Twitter because of its popularity and prevalence throughout the world, legitimate access to real-time data and the many topics discussed in it.\n",
    "<br>As an example, there are many cases when companies use Twitter to identify vulnerability in their security systems since information about it often comes in social networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, exist a lot of algorithms that solve the problem topic-modeling. Some of them are based on classical mathematical approaches as matrix decomposition, some use probabilistic methods, some are based on deep learning. In the context of our task, we have considered 3 potential methods for solution: <b>LSA</b>, <b>pLSA</b> and <b>LDA</b>. Each of them has its advantages and disadvantages.\n",
    "<br> <b>LSA</b> - Latent Semantic Analysis - is based on a singular matrix decomposition, under the assumption that words that are close in meaning will occur in similar pieces of text. This method is simple to implement, but for reliable results requires a large amount of data. \n",
    "<br><b>pLSA</b> - Latent Semantic Analysis - is based on probabilistic methods and finding hidden variables - topics. But in this approach, the number of parameters increases linearly with the number of documents. \n",
    "<br>That is why we settled on the <b>LDA</b> - Latent Dirichlet Allocation - unsupervised learning algorithm, which assumes that the topics of the documents have a Dirichlet distribution and the words in the topics also have a Dirichlet distribution. The technical part of the algorithm will be described below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to collect data in a natural way:\n",
    "<br>- we registered Twitter Developer account;\n",
    "<br>- using credentials from Twitter Developer account we run script that collected tweets by the geolocation and saved them in mongodb;\n",
    "<br>\n",
    "<br><b>As a result:</b>\n",
    "<br>- we collected  332548 tweets (10Gb in mongodb, ~100Mb in csv) from New-York geolocation since 30 of May up to 15 of June;\n",
    "<br>- we collected  6617029 tweets (~1.69Gb in csv) from USA geolocation since 15 of June up to now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's start our demo ride"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import all libs needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T13:34:48.681195Z",
     "start_time": "2019-07-05T13:34:48.670618Z"
    }
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T13:34:49.159490Z",
     "start_time": "2019-07-05T13:34:49.142765Z"
    }
   },
   "outputs": [],
   "source": [
    "# essential pyspark\n",
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import ArrayType, FloatType, StringType, IntegerType, StructField, StructType\n",
    "from pyspark.sql.functions import udf, row_number,column\n",
    "\n",
    "# vectorizer\n",
    "from pyspark.ml.feature import CountVectorizer, StopWordsRemover, HashingTF, IDF, Tokenizer\n",
    "\n",
    "# staff for LDA\n",
    "from pyspark.mllib.clustering import LDA, LDAModel\n",
    "from pyspark.mllib.linalg import Vector as oldVector, Vectors as oldVectors\n",
    "from pyspark.ml.linalg import Vector as newVector, Vectors as newVectors\n",
    "\n",
    "# pytrends for acquiring google trends\n",
    "from pytrends.pytrends.request import TrendReq\n",
    "\n",
    "# import hardcoded variables\n",
    "from utils.channels_to_filter import channels_not_to_consider\n",
    "\n",
    "# custom text preprocessing\n",
    "from utils.text_preprocessing import *\n",
    "\n",
    "# custom tools to work with google trends \n",
    "from utils.trends import *\n",
    "\n",
    "# handy functions for data merging\n",
    "from utils.data_merge import *\n",
    "\n",
    "# handy functions for topic modeling result handeling\n",
    "from utils.topic_modeling import *\n",
    "\n",
    "# datetime handling\n",
    "from datetime import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Connecting to data**  \n",
    "Please, specify path to the csv file with data right here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to CSV\n",
    "historical_tweets_data = 'data/tweets/new_york_training_tweets_15_06.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Time frames to pick data from**  \n",
    "We picked some time frames to get data from to check if our topic model can extract info about events that occured during this period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final of league championship \n",
    "lc_final_start_datetime = \"Sat Jun 01 00:00:00 +0000 2019\"\n",
    "lc_final_finish_datetime = \"Sat Jun 01 23:59:59 +0000 2019\"\n",
    "\n",
    "# Stanley cup final\n",
    "stanley_final_start_datetime = \"Wed Jun 12 00:00:00 +0000 2019\"\n",
    "stanley_final_finish_datetime = \"Wed Jun 12 23:59:59 +0000 2019\"\n",
    "\n",
    "# Draft NBA\n",
    "nba_final_start_datetime = \"Thu Jun 20 00:00:00 +0000 2019\"\n",
    "nba_finish_final_datetime = \"Sun Jun 23 23:59:59 +0000 2019\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or you can specify your own dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_datetime = ''\n",
    "finish_datetime = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use one of the dates defined above right here (e.g., we use Stanely cup final dates)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_start_datetime = str_tweet_to_datetime(stanley_final_start_datetime)\n",
    "frame_finish_datetime = str_tweet_to_datetime(stanley_final_finish_datetime)\n",
    "\n",
    "assert (frame_finish_datetime - frame_start_datetime).days <= 3, \"Date interval should not be bigger than 3 days\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Location related variables**  \n",
    "Here you can specify exact location from which you want to get tweets for topic modeling. Here we have example for NYC (local approach) and whole US (global approach). You can explore raw data to find more locations for filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T13:41:05.605577Z",
     "start_time": "2019-07-05T13:41:05.601192Z"
    }
   },
   "outputs": [],
   "source": [
    "# if True locations from locations_to_consider will be used to filter\n",
    "get_from_location = True\n",
    "\n",
    "# locations to filter relevant tweets\n",
    "locations_to_consider = [\n",
    "                         'Manhattan, NY', \n",
    "                         'Brooklyn, NY', \n",
    "                         'Queens, NY', \n",
    "                         'Bronx, NY', \n",
    "                         'Staten Island, NY'\n",
    "                         'New York, USA'\n",
    "                        ]\n",
    "\n",
    "# used to extract google trends\n",
    "if get_from_location:\n",
    "    geo = 'US-NY' \n",
    "else:\n",
    "    geo = 'US'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LDA parameters**  \n",
    "We have tuned parameters of LDA in order to obtain reliable results. The evaluation will be discussed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T13:38:36.620763Z",
     "start_time": "2019-07-05T13:38:36.617592Z"
    }
   },
   "outputs": [],
   "source": [
    "# LDA params\n",
    "num_of_topics_LDA = 15\n",
    "max_iterations_LDA = 120\n",
    "\n",
    "number_of_words_per_topic = 15  # number of words per topic\n",
    "num_of_top_interest = 15 # number of topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T13:38:37.435833Z",
     "start_time": "2019-07-05T13:38:37.427311Z"
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"pipeline\").getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the historical data, it can take a while**  \n",
    "Here we load data and filter by dates you chose above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T13:38:45.189768Z",
     "start_time": "2019-07-05T13:38:45.184190Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time range to be extracted from  data/tweets/new_york_training_tweets_15_06.csv 2019-06-12 00:00:00+00:00 2019-06-12 23:59:59+00:00\n",
      "Range for collected data (history):  2019-06-12 00:00:00+00:00 2019-06-12 23:59:59+00:00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "29538"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "times = (frame_start_datetime, frame_finish_datetime)\n",
    "print(\"Time range to be extracted from \", historical_tweets_data, times[0], times[1])\n",
    "selected_df = get_historical_df(historical_tweets_data=historical_tweets_data, historical_start_time=times[0], historical_finish_time=times[1], spark=spark)\n",
    "assert selected_df != None, \"Something goes wrong with selecting data from recent data/history data\"\n",
    "selected_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweets preprocessing  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Basic filtering**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we do basic filtering based on null values. We filter out channels that are not important in topic modeling. We defined it by applying LDA on different dates and find out that there are a lot of channels which specialized on particular topics (for example, weather, traffic in the city, photos, hiring people for a job). These topics were always distinguished. But, since they don't provide any information about important events, we have removed specialized channels from consideration.\n",
    " Also, we do filtering based on global and local location (if local filtering is enabled). Finally, we check tweet itself for the length of the message. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = selected_df\n",
    "\n",
    "# filter nans\n",
    "df = df.rdd.filter(lambda x: x[0] != None and x[1] != None and x[2] != None and x[4] != None)\n",
    "\n",
    "# filter out channels not to consider\n",
    "df = df.filter(lambda x: x[4] not in channels_not_to_consider)\n",
    "\n",
    "# filter by country\n",
    "df = df.filter(lambda x: x[1] in 'US')\n",
    "\n",
    "# filter by precise location\n",
    "if get_from_location:\n",
    "    df = df.filter(lambda x: x[2] in locations_to_consider)\n",
    "\n",
    "# filter tweet itself\n",
    "df = df.filter(lambda x: filter_tweet(x[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now the most interesting - tweet cleaning**  \n",
    "Text cleaning is crucial for any text modelling process, especially for topic modelling. We tried three different approaches: classic, using only hashtags, using only urls. In classic approach we delete all non-words (including urls, hashtags, emojis and mentions), filter out common stop words, so only plain text information is left. But still such data has a lot of noise and uninformative words, so we tried another approaches with using only hashtags (which should code the most important information) and urls."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's start from classic approach**  \n",
    "In our case it consists from those steps:  \n",
    "1) Lowercase all words  \n",
    "2) Filter words with non-letters at the beginning (mainly for mentions, e.g. \"@some_user\")  \n",
    "3) Filter http/https  \n",
    "4) Filter all non-letters (crucial to remove emoji)  \n",
    "5) Remove multiply whitespaces  \n",
    "6) Remove repeated chars (e.g. \"greeeeat\" -> \"great\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hashtags & URLs**  \n",
    "Those approaches are pretty clear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Basically applying approach discussed above**  \n",
    "Now let's apply one of the aproaches. But first specify which one you want to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_type = 'hashtags' # 'just-text', 'hashtags' or 'urls'\n",
    "\n",
    "if preprocessing_type == 'just-text':\n",
    "    process_tweet = process_text\n",
    "elif preprocessing_type == 'hashtags':\n",
    "    process_tweet = process_hashtags\n",
    "elif preprocessing_type == 'urls':\n",
    "    process_tweet = process_urls\n",
    "\n",
    "# process tweet\n",
    "df = df.map(lambda x: process_tweet(x[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Final postprocessing**  \n",
    "Here we make sure that we don't have entries without tokens at all, also we change the structure suitable for pyspark LDA class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final preprocesssing\n",
    "df = df.filter(lambda x: len(x) > 0)\n",
    "\n",
    "# make dataframes great again\n",
    "df = df.map(lambda x: [x])\n",
    "\n",
    "# schema for df\n",
    "schema = StructType([StructField('tokens', ArrayType(StringType()), True)])\n",
    "df = df.toDF(schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|              tokens|\n",
      "+--------------------+\n",
      "|        [whiteboard]|\n",
      "|         [snotyboyz]|\n",
      "| [remote, orchestra]|\n",
      "|              [rbny]|\n",
      "|[libertystatue, l...|\n",
      "|[pazviola, mauric...|\n",
      "|[originalcomposit...|\n",
      "|          [freeship]|\n",
      "|         [trioworks]|\n",
      "|[autoindustry, auto]|\n",
      "+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1924"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic modeling via Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic Model is a type of statistical model used for tagging abstract “topics” that occur in a collection of documents that best represents the information in them.<br/>\n",
    "The basic idea in the LDA is that documents are represented as a random mixture of latent topics, where each topic is characterized by a distribution of words.<br/>\n",
    "\n",
    "<img src=\"http://chdoig.github.io/pytexas2015-topic-modeling/images/lda-4.png\" width=600/>\n",
    "*http://chdoig.github.io/pytexas2015-topic-modeling/?source=post_page---------------------------#/3/4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### CountVectorizer helps to convert a collection of text documents to vectors of token counts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07182019 23:52:29\n",
      "07182019 23:52:57\n"
     ]
    }
   ],
   "source": [
    "print(time.strftime('%m%d%Y %H:%M:%S'))\n",
    "\n",
    "cv = CountVectorizer(inputCol=\"tokens\", outputCol=\"raw_features\", vocabSize=10000, minDF=2.0)\n",
    "cvmodel = cv.fit(df)\n",
    "\n",
    "print(time.strftime('%m%d%Y %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07182019 23:52:57\n",
      "07182019 23:52:57\n"
     ]
    }
   ],
   "source": [
    "print(time.strftime('%m%d%Y %H:%M:%S'))\n",
    "df = cvmodel.transform(df)\n",
    "print(time.strftime('%m%d%Y %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Process text mining to reflect the importance of a term to a document in the corpus. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"tf_idf_features\", minDocFreq=2)\n",
    "idfModel = idf.fit(df)\n",
    "\n",
    "df = idfModel.transform(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|              tokens|        raw_features|     tf_idf_features|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|        [whiteboard]|         (538,[],[])|         (538,[],[])|\n",
      "|         [snotyboyz]|         (538,[],[])|         (538,[],[])|\n",
      "| [remote, orchestra]|         (538,[],[])|         (538,[],[])|\n",
      "|              [rbny]|         (538,[],[])|         (538,[],[])|\n",
      "|[libertystatue, l...|(538,[5,62,149,31...|(538,[5,62,149,31...|\n",
      "|[pazviola, mauric...|   (538,[252],[1.0])|(538,[252],[6.464...|\n",
      "|[originalcomposit...|         (538,[],[])|         (538,[],[])|\n",
      "|          [freeship]|     (538,[2],[1.0])|(538,[2],[4.03632...|\n",
      "|         [trioworks]|         (538,[],[])|         (538,[],[])|\n",
      "|[autoindustry, auto]|(538,[337,492],[1...|(538,[337,492],[6...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Add id field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = Window().orderBy(column(\"tokens\"))\n",
    "df = df.withColumn(\"id\", row_number().over(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+---+\n",
      "|              tokens|        raw_features|     tf_idf_features| id|\n",
      "+--------------------+--------------------+--------------------+---+\n",
      "|                 [1]|    (538,[12],[1.0])|(538,[12],[4.9977...|  1|\n",
      "|                 [1]|    (538,[12],[1.0])|(538,[12],[4.9977...|  2|\n",
      "|                 [1]|    (538,[12],[1.0])|(538,[12],[4.9977...|  3|\n",
      "|                 [1]|    (538,[12],[1.0])|(538,[12],[4.9977...|  4|\n",
      "|                 [1]|    (538,[12],[1.0])|(538,[12],[4.9977...|  5|\n",
      "|                 [1]|    (538,[12],[1.0])|(538,[12],[4.9977...|  6|\n",
      "|                 [1]|    (538,[12],[1.0])|(538,[12],[4.9977...|  7|\n",
      "|                 [1]|    (538,[12],[1.0])|(538,[12],[4.9977...|  8|\n",
      "|              [1, 1]|    (538,[12],[2.0])|(538,[12],[9.9954...|  9|\n",
      "|[1, freeship, mai...|(538,[2,12,14],[1...|(538,[2,12,14],[4...| 10|\n",
      "+--------------------+--------------------+--------------------+---+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = df.rdd.map(lambda x: (x[3], oldVectors.fromML(x[2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_df = rs.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rs_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the LDA Topic Modeler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07182019 23:56:23\n",
      "07182019 23:57:02\n"
     ]
    }
   ],
   "source": [
    "print(time.strftime('%m%d%Y %H:%M:%S'))\n",
    "lda_model = LDA.train(rs_df['_1', '_2'].rdd.map(list), k=num_of_topics_LDA, maxIterations=max_iterations_LDA)\n",
    "print(time.strftime('%m%d%Y %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Now we prepare output of LDA  to be shown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07182019 23:57:30\n",
      "07182019 23:57:31\n"
     ]
    }
   ],
   "source": [
    "print(time.strftime('%m%d%Y %H:%M:%S'))\n",
    "topics = lda_model.topicsMatrix()\n",
    "vocabArray = cvmodel.vocabulary\n",
    "print(time.strftime('%m%d%Y %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07182019 23:57:31\n",
      "07182019 23:57:31\n"
     ]
    }
   ],
   "source": [
    "print(time.strftime('%m%d%Y %H:%M:%S'))\n",
    "topicIndices = sc.parallelize(lda_model.describeTopics(maxTermsPerTopic = number_of_words_per_topic))\n",
    "print(time.strftime('%m%d%Y %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07182019 23:57:31\n",
      "07182019 23:57:32\n"
     ]
    }
   ],
   "source": [
    "print(time.strftime('%m%d%Y %H:%M:%S'))\n",
    "topics_final = topicIndices.map(lambda topic:topic_render(topic, number_of_words_per_topic, vocabArray)).collect()\n",
    "print(time.strftime('%m%d%Y %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Topics based on tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #1\n",
      "0.12  manhattan\n",
      "0.044  aidenkai\n",
      "0.043  kim\n",
      "0.043  harlem\n",
      "0.035  blessed\n",
      "0.028  thevessel\n",
      "0.028  fifawwc\n",
      "0.028  subway\n",
      "0.027  hongkongprotest\n",
      "0.027  proud\n",
      "0.027  blackandwhite\n",
      "0.027  rapper\n",
      "0.027  midsommar\n",
      "0.027  mytwitteranniversary\n",
      "0.027  nycfc\n",
      "\n",
      "\n",
      "Topic #2\n",
      "0.061  ducktales\n",
      "0.061  whentheyseeus\n",
      "0.056  classof2019\n",
      "0.043  finduagain\n",
      "0.038  familytime\n",
      "0.037  ghopen\n",
      "0.037  wcw\n",
      "0.037  lgbtq\n",
      "0.031  stress\n",
      "0.031  time\n",
      "0.03  singer\n",
      "0.03  photooftheday\n",
      "0.03  whitneybiennial\n",
      "0.03  speedtest\n",
      "0.024  downwiththesickness\n",
      "\n",
      "\n",
      "Topic #3\n",
      "0.086  stanleycup\n",
      "0.07  game7\n",
      "0.039  chasdeihashem\n",
      "0.039  shtisel\n",
      "0.039  shtiselny\n",
      "0.033  ryan\n",
      "0.032  jwmg\n",
      "0.032  nhlbruins\n",
      "0.032  nhl\n",
      "0.032  cosplay\n",
      "0.032  etsy\n",
      "0.031  repost\n",
      "0.026  kevin\n",
      "0.025  sketch\n",
      "0.024  stanleycupfinal\n",
      "\n",
      "\n",
      "Topic #4\n",
      "0.249  newyork\n",
      "0.056  ny\n",
      "0.049  travel\n",
      "0.042  frenchbulldogs\n",
      "0.042  frenchbulldogsofinstagram\n",
      "0.042  frenchbulldog\n",
      "0.042  frenchbulldogsforsale\n",
      "0.042  frenchbulldogpuppiesforsale\n",
      "0.027  twastyles\n",
      "0.027  twa\n",
      "0.026  truth\n",
      "0.026  fashnme\n",
      "0.019  newjersey\n",
      "0.019  apple\n",
      "0.019  amazing\n",
      "\n",
      "\n",
      "Topic #5\n",
      "0.143  meltwatersummit\n",
      "0.105  manhattan_again\n",
      "0.079  1\n",
      "0.057  megatron\n",
      "0.054  wednesdaywisdom\n",
      "0.04  summer\n",
      "0.028  womancrushwednesday\n",
      "0.028  posefx\n",
      "0.028  soho\n",
      "0.022  runner\n",
      "0.022  soundhound\n",
      "0.022  goodmorning\n",
      "0.021  museum\n",
      "0.016  mensstyle\n",
      "0.016  tishmanspeyer\n",
      "\n",
      "\n",
      "Topic #6\n",
      "0.1  music\n",
      "0.045  wednesdaythoughts\n",
      "0.045  youtube\n",
      "0.037  nicoleharrison\n",
      "0.037  dogpound\n",
      "0.037  dj\n",
      "0.037  adweekchat\n",
      "0.029  rayonbrandtministries\n",
      "0.029  church\n",
      "0.029  people\n",
      "0.029  life\n",
      "0.029  bookedandblessed\n",
      "0.029  ham\n",
      "0.02  hamr\n",
      "0.02  amateurr\n",
      "\n",
      "\n",
      "Topic #7\n",
      "0.168  brooklyn\n",
      "0.078  blackmendontcheat\n",
      "0.058  2\n",
      "0.043  cd\n",
      "0.037  comic\n",
      "0.035  jewel\n",
      "0.028  livemusic\n",
      "0.028  williamsburg\n",
      "0.027  theatre\n",
      "0.027  mta\n",
      "0.027  1trump\n",
      "0.027  honorthemwithaction\n",
      "0.019  nycfreaks\n",
      "0.019  hometownbbq\n",
      "0.019  nassaucounty\n",
      "\n",
      "\n",
      "Topic #8\n",
      "0.07  rock\n",
      "0.063  futurefintech\n",
      "0.056  fashion\n",
      "0.036  follow\n",
      "0.035  dance\n",
      "0.027  twitter\n",
      "0.027  filmmaker\n",
      "0.027  vintage\n",
      "0.027  jimin\n",
      "0.027  win\n",
      "0.026  producer\n",
      "0.019  starwars\n",
      "0.019  gangsta\n",
      "0.019  lukeskywalker\n",
      "0.019  roastbattle\n",
      "\n",
      "\n",
      "Topic #9\n",
      "0.08  pulse\n",
      "0.059  centralpark\n",
      "0.051  jordan\n",
      "0.037  digital\n",
      "0.037  family\n",
      "0.036  curtis\n",
      "0.036  motivation\n",
      "0.029  work\n",
      "0.028  marketing\n",
      "0.028  humor\n",
      "0.026  rap\n",
      "0.02  nephew\n",
      "0.02  hustle\n",
      "0.02  opportunity\n",
      "0.02  recordlabel\n",
      "\n",
      "\n",
      "Topic #10\n",
      "0.131  pride\n",
      "0.101  gh\n",
      "0.08  ava\n",
      "0.073  icfsummit\n",
      "0.047  pridemonth\n",
      "0.033  pride2019\n",
      "0.033  socialmedia\n",
      "0.027  jax\n",
      "0.026  nina\n",
      "0.026  arifitzgerald\n",
      "0.026  stonewall50\n",
      "0.025  sudanmassacre\n",
      "0.018  adwords\n",
      "0.018  sem\n",
      "0.018  smo\n",
      "\n",
      "\n",
      "Topic #11\n",
      "0.33  nyc\n",
      "0.112  newyorkcity\n",
      "0.051  beermenus\n",
      "0.031  photography\n",
      "0.031  byjordana\n",
      "0.026  nycphotographer\n",
      "0.02  garyvee\n",
      "0.02  free\n",
      "0.02  sorrynotsorry\n",
      "0.02  pinstripepride\n",
      "0.019  nycity\n",
      "0.014  pasta\n",
      "0.014  foodosart\n",
      "0.014  foodporn\n",
      "0.014  nycgo\n",
      "\n",
      "\n",
      "Topic #12\n",
      "0.066  love\n",
      "0.059  thebachelorette\n",
      "0.053  gay\n",
      "0.041  foodie\n",
      "0.034  live\n",
      "0.033  hufflepuff\n",
      "0.026  teamexhib\n",
      "0.026  coffee\n",
      "0.025  vibe\n",
      "0.025  mib\n",
      "0.018  insight\n",
      "0.018  progress\n",
      "0.018  wd2019\n",
      "0.018  redken\n",
      "0.018  olaplex\n",
      "\n",
      "\n",
      "Topic #13\n",
      "0.166  freeship\n",
      "0.09  case\n",
      "0.075  dvd\n",
      "0.075  generic\n",
      "0.063  art\n",
      "0.042  beauty\n",
      "0.035  bronx\n",
      "0.035  streetart\n",
      "0.029  makeup\n",
      "0.029  artist\n",
      "0.024  sunset\n",
      "0.023  graffiti\n",
      "0.023  orlandostrong\n",
      "0.022  brooklyndogs\n",
      "0.022  view\n",
      "\n",
      "\n",
      "Topic #14\n",
      "0.064  wednesday\n",
      "0.032  running\n",
      "0.032  wednesdaymotivation\n",
      "0.025  hot97\n",
      "0.025  straightcash\n",
      "0.025  power1051\n",
      "0.025  cx\n",
      "0.025  nbafinals\n",
      "0.025  evolution2\n",
      "0.025  groundwar\n",
      "0.025  loveislove\n",
      "0.025  nypd\n",
      "0.025  bts\n",
      "0.025  happyhour\n",
      "0.025  design\n",
      "\n",
      "\n",
      "Topic #15\n",
      "0.078  sugarbaby\n",
      "0.058  sugardaddy\n",
      "0.051  6yearswithourhomebts\n",
      "0.051  lgm\n",
      "0.036  broadway\n",
      "0.036  방탄6주년보라해\n",
      "0.036  sugarbabyneeded\n",
      "0.036  toystory4\n",
      "0.035  legend\n",
      "0.028  seekingarrangement\n",
      "0.028  hbd_bts\n",
      "0.028  sugarbabywanted\n",
      "0.027  friend\n",
      "0.027  chernobylhbo\n",
      "0.02  flyer\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for topic in range(len(topics_final)):\n",
    "    print (\"Topic #\" + str(topic+1) + \"\")\n",
    "    for term in topics_final[topic]:\n",
    "        print (term)\n",
    "    print ('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Result of topic modelling, actual topics are highlighted\n",
    "###### Topic #1 - Hong Kong protests (31 March - 16 June)\n",
    "0.12  manhattan\n",
    "0.044  aidenkai\n",
    "0.043  kim\n",
    "0.043  harlem\n",
    "0.035  blessed\n",
    "0.028  thevessel\n",
    "0.028  fifawwc\n",
    "0.028  subway\n",
    "0.027  hongkongprotest\n",
    "0.027  proud\n",
    "0.027  blackandwhite\n",
    "0.027  rapper\n",
    "0.027  midsommar\n",
    "0.027  mytwitteranniversary\n",
    "0.027  nycfc\n",
    "\n",
    "\n",
    "Topic #2\n",
    "0.061  ducktales\n",
    "0.061  whentheyseeus\n",
    "0.056  classof2019\n",
    "0.043  finduagain\n",
    "0.038  familytime\n",
    "0.037  ghopen\n",
    "0.037  wcw\n",
    "0.037  lgbtq\n",
    "0.031  stress\n",
    "0.031  time\n",
    "0.03  singer\n",
    "0.03  photooftheday\n",
    "0.03  whitneybiennial\n",
    "0.03  speedtest\n",
    "0.024  downwiththesickness\n",
    "\n",
    "\n",
    "###### Topic #3 - Hockey tournament, 12 June - final\n",
    "0.086  stanleycup\n",
    "0.07  game7\n",
    "0.039  chasdeihashem\n",
    "0.039  shtisel\n",
    "0.039  shtiselny\n",
    "0.033  ryan\n",
    "0.032  jwmg\n",
    "0.032  nhlbruins\n",
    "0.032  nhl\n",
    "0.032  cosplay\n",
    "0.032  etsy\n",
    "0.031  repost\n",
    "0.026  kevin\n",
    "0.025  sketch\n",
    "0.024  stanleycupfinal\n",
    "\n",
    "\n",
    "Topic #4\n",
    "0.249  newyork\n",
    "0.056  ny\n",
    "0.049  travel\n",
    "0.042  frenchbulldogs\n",
    "0.042  frenchbulldogsofinstagram\n",
    "0.042  frenchbulldog\n",
    "0.042  frenchbulldogsforsale\n",
    "0.042  frenchbulldogpuppiesforsale\n",
    "0.027  twastyles\n",
    "0.027  twa\n",
    "0.026  truth\n",
    "0.026  fashnme\n",
    "0.019  newjersey\n",
    "0.019  apple\n",
    "0.019  amazing\n",
    "\n",
    "\n",
    "##### Topic #5 - Melt Social Summit, 12-13 June\n",
    "0.143  meltwatersummit\n",
    "0.105  manhattan_again\n",
    "0.079  1\n",
    "0.057  megatron\n",
    "0.054  wednesdaywisdom\n",
    "0.04  summer\n",
    "0.028  womancrushwednesday\n",
    "0.028  posefx\n",
    "0.028  soho\n",
    "0.022  runner\n",
    "0.022  soundhound\n",
    "0.022  goodmorning\n",
    "0.021  museum\n",
    "0.016  mensstyle\n",
    "0.016  tishmanspeyer\n",
    "\n",
    "\n",
    "Topic #6\n",
    "0.1  music\n",
    "0.045  wednesdaythoughts\n",
    "0.045  youtube\n",
    "0.037  nicoleharrison\n",
    "0.037  dogpound\n",
    "0.037  dj\n",
    "0.037  adweekchat\n",
    "0.029  rayonbrandtministries\n",
    "0.029  church\n",
    "0.029  people\n",
    "0.029  life\n",
    "0.029  bookedandblessed\n",
    "0.029  ham\n",
    "0.02  hamr\n",
    "0.02  amateurr\n",
    "\n",
    "\n",
    "Topic #7\n",
    "0.168  brooklyn\n",
    "0.078  blackmendontcheat\n",
    "0.058  2\n",
    "0.043  cd\n",
    "0.037  comic\n",
    "0.035  jewel\n",
    "0.028  livemusic\n",
    "0.028  williamsburg\n",
    "0.027  theatre\n",
    "0.027  mta\n",
    "0.027  1trump\n",
    "0.027  honorthemwithaction\n",
    "0.019  nycfreaks\n",
    "0.019  hometownbbq\n",
    "0.019  nassaucounty\n",
    "\n",
    "\n",
    "Topic #8\n",
    "0.07  rock\n",
    "0.063  futurefintech\n",
    "0.056  fashion\n",
    "0.036  follow\n",
    "0.035  dance\n",
    "0.027  twitter\n",
    "0.027  filmmaker\n",
    "0.027  vintage\n",
    "0.027  jimin\n",
    "0.027  win\n",
    "0.026  producer\n",
    "0.019  starwars\n",
    "0.019  gangsta\n",
    "0.019  lukeskywalker\n",
    "0.019  roastbattle\n",
    "\n",
    "\n",
    "Topic #9\n",
    "0.08  pulse\n",
    "0.059  centralpark\n",
    "0.051  jordan\n",
    "0.037  digital\n",
    "0.037  family\n",
    "0.036  curtis\n",
    "0.036  motivation\n",
    "0.029  work\n",
    "0.028  marketing\n",
    "0.028  humor\n",
    "0.026  rap\n",
    "0.02  nephew\n",
    "0.02  hustle\n",
    "0.02  opportunity\n",
    "0.02  recordlabel\n",
    "\n",
    "\n",
    "###### Topic #10 - June - month of pride \n",
    "0.131  pride\n",
    "0.101  gh\n",
    "0.08  ava\n",
    "0.073  icfsummit\n",
    "0.047  pridemonth\n",
    "0.033  pride2019\n",
    "0.033  socialmedia\n",
    "0.027  jax\n",
    "0.026  nina\n",
    "0.026  arifitzgerald\n",
    "0.026  stonewall50\n",
    "0.025  sudanmassacre\n",
    "0.018  adwords\n",
    "0.018  sem\n",
    "0.018  smo\n",
    "\n",
    "\n",
    "Topic #11\n",
    "0.33  nyc\n",
    "0.112  newyorkcity\n",
    "0.051  beermenus\n",
    "0.031  photography\n",
    "0.031  byjordana\n",
    "0.026  nycphotographer\n",
    "0.02  garyvee\n",
    "0.02  free\n",
    "0.02  sorrynotsorry\n",
    "0.02  pinstripepride\n",
    "0.019  nycity\n",
    "0.014  pasta\n",
    "0.014  foodosart\n",
    "0.014  foodporn\n",
    "0.014  nycgo\n",
    "\n",
    "\n",
    "Topic #12\n",
    "0.066  love\n",
    "0.059  thebachelorette\n",
    "0.053  gay\n",
    "0.041  foodie\n",
    "0.034  live\n",
    "0.033  hufflepuff\n",
    "0.026  teamexhib\n",
    "0.026  coffee\n",
    "0.025  vibe\n",
    "0.025  mib\n",
    "0.018  insight\n",
    "0.018  progress\n",
    "0.018  wd2019\n",
    "0.018  redken\n",
    "0.018  olaplex\n",
    "\n",
    "\n",
    "Topic #13\n",
    "0.166  freeship\n",
    "0.09  case\n",
    "0.075  dvd\n",
    "0.075  generic\n",
    "0.063  art\n",
    "0.042  beauty\n",
    "0.035  bronx\n",
    "0.035  streetart\n",
    "0.029  makeup\n",
    "0.029  artist\n",
    "0.024  sunset\n",
    "0.023  graffiti\n",
    "0.023  orlandostrong\n",
    "0.022  brooklyndogs\n",
    "0.022  view\n",
    "\n",
    "\n",
    "Topic #14\n",
    "0.064  wednesday\n",
    "0.032  running\n",
    "0.032  wednesdaymotivation\n",
    "0.025  hot97\n",
    "0.025  straightcash\n",
    "0.025  power1051\n",
    "0.025  cx\n",
    "0.025  nbafinals\n",
    "0.025  evolution2\n",
    "0.025  groundwar\n",
    "0.025  loveislove\n",
    "0.025  nypd\n",
    "0.025  bts\n",
    "0.025  happyhour\n",
    "0.025  design\n",
    "\n",
    "\n",
    "Topic #15\n",
    "0.078  sugarbaby\n",
    "0.058  sugardaddy\n",
    "0.051  6yearswithourhomebts\n",
    "0.051  lgm\n",
    "0.036  broadway\n",
    "0.036  방탄6주년보라해\n",
    "0.036  sugarbabyneeded\n",
    "0.036  toystory4\n",
    "0.035  legend\n",
    "0.028  seekingarrangement\n",
    "0.028  hbd_bts\n",
    "0.028  sugarbabywanted\n",
    "0.027  friend\n",
    "0.027  chernobylhbo\n",
    "0.02  flyer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "#### Result based on the tweets text without hashtags\n",
    "Topic #1\n",
    "0.038  get\n",
    "0.03  people\n",
    "0.027  need\n",
    "0.027  good\n",
    "0.024  want\n",
    "0.019  never\n",
    "0.017  still\n",
    "0.017  show\n",
    "0.015  first\n",
    "0.014  like\n",
    "0.013  gonna\n",
    "0.011  keep\n",
    "0.011  baby\n",
    "0.011  getting\n",
    "0.01  hope\n",
    "\n",
    "\n",
    "Topic #2\n",
    "0.029  amp\n",
    "0.02  work\n",
    "0.018  great\n",
    "0.017  th\n",
    "0.014  well\n",
    "0.012  tonight\n",
    "0.012  pm\n",
    "0.01  june\n",
    "0.009  another\n",
    "0.009  call\n",
    "0.008  nice\n",
    "0.008  open\n",
    "0.008  hour\n",
    "0.008  making\n",
    "0.008  free\n",
    "\n",
    "\n",
    "Topic #3\n",
    "0.028  got\n",
    "0.023  year\n",
    "0.018  take\n",
    "0.016  last\n",
    "0.016  best\n",
    "0.01  sure\n",
    "0.01  two\n",
    "0.009  already\n",
    "0.009  try\n",
    "0.009  wow\n",
    "0.009  literally\n",
    "0.009  read\n",
    "0.009  song\n",
    "0.008  point\n",
    "0.008  black\n",
    "\n",
    "\n",
    "Topic #4\n",
    "0.032  love\n",
    "0.03  day\n",
    "0.024  really\n",
    "0.022  make\n",
    "0.022  shit\n",
    "0.02  say\n",
    "0.02  right\n",
    "0.02  back\n",
    "0.019  lmao\n",
    "0.016  every\n",
    "0.015  happy\n",
    "0.015  come\n",
    "0.015  friend\n",
    "0.013  said\n",
    "0.012  im\n",
    "\n",
    "\n",
    "Topic #5\n",
    "0.025  thank\n",
    "0.02  thing\n",
    "0.016  feel\n",
    "0.015  yes\n",
    "0.015  ever\n",
    "0.013  real\n",
    "0.013  woman\n",
    "0.011  fucking\n",
    "0.011  amazing\n",
    "0.011  something\n",
    "0.011  god\n",
    "0.01  watch\n",
    "0.009  like\n",
    "0.009  white\n",
    "0.009  men\n",
    "\n",
    "\n",
    "Topic #6\n",
    "0.032  time\n",
    "0.032  u\n",
    "0.03  know\n",
    "0.026  see\n",
    "0.019  going\n",
    "0.018  even\n",
    "0.017  let\n",
    "0.017  much\n",
    "0.014  fuck\n",
    "0.012  made\n",
    "0.011  could\n",
    "0.01  thought\n",
    "0.01  trying\n",
    "0.01  old\n",
    "0.009  park\n",
    "\n",
    "\n",
    "Topic #7\n",
    "0.035  one\n",
    "0.022  think\n",
    "0.021  would\n",
    "0.014  also\n",
    "0.013  someone\n",
    "0.013  next\n",
    "0.011  trump\n",
    "0.011  week\n",
    "0.011  job\n",
    "0.01  tell\n",
    "0.01  mean\n",
    "0.01  many\n",
    "0.01  anyone\n",
    "0.01  talk\n",
    "0.01  everyone\n",
    "\n",
    "\n",
    "Topic #8\n",
    "0.025  lol\n",
    "0.018  life\n",
    "0.017  way\n",
    "0.016  man\n",
    "0.014  better\n",
    "0.013  nigga\n",
    "0.013  stop\n",
    "0.012  lmfao\n",
    "0.01  bad\n",
    "0.01  wanna\n",
    "0.01  start\n",
    "0.009  gotta\n",
    "0.009  find\n",
    "0.009  hard\n",
    "0.009  help\n",
    "\n",
    "\n",
    "Topic #9\n",
    "0.048  new\n",
    "0.036  york\n",
    "0.023  today\n",
    "0.022  ny\n",
    "0.02  look\n",
    "0.013  please\n",
    "0.013  city\n",
    "0.013  w\n",
    "0.012  game\n",
    "0.012  summer\n",
    "0.01  like\n",
    "0.01  video\n",
    "0.01  put\n",
    "0.009  hate\n",
    "0.009  whole\n",
    "\n",
    "\n",
    "Topic #10\n",
    "0.022  nyc\n",
    "0.02  go\n",
    "0.015  brooklyn\n",
    "0.014  guy\n",
    "0.014  oh\n",
    "0.013  girl\n",
    "0.012  thanks\n",
    "0.011  wait\n",
    "0.011  night\n",
    "0.01  yeah\n",
    "0.01  b\n",
    "0.009  always\n",
    "0.009  street\n",
    "0.008  manhattan\n",
    "0.008  photo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### As we can see, results are better when we use hashtags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hot topics in the USA from [Google trends](https://trends.google.com/trends/explore?geo=US)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check actual result of tweets data, we acquire google trends data by the specific location and the same period of time. So, send request to trends.google.com and got responce which contains top search topics and top search queries. \n",
    "There are used modified pytrends API to get get this data. We modified a little bit (add fuinctions to interface related_top_search_topics, related_top_search_queries) pytrends API to get google trends date by specifi date and timeframe, because there are not such functional in public API.\n",
    "<br>\n",
    "<br>\n",
    "pytrnds API: https://github.com/GeneralMills/pytrends\n",
    "<br>\n",
    "This repository was forked and changes saves in our public rep: https://github.com/DmytroBabenko/pytrends "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date_str = frame_start_datetime.strftime(\"%Y-%m-%d\")\n",
    "finish_date_str = frame_start_datetime.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "pytrend = TrendReq()\n",
    "pytrend.build_payload(kw_list=[' '], geo=geo, timeframe=f\"{start_date_str} {finish_date_str}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Search topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_df = pytrend.related_top_search_topics(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Google trends Search topics in New York during 2019-06-12\n",
      "+----------------------------+----------------------------------------+\n",
      "|Search topics - rising      |Search topics - top                     |\n",
      "+----------------------------+----------------------------------------+\n",
      "|Mathematical game - Topic   |New York - City in New York             |\n",
      "|Swimming pool - Topic       |New York - US State                     |\n",
      "|Mathematics - Field of study|Google - Technology company             |\n",
      "|nan                         |Google Search - Topic                   |\n",
      "|nan                         |2019 - Topic                            |\n",
      "|nan                         |Weather - Topic                         |\n",
      "|nan                         |YouTube - Video sharing company         |\n",
      "|nan                         |Facebook - Social networking service    |\n",
      "|nan                         |Facebook, Inc. - Social network company |\n",
      "|nan                         |Amazon.com - E-commerce company         |\n",
      "|nan                         |Definition - Topic                      |\n",
      "|nan                         |Film - Topic                            |\n",
      "|nan                         |United States - Country in North America|\n",
      "|nan                         |Brooklyn - New York City borough        |\n",
      "|nan                         |Restaurant - Topic                      |\n",
      "+----------------------------+----------------------------------------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_google_trend_title(frame_start_datetime, frame_start_datetime, \"Search topics\", geo)\n",
    "topics_df.select(\"Search topics - rising\", \"Search topics - top\").show(num_of_top_interest, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Search queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_df = pytrend.related_top_search_queries(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Google trends Search queries in New York during 2019-06-12\n",
      "+-----------------------+--------------------+------+---+-----+\n",
      "|Search queries - rising|Search queries - top|Rising|Top|geo  |\n",
      "+-----------------------+--------------------+------+---+-----+\n",
      "|max landis             |google              |+500% |100|US-NY|\n",
      "|toy story 4            |weather             |+160% |77 |US-NY|\n",
      "|iready                 |facebook            |+160% |56 |US-NY|\n",
      "|elizabeth lederer      |youtube             |+140% |53 |US-NY|\n",
      "|jon stewart            |amazon              |+130% |49 |US-NY|\n",
      "|hong kong              |news                |+130% |46 |US-NY|\n",
      "|boston bruins          |world cup           |+120% |27 |US-NY|\n",
      "|dallas cowboys         |craigslist          |+110% |24 |US-NY|\n",
      "|inmate lookup          |instagram           |+90%  |24 |US-NY|\n",
      "|julianne hough         |translate           |+80%  |22 |US-NY|\n",
      "|espn mlb               |gmail               |+80%  |21 |US-NY|\n",
      "|nbt bank login         |nba                 |+80%  |21 |US-NY|\n",
      "|soundcloud             |yankees             |+70%  |20 |US-NY|\n",
      "|ny lottery post        |go                  |+70%  |19 |US-NY|\n",
      "|groupon                |calculator          |+60%  |18 |US-NY|\n",
      "+-----------------------+--------------------+------+---+-----+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_google_trend_title(frame_start_datetime, frame_start_datetime, \"Search queries\", geo)\n",
    "queries_df.show(num_of_top_interest, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation and justification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
