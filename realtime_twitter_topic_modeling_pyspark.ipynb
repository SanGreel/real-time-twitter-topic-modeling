{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement \n",
    "**TODO: describe the problem**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML problem\n",
    "**TODO: describel our ml approach**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data description\n",
    "In order to collect data in a natural way:\n",
    "<br>- we registered Twitter Developer account;\n",
    "<br>- using credentials from Twitter Developer account we run script that collected tweets by the geolocation and saved them in mongodb;\n",
    "<br>\n",
    "<br><b>As a result:</b>\n",
    "<br>- we collected  332548 tweets (10Gb in mongodb, ~100Mb in csv) from New-York geolocation since 30 of May up to 15 of June;\n",
    "<br>- we collected  6617029 tweets (~1.69Gb in csv) from USA geolocation since 15 of June up to now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's start our demo ride"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import all libs needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T13:34:48.681195Z",
     "start_time": "2019-07-05T13:34:48.670618Z"
    }
   },
   "outputs": [],
   "source": [
    "# crucial thing\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T13:34:49.159490Z",
     "start_time": "2019-07-05T13:34:49.142765Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'variables'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-2574c2866f10>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# custom text preprocessing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_preprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# custom tools to work with google trends\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/real-time-twitter-topic-modeling/utils/text_preprocessing.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mvariables\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mchannels_not_to_consider\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'stopwords'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'variables'"
     ]
    }
   ],
   "source": [
    "# essential pyspark\n",
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import ArrayType, FloatType, StringType, IntegerType, StructField, StructType\n",
    "from pyspark.sql.functions import udf, row_number,column\n",
    "\n",
    "# vectorizer\n",
    "from pyspark.ml.feature import CountVectorizer, StopWordsRemover, HashingTF, IDF, Tokenizer\n",
    "\n",
    "# staff for LDA\n",
    "from pyspark.mllib.clustering import LDA, LDAModel\n",
    "from pyspark.mllib.linalg import Vector as oldVector, Vectors as oldVectors\n",
    "from pyspark.ml.linalg import Vector as newVector, Vectors as newVectors\n",
    "\n",
    "# pytrends for acquiring google trends\n",
    "from pytrends.pytrends.request import TrendReq\n",
    "\n",
    "# import hardcoded variables\n",
    "from utils.channels_to_filter import channels_not_to_consider\n",
    "\n",
    "# custom text preprocessing\n",
    "from utils.text_preprocessing import *\n",
    "\n",
    "# custom tools to work with google trends \n",
    "from utils.trends import *\n",
    "\n",
    "# handy functions\n",
    "from utils import *\n",
    "\n",
    "# datetime handling\n",
    "from datetime import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Connecting to data**  \n",
    "Please, specify path to the csv file with data right here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to CSV\n",
    "historical_tweets_data = '../data/tweets/new_york_training_tweets_15_06.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Time frames to pick data from**  \n",
    "We picked some time frames to get data from to check if our topic model can extract info about events that occured during this period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final of league championship \n",
    "lc_final_start_datetime = \"Sat Jun 01 00:00:00 +0000 2019\"\n",
    "lc_final_finish_datetime = \"Sat Jun 01 23:59:59 +0000 2019\"\n",
    "\n",
    "# Stanley cup final\n",
    "stanley_final_start_datetime = \"Wed Jun 12 00:00:00 +0000 2019\"\n",
    "stanley_final_finish_datetime = \"Wed Jun 12 23:59:59 +0000 2019\"\n",
    "\n",
    "# Draft NBA\n",
    "nba_final_start_datetime = \"Thu Jun 20 00:00:00 +0000 2019\"\n",
    "nba_finish_final_datetime = \"Sun Jun 23 23:59:59 +0000 2019\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or you can specify your own dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_datetime = ''\n",
    "finish_datetime = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use one of the dates defined above right here (e.g., we use Stanely cup final dates)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'str_tweet_to_datetime' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-c049e6026b1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mframe_start_datetime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr_tweet_to_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstanley_final_start_datetime\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mframe_finish_datetime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr_tweet_to_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstanley_final_finish_datetime\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mframe_finish_datetime\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mframe_start_datetime\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdays\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Date interval should not be bigger than 3 days\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'str_tweet_to_datetime' is not defined"
     ]
    }
   ],
   "source": [
    "frame_start_datetime = str_tweet_to_datetime(stanley_final_start_datetime)\n",
    "frame_finish_datetime = str_tweet_to_datetime(stanley_final_finish_datetime)\n",
    "\n",
    "assert (frame_finish_datetime - frame_start_datetime).days <= 3, \"Date interval should not be bigger than 3 days\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Location related variables**  \n",
    "Here you can specify exact location from which you want to get tweets for topic modeling. Here we have example for NYC (local approach) and whole US (global approach). You can explore raw data to find more locations for filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T13:41:05.605577Z",
     "start_time": "2019-07-05T13:41:05.601192Z"
    }
   },
   "outputs": [],
   "source": [
    "# if True locations from locations_to_consider will be used to filter\n",
    "get_from_location = True\n",
    "\n",
    "# locations to filter relevant tweets\n",
    "locations_to_consider = [\n",
    "                         'Manhattan, NY', \n",
    "                         'Brooklyn, NY', \n",
    "                         'Queens, NY', \n",
    "                         'Bronx, NY', \n",
    "                         'Staten Island, NY'\n",
    "                         'New York, USA'\n",
    "                        ]\n",
    "\n",
    "# used to extract google trends\n",
    "if get_from_location:\n",
    "    geo = 'US-NY' \n",
    "else:\n",
    "    geo = 'US'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Magic machine learning variables (parameters)**  \n",
    "You can tune those parameters to improve output of LDA. But we don't recommend to change them as we ran a lot of experiments and came up with the best configuration :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T13:38:36.620763Z",
     "start_time": "2019-07-05T13:38:36.617592Z"
    }
   },
   "outputs": [],
   "source": [
    "# LDA params\n",
    "num_of_topics_LDA = 20\n",
    "max_iterations_LDA = 10\n",
    "\n",
    "number_of_words_per_topic = 15  # number of words per topic\n",
    "num_of_top_interest = 15 # number of topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T13:38:37.435833Z",
     "start_time": "2019-07-05T13:38:37.427311Z"
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"pipeline\").getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the historical data, it can take a while**  \n",
    "Here we load data and filter by dates you chose above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T13:38:45.189768Z",
     "start_time": "2019-07-05T13:38:45.184190Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time range to be extracted from  ../get-tweets-by-geolocation/data/new_york_training_tweets_15_06.csv 2019-06-12 00:00:00+00:00 2019-06-12 23:59:59+00:00\n",
      "Range for collected data (history):  2019-06-12 00:00:00+00:00 2019-06-12 23:59:59+00:00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "29538"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "times = (frame_start_datetime, frame_finish_datetime)\n",
    "print(\"Time range to be extracted from \", historical_tweets_data, times[0], times[1])\n",
    "selected_df = get_historical_df(historical_tweets_data=historical_tweets_data, historical_start_time=times[0], historical_finish_time=times[1], spark=spark)\n",
    "assert selected_df != None, \"Something goes wrong with selecting data from recent data/history data\"\n",
    "selected_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Google Trends data**  \n",
    "Here we load google trends data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T13:38:43.580633Z",
     "start_time": "2019-07-05T13:38:43.526264Z"
    }
   },
   "outputs": [],
   "source": [
    "google_trends_search_queries_us = spark.read.csv('../data/google-trends/google-trends-search-queries-US.csv', inferSchema=True, header=True)\n",
    "google_trends_search_topics_us = spark.read.csv('../data/google-trends/google-trends-search-topics-US.csv', inferSchema=True, header=True)\n",
    "google_trends_search_queries_us_ny = spark.read.csv('../data/google-trends/google-trends-search-queries-US-NY.csv', inferSchema=True, header=True)\n",
    "google_trends_search_topics_us_ny = spark.read.csv('../data/google-trends/google-trends-search-topics-US-NY.csv', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_trends_dict = {}\n",
    "google_trends_dict['US'] = [google_trends_search_topics_us, google_trends_search_queries_us]\n",
    "google_trends_dict['US-NY'] = [google_trends_search_topics_us_ny, google_trends_search_queries_us_ny]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweets preprocessing  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Basic filtering**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we do basic filtering based on null values. We filter out channels we don't want to consider (we pick them manually based on our EDA), because they provide a lot of noise to the data (typical examples are profiles, which report weather or traffic condition). Also, we do filtering based on global and local location (if local filtering is enabled). Finally, we check tweet itself for the length of the message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = selected_df\n",
    "\n",
    "# filter nans\n",
    "df = df.rdd.filter(lambda x: x[0] != None and x[1] != None and x[2] != None and x[4] != None)\n",
    "\n",
    "# filter out channels not to consider\n",
    "df = df.filter(lambda x: x[4] not in channels_not_to_consider)\n",
    "\n",
    "# filter by country\n",
    "df = df.filter(lambda x: x[1] in 'US')\n",
    "\n",
    "# filter by precise location\n",
    "if get_from_location:\n",
    "    df = df.filter(lambda x: x[2] in locations_to_consider)\n",
    "\n",
    "# filter tweet itself\n",
    "df = df.filter(lambda x: filter_tweet(x[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now the most interesting - tweet cleaning**  \n",
    "Text cleaning is crucial for any text modelling process, especially for topic modelling. We tried three different approaches: classic, using only hashtags, using only urls. In classic approach we delete all non-words (including urls, hashtags, emojis and mentions), filter out common stop words, so only plain text information is left. But still such data has a lot of noise and uninformative words, so we tried another approaches with using only hashtags (which should code the most important information) and urls."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's start from classic approach**  \n",
    "In our case it consists from those steps:  \n",
    "1) Lowercase all words  \n",
    "2) Filter words with non-letters at the beginning (mainly for mentions, e.g. \"@some_user\")  \n",
    "3) Filter http/https  \n",
    "4) Filter all non-letters (crucial to remove emoji)  \n",
    "5) Remove multiply whitespaces  \n",
    "6) Remove repeated chars (e.g. \"greeeeat\" -> \"great\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hashtags & URLs**  \n",
    "Those approaches are pretty clear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Basically applying approach discussed above**  \n",
    "Now let's apply one of the aproaches. But first specify which one you want to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_hashtags(tweet):\n",
    "    pass\n",
    "\n",
    "def process_urls(tweet):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_type = 'just-text' # or 'hashtags', or 'urls'\n",
    "\n",
    "if preprocessing_type == 'just-text':\n",
    "    process_tweet = process_text\n",
    "elif preprocessing_type == 'hashtags':\n",
    "    process_tweet = process_hashtags\n",
    "elif preprocessing_type == 'urls':\n",
    "    process_tweet = process_urls\n",
    "\n",
    "# process tweet\n",
    "df = df.map(lambda x: process_tweet(x[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Final postprocessing**  \n",
    "Here we make sure that we don't have entries without tokens at all, also we change the structure suitable for pyspark LDA class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final preprocesssing\n",
    "df = df.filter(lambda x: len(x) > 0)\n",
    "\n",
    "# make dataframes great again\n",
    "df = df.map(lambda x: [x])\n",
    "\n",
    "# schema for df\n",
    "schema = StructType([StructField('tokens', ArrayType(StringType()), True)])\n",
    "df = df.toDF(schema=schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic modeling via Latent Dirichlet allocation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2.show(10, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(time.strftime('%m%d%Y %H:%M:%S'))\n",
    "\n",
    "cv = CountVectorizer(inputCol=\"tokens\", outputCol=\"raw_features\", vocabSize=10000, minDF=2.0)\n",
    "cvmodel = cv.fit(df)\n",
    "\n",
    "print(time.strftime('%m%d%Y %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(time.strftime('%m%d%Y %H:%M:%S'))\n",
    "df = cvmodel.transform(df)\n",
    "print(time.strftime('%m%d%Y %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"tf_idf_features\", minDocFreq=2)\n",
    "idfModel = idf.fit(df)\n",
    "\n",
    "df = idfModel.transform(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(10, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.drop(\"name\")\n",
    "#df.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = Window().orderBy(column(\"tokens\"))\n",
    "df = df.withColumn(\"id\", row_number().over(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(10, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = df.rdd.map(lambda x: (x[3], oldVectors.fromML(x[2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_df = rs.toDF()\n",
    "rs_df.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the LDA Topic Modeler\n",
    "# Note the time before and after is printed in order to find out how much time it takes to process x number of records\n",
    "\n",
    "\n",
    "print(time.strftime('%m%d%Y %H:%M:%S'))\n",
    "lda_model = LDA.train(rs_df['_1', '_2'].rdd.map(list), k=num_of_topics_LDA, maxIterations=max_iterations_LDA)\n",
    "print(time.strftime('%m%d%Y %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(time.strftime('%m%d%Y %H:%M:%S'))\n",
    "topics = lda_model.topicsMatrix()\n",
    "vocabArray = cvmodel.vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordNumbers = 15\n",
    "\n",
    "topicIndices = sc.parallelize(lda_model.describeTopics(maxTermsPerTopic = number_of_words_per_topic))\n",
    "\n",
    "def topic_render(topic):  # specify vector id of words to actual words\n",
    "    terms = topic[0]\n",
    "    prob = topic[1]\n",
    "    \n",
    "    result = []\n",
    "    for i in range(number_of_words_per_topic):\n",
    "        term = str(round(prob[i],3))+\"  \"+vocabArray[terms[i]]\n",
    "        result.append(term)\n",
    "    return result\n",
    "print(time.strftime('%m%d%Y %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(time.strftime('%m%d%Y %H:%M:%S'))\n",
    "topics_final = topicIndices.map(lambda topic:topic_render(topic)).collect()\n",
    "print(time.strftime('%m%d%Y %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# based on the simple vectors(+number of words)\n",
    "\n",
    "for topic in range(len(topics_final)):\n",
    "    print (\"Topic #\" + str(topic+1) + \"\")\n",
    "    for term in topics_final[topic]:\n",
    "        print (term)\n",
    "    print ('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hot topics in the USA from [Google trends](https://trends.google.com/trends/explore?geo=US)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = frame_start_datetime #str_tweet_to_datetime(frame_start_datetime)\n",
    "finish_date = frame_finish_datetime #str_tweet_to_datetime(frame_finish_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_trends_topics, google_trends_queries = get_google_trends_by_geo(geo, google_trends_dict) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Google trends search queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "interesting_google_topics = google_trends_topics.filter(\n",
    "    (google_trends_topics.Date >= start_date) & (google_trends_topics.Date <= finish_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Google trends Search topics in New York during 2019-06-12 - 2019-06-12\n",
      "+----------+----------------------+----------------------------------------+\n",
      "|Date      |Search topics - rising|Search topics - top                     |\n",
      "+----------+----------------------+----------------------------------------+\n",
      "|2019-06-12|Download - Topic      |New York - City in New York             |\n",
      "|2019-06-12|null                  |New York - US State                     |\n",
      "|2019-06-12|null                  |Google Search - Topic                   |\n",
      "|2019-06-12|null                  |Google - Technology company             |\n",
      "|2019-06-12|null                  |2019 - Topic                            |\n",
      "|2019-06-12|null                  |Weather - Topic                         |\n",
      "|2019-06-12|null                  |YouTube - Video sharing company         |\n",
      "|2019-06-12|null                  |Facebook, Inc. - Social network company |\n",
      "|2019-06-12|null                  |Facebook - Social networking service    |\n",
      "|2019-06-12|null                  |Amazon.com - E-commerce company         |\n",
      "|2019-06-12|null                  |Film - Topic                            |\n",
      "|2019-06-12|null                  |Definition - Topic                      |\n",
      "|2019-06-12|null                  |Brooklyn - New York City borough        |\n",
      "|2019-06-12|null                  |United States - Country in North America|\n",
      "|2019-06-12|null                  |New Jersey - US State                   |\n",
      "+----------+----------------------+----------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_google_trend_title(start_date, finish_date, \"Search topics\", geo)\n",
    "interest_google_topics = convert_datetime_in_interesting_google(interesting_google_topics)\n",
    "interest_google_topics.select(\"Date\",\"Search topics - rising\", \"Search topics - top\").show(num_of_top_interest, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case when timeframe is more than 1 day, filter correctly this google-trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interesing_google_topics_unique= unique_google_trends_by_time_frame(interesting_google_topics, geo)\n",
    "# print_google_trend_title(start_date, finish_date, \"Search topics\")\n",
    "# interesing_google_topics_unique.select(\"Search topics - rising\", \"Search topics - top\").show(num_of_top_interest, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Google trends search queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "interesting_google_queries = google_trends_queries.filter(\n",
    "    (google_trends_queries.Date >= start_date) & (google_trends_queries.Date <= finish_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Google trends Search queries in New York during 2019-06-12 - 2019-06-12\n",
      "+-----------------------+--------------------+------+---+-----+\n",
      "|Search queries - rising|Search queries - top|Rising|Top|geo  |\n",
      "+-----------------------+--------------------+------+---+-----+\n",
      "|sudanese massacre      |google              |+300% |100|US-NY|\n",
      "|hong kong              |weather             |+250% |79 |US-NY|\n",
      "|ny yankees score       |facebook            |+180% |59 |US-NY|\n",
      "|iready                 |youtube             |+150% |54 |US-NY|\n",
      "|boston bruins          |amazon              |+150% |49 |US-NY|\n",
      "|raz kids               |news                |+110% |46 |US-NY|\n",
      "|bruins                 |definition          |+110% |33 |US-NY|\n",
      "|cool math games        |craigslist          |+100% |28 |US-NY|\n",
      "|verizon wireless       |gmail               |+90%  |24 |US-NY|\n",
      "|bitcoin price          |translate           |+90%  |23 |US-NY|\n",
      "|scratch                |nba                 |+90%  |23 |US-NY|\n",
      "|max landis             |instagram           |+90%  |22 |US-NY|\n",
      "|sudan                  |yankees             |+80%  |21 |US-NY|\n",
      "|nhl playoffs           |people              |+80%  |21 |US-NY|\n",
      "|trump news             |yahoo               |+70%  |19 |US-NY|\n",
      "+-----------------------+--------------------+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "interesing_google_queries_unique= unique_google_trends_by_time_frame(interesting_google_queries, spark)\n",
    "print_google_trend_title(start_date, finish_date, \"Search queries\", geo)\n",
    "interesing_google_queries_unique.show(num_of_top_interest, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print_google_trend_title(start_date, finish_date, \"Search queries\")\n",
    "# interest_google_queries = convert_datetime_in_interesting_google(interesting_google_queries)\n",
    "# interest_google_queries.select(\"Date\", \"Search queries - rising\", \"Search queries - top\").show(num_of_top_interest, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hot topics - google trends (directly) (probably this will be removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date_str = start_date.strftime(\"%Y-%m-%d\")\n",
    "finish_date_str = finish_date.strftime(\"%Y-%m-%d\")\n",
    "pytrend = TrendReq()\n",
    "pytrend.build_payload(kw_list=[' '], geo=geo, timeframe=f\"{start_date_str} {finish_date_str}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Search topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_df = pytrend.related_top_search_topics(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Google trends Search topics in New York during 2019-06-12 - 2019-06-12\n",
      "+----------------------+----------------------------------------+\n",
      "|Search topics - rising|Search topics - top                     |\n",
      "+----------------------+----------------------------------------+\n",
      "|nan                   |New York - City in New York             |\n",
      "|nan                   |New York - US State                     |\n",
      "|nan                   |Google Search - Topic                   |\n",
      "|nan                   |Google - Technology company             |\n",
      "|nan                   |2019 - Topic                            |\n",
      "|nan                   |Weather - Topic                         |\n",
      "|nan                   |YouTube - Video sharing company         |\n",
      "|nan                   |Facebook - Social networking service    |\n",
      "|nan                   |Facebook, Inc. - Social network company |\n",
      "|nan                   |Amazon.com - E-commerce company         |\n",
      "|nan                   |Film - Topic                            |\n",
      "|nan                   |Definition - Topic                      |\n",
      "|nan                   |Brooklyn - New York City borough        |\n",
      "|nan                   |United States - Country in North America|\n",
      "|nan                   |Restaurant - Topic                      |\n",
      "+----------------------+----------------------------------------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_google_trend_title(start_date, finish_date, \"Search topics\", geo)\n",
    "topics_df.select(\"Search topics - rising\", \"Search topics - top\").show(num_of_top_interest, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Search queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_df = pytrend.related_top_search_queries(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Google trends Search queries in New York during 2019-06-12 - 2019-06-12\n",
      "+-----------------------+--------------------+------+---+-----+\n",
      "|Search queries - rising|Search queries - top|Rising|Top|geo  |\n",
      "+-----------------------+--------------------+------+---+-----+\n",
      "|britt mchenry          |google              |+750% |100|US-NY|\n",
      "|courtney stodden       |weather             |+450% |71 |US-NY|\n",
      "|mega millions          |facebook            |+300% |58 |US-NY|\n",
      "|hong kong              |youtube             |+250% |50 |US-NY|\n",
      "|jon stewart            |amazon              |+150% |46 |US-NY|\n",
      "|khan academy           |news                |+150% |44 |US-NY|\n",
      "|john stewart           |craigslist          |+140% |26 |US-NY|\n",
      "|elizabeth lederer      |instagram           |+140% |23 |US-NY|\n",
      "|icc world cup 2019     |nba                 |+140% |22 |US-NY|\n",
      "|bruins                 |translate           |+120% |21 |US-NY|\n",
      "|boston bruins          |gmail               |+110% |20 |US-NY|\n",
      "|run 3                  |drive               |+100% |20 |US-NY|\n",
      "|snake game             |yankees             |+100% |19 |US-NY|\n",
      "|us open                |yahoo               |+90%  |19 |US-NY|\n",
      "|mlb standings          |apple               |+90%  |18 |US-NY|\n",
      "+-----------------------+--------------------+------+---+-----+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_google_trend_title(start_date, finish_date, \"Search queries\", geo)\n",
    "queries_df.show(num_of_top_interest, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation and justification (Conclusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
