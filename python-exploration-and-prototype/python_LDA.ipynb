{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import itertools\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tweets_LDA():\n",
    "    \n",
    "    '''\n",
    "    Class gets as input raw-data from twitter in csv format.\n",
    "    There realized methods for data preprocessing and perfoming LDA on the preprocessed data.\n",
    "    As output - words that are indicators of particular topic for tweets.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self,):\n",
    "        self.path_to_file = ''\n",
    "        self.tweets_frame = pd.DataFrame()\n",
    "        \n",
    "    def load_data(self,path_to_file):\n",
    "        self.path_to_file = path_to_file\n",
    "        self.tweets_frame = pd.read_csv(self.path_to_file)\n",
    "        \n",
    "        \n",
    "    def data_preprocessing(self,date='',channels_not_to_consider=[]):\n",
    "        '''\n",
    "        date format example: 'Sat Jun 01 00:00:02'\n",
    "        '''\n",
    "        WPT = nltk.WordPunctTokenizer()\n",
    "        lemmanizer = WordNetLemmatizer()\n",
    "        stop_word_list = nltk.corpus.stopwords.words('english')\n",
    "        \n",
    "        if date!='':\n",
    "            self.tweets_frame = self.tweets_frame[self.tweets_frame['created_at'].map(lambda x: x[:len(date)])==date]\n",
    "        if len(channels_not_to_consider)>0:\n",
    "            self.tweets_frame = self.tweets_frame[self.tweets_frame['screen_name'].isin(channels_not_to_consider)==False]\n",
    "            self.tweets_frame = self.tweets_frame[self.tweets_frame['screen_name'].map(lambda x: x.startswith('tmj_'))==False]\n",
    "        \n",
    "        self.tweets_frame['tweets_processed'] =\\\n",
    "                self.tweets_frame['tweet'].map(lambda x: [lemmanizer.lemmatize(y) for y in re.sub(\"[\\d+0-9.â€¦#!'\\\"_?,;/:()â€™%*ðŸ¤¯â€œâ€&ðŸ§¨$ðŸ§¨ðŸ§¡]\", \"\", x.lower()).split() \n",
    "                                         if y not in stop_word_list \n",
    "                                         and y not in ['ãƒ»ãƒ»ãƒ»','','-']\n",
    "                                         and not y.startswith('http')\n",
    "                                         and not y.startswith('@')])\n",
    "        \n",
    "        self.tweets_frame['tweets_processed'] =\\\n",
    "                self.tweets_frame['tweets_processed'].map(lambda x: [y for y in x if len(y)>2])\n",
    "        \n",
    "        self.tweets_frame = self.tweets_frame[self.tweets_frame['tweets_processed'].map(len)>0]\n",
    "        \n",
    "    def LDA(self,n_topics):\n",
    "        \n",
    "        def create_co_occurences_matrix(allowed_words, documents):\n",
    "            word_to_id = dict(zip(allowed_words, range(len(allowed_words))))\n",
    "            documents_as_ids = [np.sort([word_to_id[w] for w in doc if w in word_to_id]).astype('uint32') for doc in documents]\n",
    "            row_ind, col_ind = zip(*itertools.chain(*[[(i, w) for w in doc] for i, doc in enumerate(documents_as_ids)]))\n",
    "            data = np.ones(len(row_ind), dtype='uint32')  # use unsigned int for better memory utilization\n",
    "            max_word_id = max(itertools.chain(*documents_as_ids)) + 1\n",
    "            docs_words_matrix = csr_matrix((data, (row_ind, col_ind)), shape=(len(documents_as_ids), max_word_id))  # efficient arithmetic operations with CSR * CSR\n",
    "            words_cooc_matrix = docs_words_matrix.T * docs_words_matrix  # multiplying docs_words_matrix with its transpose matrix would generate the co-occurences matrix\n",
    "            words_cooc_matrix.setdiag(0)\n",
    "            return words_cooc_matrix, word_to_id \n",
    "        \n",
    "        def print_topics(model, count_vectorizer, n_top_words,words):\n",
    "            #words = cross_tab.columns\n",
    "            for topic_idx, topic in enumerate(model.components_):\n",
    "                print(\"\\nTopic #%d:\" % topic_idx)\n",
    "                print(\" \".join([words[i]+':'+str(round(topic[i]/topic.max(),4))\n",
    "                                for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "        \n",
    "        merged = list(itertools.chain.from_iterable(self.tweets_frame['tweets_processed'].values))\n",
    "        merged_cnts = np.unique(merged,return_counts=True)\n",
    "        words = merged_cnts[0][merged_cnts[1]>2]\n",
    "        a, b = create_co_occurences_matrix(words,self.tweets_frame['tweets_processed'].values)\n",
    "        lda_ = LDA(n_components=n_topics)\n",
    "        lda_.fit(a)\n",
    "        print_topics(lda_,' ',10,words)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweets_frame = pd.read_csv('../data/tweets/new_york_training_tweets_15_06.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Tweets_LDA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.load_data('../data/tweets/new_york_training_tweets_15_06.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "332548"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a.tweets_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels_not_to_consider = [#traffic\n",
    "        '511NY', '511NYC', 'TotalTrafficNYC', '511nyNJ', '511NYMidHudson',\n",
    "       'Reported_NYC', '511ny456', '511nyAlbany', '511nyACE', '511ny123', \n",
    "        '511nyBDFV', '511nyWNY', '511nyRochester',\n",
    "       '511nyLongIsland', 'AllKindsWeather', \n",
    "       '511ny7',\n",
    "    #jobs\n",
    "    'tmj_nyc_jobs', 'tmj_nyc_adv', 'CalvaryHospJobs',\n",
    "       'tmj_RAM_cstsrv', 'tmj_nyc_mgmt', 'tmj_roc_cler',\n",
    "       'tmj_NAS_edu', 'tmj_nyc_it', 'tmj_NYC_schn', 'tmj_NJN_cstsrv',\n",
    "       'tmj_nwk_retail', 'tmj_nyc_nursing', 'tmj_nyc_sales', 'tmj_nyc_cler',\n",
    "       'USSJobs', 'tmj_nwk_sales', 'tmj_nyc_legal', 'tmj_nwk_eng',\n",
    "       'tmj_nyc_retail', 'tmj_nyc_transp', 'WGPNursingJobs', 'tmj_nwk_socsci',\n",
    "       'tmj_NYC_adm', 'nwkmeddevice', 'tmj_nyc_edu', 'tmj_nwk_secure',\n",
    "       'nwknursing',\n",
    "    'tmj_nyc_finance', 'tmj_NYC_secure', 'tmj_nyc_acct', 'tmj_nwk_cler',\n",
    "       'tmj_nyc_banking', 'tmj_ny_hrta', 'tmj_nwk_mgmt', 'tmj_nwk_auto',\n",
    "       'tmj_nyc_cstsrv', 'tmj_nyc_health', 'nwkhealth', 'tmj_nwk_cstsrv',\n",
    "       'tmj_nyc_manuf', 'tmj_cte_nursing', 'tmj_nyc_eng', 'tmj_nyc_itpm1',\n",
    "       'tmj_nyc_hr', 'tmj_NY_sales', 'CVSHealthJobs', 'tmj_NAS_mgmt',\n",
    "       'CompassJobBoard', 'tmj_nyc_hrta', 'tmj_NAS_nursing', 'BostonMarketJob',\n",
    "       'tmj_nyc_labor', 'MetsAvenue', 'tmj_nwk_schn', 'tmj_RAM_nursing',\n",
    "       'tmj_nwk_acct', 'tmj_nwk_jobs', 'tmj_nyc_art', 'tmj_nwk_labor',\n",
    "       'tmj_roc_eng', 'tmj_NAS_transp', 'CA_ROC_Jobs2', 'tmj_nyc_cosmo',\n",
    "       'tmj_RAM_edu', 'tmj_NAS_health', 'tmj_nwk_facmgmt', 'tmj_NAS_facmgmt',\n",
    "       'GodivaJobs', 'tmj_RAM_acct', 'tmj_roc_health', 'tmj_nyc_itdb',\n",
    "       'tmj_nwk_transp', 'tmj_nwk_edu', 'tmj_RAM_retail', 'tmj_RAM_mgmt',\n",
    "       'tmj_NAS_socsci', 'tmj_nwk_prod', 'tmj_nyc_realest', 'tmj_NJ_facmgmt', 'tmj_njn_retail',\n",
    "       'tmj_roc_nursing', 'tmj_nwk_finance', 'Fly_Sistah', 'tmj_NYC_skltrd',\n",
    "       'tmj_nya_nursing', 'tmj_nwk_web', 'tmj_roc_cstsrv', 'tmj_nys_jobs',\n",
    "       'tmj_njc_hrta', 'tmj_NAS_retail', 'tmj_roc_hrta',\n",
    "       'ChurchCathy', 'tmj_NAS_secure', 'tmj_RAM_art', 'tmj_NAS_labor',\n",
    "       'tmj_NAS_physici', 'tmj_nwk_skltrd', 'tmj_roc_sales', 'tmj_nwk_purch',\n",
    "       'tmj_NYS_NURSING', 'tmj_nwk_physici', 'tmj_njn_hrta', 'Mezikenyc',\n",
    "       'JCI_Jobs', 'tmj_NAS_acct', 'tmj_NYC_gensci', 'tmj_nya_eng',\n",
    "       'tmj_nwk_nonprft', 'tmj_roc_manuf', 'nwkitsupport', 'tmj_NY_LABOR',\n",
    "       'tmj_ny_mgmt', 'tmj_njn_health',\n",
    "       'tmj_nj_hrta', 'tmj_NAS_cstsrv', 'tmj_nwk_it', 'tmj_nya_transp',\n",
    "       'tmj_ct_nursing', 'tmj_NJ_sales', 'tmj_nya_acct',\n",
    "       'nwkmanuf', 'tmj_nys_cstsrv', 'tmj_njn_nursing',\n",
    "       'tmj_njn_mgmt', 'cbwaszak', 'tmj_NAS_cler', 'tmj_RAM_auto',\n",
    "       'tmj_nwk_art'\n",
    "    'WWEWomenMatter',\n",
    "    #photos\n",
    "    'ThomGambino', 'Xsanthemum', 'francesco212', 'Empressjurnee',\n",
    "       'andrerivera801', 'janice830', 'Ingridebap', 'StevieSoFetch_',\n",
    "       'EstebanDaHost', 'graceyhanderson', 'bccdny', 'brian_wood_'\n",
    "       ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.data_preprocessing('Sat Jun 01', channels_not_to_consider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sat Jun 01 16:55:57 +0000 2019    8\n",
       "Sat Jun 01 21:39:15 +0000 2019    7\n",
       "Sat Jun 01 02:56:51 +0000 2019    6\n",
       "Sat Jun 01 03:24:33 +0000 2019    6\n",
       "Sat Jun 01 17:06:55 +0000 2019    6\n",
       "Sat Jun 01 19:19:02 +0000 2019    6\n",
       "Sat Jun 01 16:16:54 +0000 2019    6\n",
       "Sat Jun 01 18:40:10 +0000 2019    6\n",
       "Sat Jun 01 14:43:27 +0000 2019    6\n",
       "Sat Jun 01 23:37:29 +0000 2019    6\n",
       "Sat Jun 01 00:50:53 +0000 2019    6\n",
       "Sat Jun 01 21:01:13 +0000 2019    6\n",
       "Sat Jun 01 00:28:40 +0000 2019    6\n",
       "Sat Jun 01 20:49:51 +0000 2019    6\n",
       "Sat Jun 01 02:13:29 +0000 2019    6\n",
       "Sat Jun 01 16:51:04 +0000 2019    6\n",
       "Sat Jun 01 15:17:23 +0000 2019    5\n",
       "Sat Jun 01 19:46:56 +0000 2019    5\n",
       "Sat Jun 01 13:57:51 +0000 2019    5\n",
       "Sat Jun 01 02:52:13 +0000 2019    5\n",
       "Sat Jun 01 00:49:17 +0000 2019    5\n",
       "Sat Jun 01 13:18:48 +0000 2019    5\n",
       "Sat Jun 01 18:55:23 +0000 2019    5\n",
       "Sat Jun 01 22:17:14 +0000 2019    5\n",
       "Sat Jun 01 21:16:11 +0000 2019    5\n",
       "Sat Jun 01 23:44:39 +0000 2019    5\n",
       "Sat Jun 01 16:26:32 +0000 2019    5\n",
       "Sat Jun 01 17:59:11 +0000 2019    5\n",
       "Sat Jun 01 19:12:12 +0000 2019    5\n",
       "Sat Jun 01 02:13:01 +0000 2019    5\n",
       "                                 ..\n",
       "Sat Jun 01 15:41:21 +0000 2019    1\n",
       "Sat Jun 01 18:37:55 +0000 2019    1\n",
       "Sat Jun 01 03:49:45 +0000 2019    1\n",
       "Sat Jun 01 18:00:39 +0000 2019    1\n",
       "Sat Jun 01 18:08:48 +0000 2019    1\n",
       "Sat Jun 01 14:49:59 +0000 2019    1\n",
       "Sat Jun 01 22:13:02 +0000 2019    1\n",
       "Sat Jun 01 04:55:27 +0000 2019    1\n",
       "Sat Jun 01 05:09:33 +0000 2019    1\n",
       "Sat Jun 01 19:36:20 +0000 2019    1\n",
       "Sat Jun 01 22:18:53 +0000 2019    1\n",
       "Sat Jun 01 13:36:06 +0000 2019    1\n",
       "Sat Jun 01 10:41:39 +0000 2019    1\n",
       "Sat Jun 01 17:54:59 +0000 2019    1\n",
       "Sat Jun 01 08:10:01 +0000 2019    1\n",
       "Sat Jun 01 23:12:49 +0000 2019    1\n",
       "Sat Jun 01 11:23:19 +0000 2019    1\n",
       "Sat Jun 01 20:33:50 +0000 2019    1\n",
       "Sat Jun 01 13:27:04 +0000 2019    1\n",
       "Sat Jun 01 17:00:07 +0000 2019    1\n",
       "Sat Jun 01 21:20:10 +0000 2019    1\n",
       "Sat Jun 01 14:43:46 +0000 2019    1\n",
       "Sat Jun 01 19:09:33 +0000 2019    1\n",
       "Sat Jun 01 12:14:26 +0000 2019    1\n",
       "Sat Jun 01 11:55:21 +0000 2019    1\n",
       "Sat Jun 01 00:08:23 +0000 2019    1\n",
       "Sat Jun 01 12:23:37 +0000 2019    1\n",
       "Sat Jun 01 03:03:10 +0000 2019    1\n",
       "Sat Jun 01 17:21:54 +0000 2019    1\n",
       "Sat Jun 01 03:30:44 +0000 2019    1\n",
       "Name: created_at, Length: 38169, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.tweets_frame['created_at'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53875"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a.tweets_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic #0:\n",
      "link:1.0 fashion:0.9745 new:0.9715 newyork:0.797 bio:0.7146 amp:0.5999 t-shirt:0.5848 latest:0.5782 show:0.5707 classic:0.5641\n",
      "\n",
      "Topic #1:\n",
      "one:1.0 time:0.9434 like:0.9012 ive:0.728 get:0.6884 year:0.688 last:0.6632 day:0.6307 good:0.5629 today:0.5565\n",
      "\n",
      "Topic #2:\n",
      "bronx:1.0 like:0.9875 avenue:0.987 one:0.6976 get:0.6975 nxttakeover:0.693 long:0.655 class:0.5378 tonight:0.5173 girl:0.514\n",
      "\n",
      "Topic #3:\n",
      "like:1.0 dont:0.4947 get:0.3808 people:0.3473 look:0.3252 lol:0.3077 know:0.2855 think:0.2772 even:0.2631 would:0.2464\n",
      "\n",
      "Topic #4:\n",
      "america:1.0 bill:0.8836 hbo:0.8532 tit:0.8227 sayin:0.8166 claiming:0.8043 unofficial:0.7954 realtime:0.783 readthatagain:0.7629 ðŸ‘¬ðŸ‘«ðŸ‘­:0.7629\n",
      "\n",
      "Topic #5:\n",
      "happy:1.0 pride:0.8132 month:0.8072 day:0.6471 today:0.5114 amp:0.448 love:0.4464 june:0.3929 great:0.3724 thank:0.35\n",
      "\n",
      "Topic #6:\n",
      "like:1.0 dont:0.8217 get:0.72 know:0.5359 shit:0.4898 love:0.4162 one:0.4079 time:0.3957 really:0.3898 people:0.3745\n",
      "\n",
      "Topic #7:\n",
      "case:1.0 freeship:0.9669 liverpool:0.7813 champion:0.7037 final:0.6613 uclfinal:0.5885 league:0.5648 clear:0.4087 dvd:0.4071 team:0.356\n",
      "\n",
      "Topic #8:\n",
      "trump:1.0 like:0.5899 people:0.5747 amp:0.465 one:0.4594 would:0.4384 dont:0.4227 say:0.3634 think:0.3477 get:0.3389\n",
      "\n",
      "Topic #9:\n",
      "ball:1.0 music:0.6436 governor:0.5176 festival:0.4976 gov:0.4153 day:0.2635 govballnyc:0.1873 govball:0.1866 world:0.1436 island:0.1415\n",
      "\n",
      "Topic #10:\n",
      "yankee:1.0 game:0.5449 red:0.452 sox:0.3719 stadium:0.2959 win:0.2507 baseball:0.224 tonight:0.2195 team:0.1735 mlb:0.1619\n",
      "\n",
      "Topic #11:\n",
      "message:1.0 know:0.8569 office:0.8244 text:0.8222 robert:0.8066 muellers:0.7597 deleted:0.7432 lisa:0.7209 dems:0.4883 scaring:0.3934\n",
      "\n",
      "Topic #12:\n",
      "amp:1.0 shop:0.9736 play:0.939 dance:0.7953 cbd:0.7609 life:0.7474 water:0.6356 color:0.6239 artist:0.5501 show:0.5248\n",
      "\n",
      "Topic #13:\n",
      "chance:1.0 tonight:0.931 t-storm:0.8889 sat:0.8683 jun:0.8003 forecast:0.7928 today:0.4413 sunday:0.4042 mostly:0.402 sunny:0.3794\n",
      "\n",
      "Topic #14:\n",
      "people:1.0 like:0.7048 dont:0.6762 youre:0.5459 one:0.5185 know:0.4814 think:0.3871 get:0.3631 really:0.3145 time:0.291\n",
      "\n",
      "Topic #15:\n",
      "people:1.0 make:0.8863 today:0.8439 dont:0.831 gun:0.8215 get:0.7989 amp:0.7686 need:0.7606 one:0.7047 time:0.6112\n",
      "\n",
      "Topic #16:\n",
      "love:1.0 movie:0.9305 amp:0.6366 fan:0.6035 king:0.6032 godzilla:0.5312 discover:0.4992 one:0.4894 happy:0.4793 monster:0.4667\n",
      "\n",
      "Topic #17:\n",
      "food:1.0 music:0.8718 amp:0.6663 hookah:0.46 get:0.429 valet:0.3877 sugardaddysfridays:0.3877 djfrankswift:0.3877 like:0.386 today:0.3844\n",
      "\n",
      "Topic #18:\n",
      "york:1.0 new:0.5657 photo:0.2279 posted:0.2202 jersey:0.1115 height:0.0968 amp:0.0912 free:0.0912 open:0.0824 saturday:0.0808\n",
      "\n",
      "Topic #19:\n",
      "new:1.0 york:0.4866 nyc:0.3221 park:0.1993 brooklyn:0.1988 city:0.1805 time:0.1313 amp:0.124 jersey:0.1094 day:0.1087\n"
     ]
    }
   ],
   "source": [
    "a.LDA(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.google.com/search?q=liverpool+%D0%BC%D0%B0%D1%82%D1%87&rlz=1C1CHZL_enUA833UA833&oq=liverpool+%D0%BC%D0%B0%D1%82%D1%87+&aqs=chrome..69i57j0l5.8428j0j7&sourceid=chrome&ie=UTF-8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "alternative_date = Tweets_LDA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "alternative_date.load_data('get-tweets-by-geolocation/data/new_york_training_tweets_15_06.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "332548"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(alternative_date.tweets_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Thu Jun 06    67546\n",
       "Sat Jun 01    59087\n",
       "Fri May 31    46951\n",
       "Wed Jun 05    38703\n",
       "Wed Jun 12    29989\n",
       "Sun Jun 02    29291\n",
       "Thu Jun 13    27292\n",
       "Fri Jun 07    16485\n",
       "Tue Jun 04    14549\n",
       "Thu May 30     2655\n",
       "Name: created_at, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alternative_date.tweets_frame['created_at'].map(lambda x: x[:10]).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "alternative_date.data_preprocessing('Thu Jun 13',channels_not_to_consider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24971"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(alternative_date.tweets_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic #0:\n",
      "trump:1.0 would:0.6534 get:0.6515 people:0.594 know:0.5672 dont:0.5633 one:0.5334 amp:0.5299 like:0.4981 say:0.4798\n",
      "\n",
      "Topic #1:\n",
      "amp:1.0 printing:0.6949 free:0.6494 card:0.582 business:0.5169 full:0.4942 new:0.4853 print:0.4441 price:0.4436 flyer:0.3729\n",
      "\n",
      "Topic #2:\n",
      "size:1.0 birthday:0.8868 rare:0.8036 white:0.7967 bid:0.7441 party:0.7313 welcome:0.7195 nike:0.711 hurry-up:0.7064 preowned:0.6657\n",
      "\n",
      "Topic #3:\n",
      "construction:1.0 town:0.9676 bloomfield:0.8781 direction:0.7982 line:0.7671 pulse:0.6917 year:0.4173 street:0.3945 center:0.3859 road:0.3562\n",
      "\n",
      "Topic #4:\n",
      "mph:1.0 wind:0.885 humidity:0.7158 -gt:0.6552 weather:0.5592 pressure:0.5485 sky:0.5244 cloud:0.507 clear:0.5029 current:0.4776\n",
      "\n",
      "Topic #5:\n",
      "cup:1.0 blue:0.8811 stanley:0.6443 boston:0.5735 stanleycup:0.5678 game:0.5438 win:0.524 team:0.464 louis:0.372 year:0.3688\n",
      "\n",
      "Topic #6:\n",
      "time:1.0 amp:0.5801 one:0.4295 real:0.3862 would:0.3442 create:0.2982 year:0.2931 people:0.2919 wednesday:0.2906 need:0.2825\n",
      "\n",
      "Topic #7:\n",
      "game:1.0 love:0.5552 blue:0.5097 fan:0.3867 one:0.3348 team:0.3242 hockey:0.3139 like:0.3063 great:0.3026 tonight:0.2936\n",
      "\n",
      "Topic #8:\n",
      "ive:1.0 one:0.9349 like:0.922 year:0.8419 time:0.8036 never:0.7408 think:0.6952 dont:0.6322 know:0.584 get:0.5543\n",
      "\n",
      "Topic #9:\n",
      "like:1.0 dont:0.7432 get:0.5517 know:0.4524 people:0.3856 one:0.3855 shit:0.3584 really:0.3558 love:0.3376 lol:0.313\n",
      "\n",
      "Topic #10:\n",
      "tonight:1.0 today:0.989 forecast:0.8248 jun:0.7911 thu:0.7856 rain:0.7582 chance:0.7411 shower:0.6132 cloudy:0.373 mostly:0.2905\n",
      "\n",
      "Topic #11:\n",
      "dream:1.0 biden:0.9708 joe:0.7962 please:0.7802 good:0.7657 checked:0.7493 hold:0.6627 palestinian:0.5989 puerto:0.5855 offer:0.5481\n",
      "\n",
      "Topic #12:\n",
      "like:1.0 dont:0.8937 make:0.7132 people:0.7017 cant:0.6141 one:0.5873 know:0.5767 see:0.5546 got:0.4829 link:0.4547\n",
      "\n",
      "Topic #13:\n",
      "new:1.0 york:0.8101 nyc:0.2151 city:0.1847 park:0.1463 brooklyn:0.1303 photo:0.1277 posted:0.112 manhattan:0.1082 street:0.0839\n",
      "\n",
      "Topic #14:\n",
      "day:1.0 get:0.7632 time:0.6872 back:0.6566 see:0.5564 good:0.5545 new:0.5244 love:0.5077 amp:0.4944 got:0.4743\n",
      "\n",
      "Topic #15:\n",
      "day:1.0 one:0.9306 like:0.7997 amp:0.7434 today:0.6095 love:0.5968 year:0.5378 last:0.5284 school:0.5276 time:0.5246\n",
      "\n",
      "Topic #16:\n",
      "case:1.0 freeship:0.874 plastic:0.4791 sleeve:0.4172 dvd:0.346 amp:0.341 black:0.3315 paper:0.284 bag:0.2827 medium:0.2643\n",
      "\n",
      "Topic #17:\n",
      "amp:1.0 food:0.4634 support:0.4324 facebook:0.4215 like:0.3734 want:0.365 new:0.3587 get:0.3422 eat:0.3271 cheese:0.3046\n",
      "\n",
      "Topic #18:\n",
      "jessica:1.0 biel:0.9976 nyc:0.8298 club:0.7602 join:0.6487 june:0.6165 like:0.587 new:0.5684 two:0.5456 day:0.5227\n",
      "\n",
      "Topic #19:\n",
      "trump:1.0 president:0.4648 amp:0.4188 family:0.3475 democrat:0.3098 support:0.2889 would:0.2886 need:0.2736 people:0.2626 dont:0.2509\n"
     ]
    }
   ],
   "source": [
    "alternative_date.LDA(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic #0:\n",
      "like:1.0 dont:0.6956 get:0.6158 one:0.6083 love:0.4831 know:0.4762 time:0.4445 people:0.4058 good:0.3962 see:0.3933\n",
      "\n",
      "Topic #1:\n",
      "new:1.0 york:0.6643 nyc:0.2856 day:0.2594 amp:0.2339 night:0.1725 time:0.1715 city:0.1676 great:0.1512 brooklyn:0.1476\n",
      "\n",
      "Topic #2:\n",
      "white:1.0 case:0.9274 size:0.9074 freeship:0.7902 rare:0.7414 bid:0.654 nike:0.6265 hurry-up:0.6208 preowned:0.585 amp:0.5696\n",
      "\n",
      "Topic #3:\n",
      "birthday:1.0 mph:0.8464 party:0.7657 welcome:0.7554 wind:0.726 humidity:0.6257 bash:0.6252 elbaeverlasting:0.5923 moneymachinewednesdays:0.5923 -gt:0.5707\n",
      "\n",
      "Topic #4:\n",
      "blue:1.0 cup:0.9341 game:0.8701 tonight:0.6438 stanleycup:0.6419 stanley:0.592 today:0.5614 boston:0.5469 win:0.5324 team:0.5231\n",
      "\n",
      "Topic #5:\n",
      "like:1.0 amp:0.9911 get:0.8736 dont:0.841 trump:0.7667 people:0.6034 know:0.5431 would:0.5285 time:0.528 want:0.5156\n"
     ]
    }
   ],
   "source": [
    "alternative_date.LDA(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://ru.wikipedia.org/wiki/%D0%9F%D0%BB%D0%B5%D0%B9-%D0%BE%D1%84%D1%84_%D0%9A%D1%83%D0%B1%D0%BA%D0%B0_%D0%A1%D1%82%D1%8D%D0%BD%D0%BB%D0%B8_2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Channels filtration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['richmintz', 'SeannyFK', 'JoshyTweetz', 'Lilsunshinegurl', 'hBencee',\n",
       "       'MichaelRMyers5', 'scumbagking__', 'BrownBagCycling', 'dchambersDPM',\n",
       "       'Ernzcognito', 'TheOnlyMikeQ', 'oheydiids', 'AshleyKrista', 'valoria_z',\n",
       "       'Eli_Rivs', 'VHLiv', 'pair_up_', 'jaydestro', 'so_many_amys', 'bjota13',\n",
       "       'Graphix_Divine7', 'johnnybebad6661', 'hannahcomedian',\n",
       "       'JoeyIannitelli', 'RichardPriem', '511nyAdirondack', 'TotalTrafficPHL',\n",
       "       'LivingLegend_23', 'TKYSK8R', 'GormoExJourno'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.tweets_frame[a.tweets_frame['tweets_processed'].map(lambda x: 'traffic' in x \n",
    "                                                      or 'lane' in x \n",
    "                                                      or 'incident' in x \n",
    "                                                      or 'blocked' in x)]\\\n",
    "                                                    ['screen_name'].value_counts().index[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['DianaLaRosa3', 'AppelHowie', 'ArrestALSNow', 'tmj_njs_hrta', 'Iam_ALW',\n",
       "       'tmj_nyc_pharm', 'tmj_RAM_itpm', 'tmj_cte_sales', 'NYPDPSA8',\n",
       "       'tmj_nwk_defben'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.tweets_frame[a.tweets_frame['tweets_processed'].map(lambda x: 'job' in x \n",
    "                                                      or 'hiring' in x \n",
    "                                                      or 'link' in x\n",
    "                                                     or 'apply' in x)]\\\n",
    "                                                    ['screen_name'].value_counts().index[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Guy173', 'Aussiethunda', 'DonMcKenzie', 'cpklapper', 'sdmack',\n",
       "       'andresflava', 'JustJoeyLopez', 'emoleechen', 'oalgarin',\n",
       "       'CharlesJHernan2', 'HippieHooper', 'tariqalhadi'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.tweets_frame[a.tweets_frame['tweets_processed'].map(lambda x: 'photo' in x \n",
    "                                                      or 'new' in x \n",
    "                                                      or 'york' in x\n",
    "                                                     or 'amp' in x)]\\\n",
    "                                                    ['screen_name'].value_counts().index[:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['WWEWomenMatter', 'almighty_red', 'riordainn', 'hammertime1009',\n",
       "       'kareemthagreat'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.tweets_frame[a.tweets_frame['tweets_processed'].map(lambda x: 'like' in x )]\\\n",
    "                                                    ['screen_name'].value_counts().index[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
