{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import itertools\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_frame = pd.read_csv('get-tweets-by-geolocation/data/new_york_training_tweets_15_06.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tweets_LDA():\n",
    "    \n",
    "    '''\n",
    "    Class gets as input raw-data from twitter in csv format.\n",
    "    There realized methods for data preprocessing and perfoming LDA on the preprocessed data.\n",
    "    As output - words that are indicators of particular topic for tweets.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self,):\n",
    "        self.path_to_file = ''\n",
    "        self.tweets_frame = pd.DataFrame()\n",
    "        \n",
    "    def load_data(self,path_to_file):\n",
    "        self.path_to_file = path_to_file\n",
    "        self.tweets_frame = pd.read_csv(self.path_to_file)\n",
    "        \n",
    "        \n",
    "    def data_preprocessing(self,date='',channels_not_to_consider=[]):\n",
    "        '''\n",
    "        date format example: 'Sat Jun 01 00:00:02'\n",
    "        '''\n",
    "        WPT = nltk.WordPunctTokenizer()\n",
    "        lemmanizer = WordNetLemmatizer()\n",
    "        stop_word_list = nltk.corpus.stopwords.words('english')\n",
    "        \n",
    "        if date!='':\n",
    "            self.tweets_frame = self.tweets_frame[self.tweets_frame['created_at'].map(lambda x: x[:len(date)])==date]\n",
    "        if len(channels_not_to_consider)>0:\n",
    "            self.tweets_frame = self.tweets_frame[self.tweets_frame['screen_name'].isin(channels_not_to_consider)==False]\n",
    "            self.tweets_frame = self.tweets_frame[self.tweets_frame['screen_name'].map(lambda x: x.startswith('tmj_'))==False]\n",
    "        \n",
    "        self.tweets_frame['tweets_processed'] =\\\n",
    "                self.tweets_frame['tweet'].map(lambda x: [lemmanizer.lemmatize(y) for y in re.sub(\"[\\d+0-9.â€¦#!'\\\"_?,;/:()â€™%*ðŸ¤¯â€œâ€&ðŸ§¨$ðŸ§¨ðŸ§¡]\", \"\", x.lower()).split() \n",
    "                                         if y not in stop_word_list \n",
    "                                         and y not in ['ãƒ»ãƒ»ãƒ»','','-']\n",
    "                                         and not y.startswith('http')\n",
    "                                         and not y.startswith('@')])\n",
    "        \n",
    "        self.tweets_frame['tweets_processed'] =\\\n",
    "                self.tweets_frame['tweets_processed'].map(lambda x: [y for y in x if len(y)>2])\n",
    "        \n",
    "        self.tweets_frame = self.tweets_frame[self.tweets_frame['tweets_processed'].map(len)>0]\n",
    "        \n",
    "    def LDA(self,n_topics):\n",
    "        \n",
    "        def create_co_occurences_matrix(allowed_words, documents):\n",
    "            word_to_id = dict(zip(allowed_words, range(len(allowed_words))))\n",
    "            documents_as_ids = [np.sort([word_to_id[w] for w in doc if w in word_to_id]).astype('uint32') for doc in documents]\n",
    "            row_ind, col_ind = zip(*itertools.chain(*[[(i, w) for w in doc] for i, doc in enumerate(documents_as_ids)]))\n",
    "            data = np.ones(len(row_ind), dtype='uint32')  # use unsigned int for better memory utilization\n",
    "            max_word_id = max(itertools.chain(*documents_as_ids)) + 1\n",
    "            docs_words_matrix = csr_matrix((data, (row_ind, col_ind)), shape=(len(documents_as_ids), max_word_id))  # efficient arithmetic operations with CSR * CSR\n",
    "            words_cooc_matrix = docs_words_matrix.T * docs_words_matrix  # multiplying docs_words_matrix with its transpose matrix would generate the co-occurences matrix\n",
    "            words_cooc_matrix.setdiag(0)\n",
    "            return words_cooc_matrix, word_to_id \n",
    "        \n",
    "        def print_topics(model, count_vectorizer, n_top_words,words):\n",
    "            #words = cross_tab.columns\n",
    "            for topic_idx, topic in enumerate(model.components_):\n",
    "                print(\"\\nTopic #%d:\" % topic_idx)\n",
    "                print(\" \".join([words[i]+':'+str(round(topic[i]/topic.max(),4))\n",
    "                                for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "        \n",
    "        merged = list(itertools.chain.from_iterable(self.tweets_frame['tweets_processed'].values))\n",
    "        merged_cnts = np.unique(merged,return_counts=True)\n",
    "        words = merged_cnts[0][merged_cnts[1]>2]\n",
    "        a, b = create_co_occurences_matrix(words,self.tweets_frame['tweets_processed'].values)\n",
    "        lda_ = LDA(n_components=n_topics)\n",
    "        lda_.fit(a)\n",
    "        print_topics(lda_,' ',10,words)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Tweets_LDA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.load_data('get-tweets-by-geolocation/data/usa_training_tweets_30_06.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(a.tweets_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.tweets_frame['created_at'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.data_preprocessing('Sat Jun 01',channels_not_to_consider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53875"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a.tweets_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic #0:\n",
      "child:1.0 john:0.7889 airport:0.7571 catholic:0.6085 international:0.5114 ice:0.4992 kennedy:0.4187 show:0.4118 newark:0.4103 church:0.3983\n",
      "\n",
      "Topic #1:\n",
      "school:1.0 high:0.8403 red:0.6803 yankee:0.6201 team:0.5097 game:0.4709 real:0.4301 water:0.3996 mlb:0.3853 job:0.3682\n",
      "\n",
      "Topic #2:\n",
      "new:1.0 york:0.7769 nyc:0.2152 city:0.1612 photo:0.1394 brooklyn:0.138 park:0.1341 jersey:0.1259 posted:0.1221 newyork:0.0836\n",
      "\n",
      "Topic #3:\n",
      "liverpool:1.0 year:0.9942 final:0.9323 uclfinal:0.7892 champion:0.7677 league:0.6857 game:0.6688 today:0.6195 win:0.5655 time:0.5387\n",
      "\n",
      "Topic #4:\n",
      "know:1.0 message:0.9163 text:0.7584 office:0.741 robert:0.6953 lisa:0.6492 deleted:0.6424 muellers:0.6377 bronx:0.5182 dems:0.437\n",
      "\n",
      "Topic #5:\n",
      "freeship:1.0 case:0.9317 get:0.5797 book:0.55 top:0.4955 clear:0.4263 dvd:0.3934 class:0.3697 amp:0.3523 white:0.3517\n",
      "\n",
      "Topic #6:\n",
      "trump:1.0 people:0.6761 president:0.5408 gun:0.4559 report:0.4235 amp:0.3686 shooting:0.3537 state:0.3506 one:0.3477 like:0.3398\n",
      "\n",
      "Topic #7:\n",
      "new:1.0 amp:0.9952 bookcon:0.9194 today:0.7461 check:0.6863 come:0.6616 open:0.6592 book:0.6201 got:0.6103 one:0.5927\n",
      "\n",
      "Topic #8:\n",
      "time:1.0 like:0.9509 one:0.8829 get:0.8432 day:0.7243 ive:0.6626 got:0.662 first:0.6398 year:0.5768 last:0.5622\n",
      "\n",
      "Topic #9:\n",
      "t-shirt:1.0 fashion:0.8379 art:0.7821 hiphoped:0.6493 animal:0.649 one:0.6469 tee:0.6203 piece:0.6031 african:0.5939 jersey:0.5849\n",
      "\n",
      "Topic #10:\n",
      "music:1.0 ball:0.6862 food:0.5418 hookah:0.5076 djfrankswift:0.4423 sugardaddysfridays:0.4423 valet:0.4423 america:0.4278 governor:0.426 hbo:0.4065\n",
      "\n",
      "Topic #11:\n",
      "trump:1.0 amp:0.2702 tax:0.2437 plan:0.2398 like:0.2361 democrat:0.2246 due:0.199 min:0.1823 delay:0.1683 gop:0.1638\n",
      "\n",
      "Topic #12:\n",
      "tonight:1.0 chance:0.9471 t-storm:0.8287 sat:0.8061 jun:0.7519 forecast:0.7398 today:0.4677 sunday:0.4418 mostly:0.3898 sunny:0.3613\n",
      "\n",
      "Topic #13:\n",
      "amp:1.0 night:0.5491 get:0.4395 new:0.4276 like:0.3799 last:0.3204 bar:0.2975 make:0.2811 nyc:0.2685 breakfast:0.2615\n",
      "\n",
      "Topic #14:\n",
      "like:1.0 shit:0.5222 dont:0.5043 get:0.4108 know:0.3752 really:0.3505 nigga:0.3351 lol:0.3233 people:0.3048 yall:0.299\n",
      "\n",
      "Topic #15:\n",
      "day:1.0 great:0.8924 today:0.8817 good:0.7518 thank:0.7442 love:0.7298 amp:0.6864 one:0.6796 new:0.6367 time:0.5892\n",
      "\n",
      "Topic #16:\n",
      "dont:1.0 like:0.9587 get:0.715 people:0.6748 know:0.6226 one:0.5967 cant:0.4908 make:0.4746 time:0.4726 see:0.4141\n",
      "\n",
      "Topic #17:\n",
      "amp:1.0 play:0.7716 link:0.7442 get:0.6638 check:0.6415 free:0.5854 shot:0.5594 shop:0.5473 height:0.5187 bio:0.5049\n",
      "\n",
      "Topic #18:\n",
      "yankee:1.0 sox:0.403 game:0.3704 stadium:0.354 red:0.3277 baseball:0.1715 fan:0.1693 sale:0.1673 let:0.1548 chris:0.1263\n",
      "\n",
      "Topic #19:\n",
      "happy:1.0 pride:0.9566 month:0.824 june:0.3801 day:0.3344 birthday:0.3139 amp:0.274 new:0.2691 love:0.2684 nyc:0.2493\n"
     ]
    }
   ],
   "source": [
    "a.LDA(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.google.com/search?q=liverpool+%D0%BC%D0%B0%D1%82%D1%87&rlz=1C1CHZL_enUA833UA833&oq=liverpool+%D0%BC%D0%B0%D1%82%D1%87+&aqs=chrome..69i57j0l5.8428j0j7&sourceid=chrome&ie=UTF-8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "alternative_date = Tweets_LDA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "alternative_date.load_data('get-tweets-by-geolocation/data/new_york_training_tweets_15_06.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "332548"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(alternative_date.tweets_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Thu Jun 06    67546\n",
       "Sat Jun 01    59087\n",
       "Fri May 31    46951\n",
       "Wed Jun 05    38703\n",
       "Wed Jun 12    29989\n",
       "Sun Jun 02    29291\n",
       "Thu Jun 13    27292\n",
       "Fri Jun 07    16485\n",
       "Tue Jun 04    14549\n",
       "Thu May 30     2655\n",
       "Name: created_at, dtype: int64"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alternative_date.tweets_frame['created_at'].map(lambda x: x[:10]).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "alternative_date.data_preprocessing('Thu Jun 13',channels_not_to_consider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24971"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(alternative_date.tweets_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic #0:\n",
      "trump:1.0 would:0.6534 get:0.6515 people:0.594 know:0.5672 dont:0.5633 one:0.5334 amp:0.5299 like:0.4981 say:0.4798\n",
      "\n",
      "Topic #1:\n",
      "amp:1.0 printing:0.6949 free:0.6494 card:0.582 business:0.5169 full:0.4942 new:0.4853 print:0.4441 price:0.4436 flyer:0.3729\n",
      "\n",
      "Topic #2:\n",
      "size:1.0 birthday:0.8868 rare:0.8036 white:0.7967 bid:0.7441 party:0.7313 welcome:0.7195 nike:0.711 hurry-up:0.7064 preowned:0.6657\n",
      "\n",
      "Topic #3:\n",
      "construction:1.0 town:0.9676 bloomfield:0.8781 direction:0.7982 line:0.7671 pulse:0.6917 year:0.4173 street:0.3945 center:0.3859 road:0.3562\n",
      "\n",
      "Topic #4:\n",
      "mph:1.0 wind:0.885 humidity:0.7158 -gt:0.6552 weather:0.5592 pressure:0.5485 sky:0.5244 cloud:0.507 clear:0.5029 current:0.4776\n",
      "\n",
      "Topic #5:\n",
      "cup:1.0 blue:0.8811 stanley:0.6443 boston:0.5735 stanleycup:0.5678 game:0.5438 win:0.524 team:0.464 louis:0.372 year:0.3688\n",
      "\n",
      "Topic #6:\n",
      "time:1.0 amp:0.5801 one:0.4295 real:0.3862 would:0.3442 create:0.2982 year:0.2931 people:0.2919 wednesday:0.2906 need:0.2825\n",
      "\n",
      "Topic #7:\n",
      "game:1.0 love:0.5552 blue:0.5097 fan:0.3867 one:0.3348 team:0.3242 hockey:0.3139 like:0.3063 great:0.3026 tonight:0.2936\n",
      "\n",
      "Topic #8:\n",
      "ive:1.0 one:0.9349 like:0.922 year:0.8419 time:0.8036 never:0.7408 think:0.6952 dont:0.6322 know:0.584 get:0.5543\n",
      "\n",
      "Topic #9:\n",
      "like:1.0 dont:0.7432 get:0.5517 know:0.4524 people:0.3856 one:0.3855 shit:0.3584 really:0.3558 love:0.3376 lol:0.313\n",
      "\n",
      "Topic #10:\n",
      "tonight:1.0 today:0.989 forecast:0.8248 jun:0.7911 thu:0.7856 rain:0.7582 chance:0.7411 shower:0.6132 cloudy:0.373 mostly:0.2905\n",
      "\n",
      "Topic #11:\n",
      "dream:1.0 biden:0.9708 joe:0.7962 please:0.7802 good:0.7657 checked:0.7493 hold:0.6627 palestinian:0.5989 puerto:0.5855 offer:0.5481\n",
      "\n",
      "Topic #12:\n",
      "like:1.0 dont:0.8937 make:0.7132 people:0.7017 cant:0.6141 one:0.5873 know:0.5767 see:0.5546 got:0.4829 link:0.4547\n",
      "\n",
      "Topic #13:\n",
      "new:1.0 york:0.8101 nyc:0.2151 city:0.1847 park:0.1463 brooklyn:0.1303 photo:0.1277 posted:0.112 manhattan:0.1082 street:0.0839\n",
      "\n",
      "Topic #14:\n",
      "day:1.0 get:0.7632 time:0.6872 back:0.6566 see:0.5564 good:0.5545 new:0.5244 love:0.5077 amp:0.4944 got:0.4743\n",
      "\n",
      "Topic #15:\n",
      "day:1.0 one:0.9306 like:0.7997 amp:0.7434 today:0.6095 love:0.5968 year:0.5378 last:0.5284 school:0.5276 time:0.5246\n",
      "\n",
      "Topic #16:\n",
      "case:1.0 freeship:0.874 plastic:0.4791 sleeve:0.4172 dvd:0.346 amp:0.341 black:0.3315 paper:0.284 bag:0.2827 medium:0.2643\n",
      "\n",
      "Topic #17:\n",
      "amp:1.0 food:0.4634 support:0.4324 facebook:0.4215 like:0.3734 want:0.365 new:0.3587 get:0.3422 eat:0.3271 cheese:0.3046\n",
      "\n",
      "Topic #18:\n",
      "jessica:1.0 biel:0.9976 nyc:0.8298 club:0.7602 join:0.6487 june:0.6165 like:0.587 new:0.5684 two:0.5456 day:0.5227\n",
      "\n",
      "Topic #19:\n",
      "trump:1.0 president:0.4648 amp:0.4188 family:0.3475 democrat:0.3098 support:0.2889 would:0.2886 need:0.2736 people:0.2626 dont:0.2509\n"
     ]
    }
   ],
   "source": [
    "alternative_date.LDA(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic #0:\n",
      "like:1.0 dont:0.6956 get:0.6158 one:0.6083 love:0.4831 know:0.4762 time:0.4445 people:0.4058 good:0.3962 see:0.3933\n",
      "\n",
      "Topic #1:\n",
      "new:1.0 york:0.6643 nyc:0.2856 day:0.2594 amp:0.2339 night:0.1725 time:0.1715 city:0.1676 great:0.1512 brooklyn:0.1476\n",
      "\n",
      "Topic #2:\n",
      "white:1.0 case:0.9274 size:0.9074 freeship:0.7902 rare:0.7414 bid:0.654 nike:0.6265 hurry-up:0.6208 preowned:0.585 amp:0.5696\n",
      "\n",
      "Topic #3:\n",
      "birthday:1.0 mph:0.8464 party:0.7657 welcome:0.7554 wind:0.726 humidity:0.6257 bash:0.6252 elbaeverlasting:0.5923 moneymachinewednesdays:0.5923 -gt:0.5707\n",
      "\n",
      "Topic #4:\n",
      "blue:1.0 cup:0.9341 game:0.8701 tonight:0.6438 stanleycup:0.6419 stanley:0.592 today:0.5614 boston:0.5469 win:0.5324 team:0.5231\n",
      "\n",
      "Topic #5:\n",
      "like:1.0 amp:0.9911 get:0.8736 dont:0.841 trump:0.7667 people:0.6034 know:0.5431 would:0.5285 time:0.528 want:0.5156\n"
     ]
    }
   ],
   "source": [
    "alternative_date.LDA(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://ru.wikipedia.org/wiki/%D0%9F%D0%BB%D0%B5%D0%B9-%D0%BE%D1%84%D1%84_%D0%9A%D1%83%D0%B1%D0%BA%D0%B0_%D0%A1%D1%82%D1%8D%D0%BD%D0%BB%D0%B8_2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Channels filtration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['richmintz', 'SeannyFK', 'JoshyTweetz', 'Lilsunshinegurl', 'hBencee',\n",
       "       'MichaelRMyers5', 'scumbagking__', 'BrownBagCycling', 'dchambersDPM',\n",
       "       'Ernzcognito', 'TheOnlyMikeQ', 'oheydiids', 'AshleyKrista', 'valoria_z',\n",
       "       'Eli_Rivs', 'VHLiv', 'pair_up_', 'jaydestro', 'so_many_amys', 'bjota13',\n",
       "       'Graphix_Divine7', 'johnnybebad6661', 'hannahcomedian',\n",
       "       'JoeyIannitelli', 'RichardPriem', '511nyAdirondack', 'TotalTrafficPHL',\n",
       "       'LivingLegend_23', 'TKYSK8R', 'GormoExJourno'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.tweets_frame[a.tweets_frame['tweets_processed'].map(lambda x: 'traffic' in x \n",
    "                                                      or 'lane' in x \n",
    "                                                      or 'incident' in x \n",
    "                                                      or 'blocked' in x)]\\\n",
    "                                                    ['screen_name'].value_counts().index[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['DianaLaRosa3', 'AppelHowie', 'ArrestALSNow', 'tmj_njs_hrta', 'Iam_ALW',\n",
       "       'tmj_nyc_pharm', 'tmj_RAM_itpm', 'tmj_cte_sales', 'NYPDPSA8',\n",
       "       'tmj_nwk_defben'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.tweets_frame[a.tweets_frame['tweets_processed'].map(lambda x: 'job' in x \n",
    "                                                      or 'hiring' in x \n",
    "                                                      or 'link' in x\n",
    "                                                     or 'apply' in x)]\\\n",
    "                                                    ['screen_name'].value_counts().index[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Guy173', 'Aussiethunda', 'DonMcKenzie', 'cpklapper', 'sdmack',\n",
       "       'andresflava', 'JustJoeyLopez', 'emoleechen', 'oalgarin',\n",
       "       'CharlesJHernan2', 'HippieHooper', 'tariqalhadi'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.tweets_frame[a.tweets_frame['tweets_processed'].map(lambda x: 'photo' in x \n",
    "                                                      or 'new' in x \n",
    "                                                      or 'york' in x\n",
    "                                                     or 'amp' in x)]\\\n",
    "                                                    ['screen_name'].value_counts().index[:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['WWEWomenMatter', 'almighty_red', 'riordainn', 'hammertime1009',\n",
       "       'kareemthagreat'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.tweets_frame[a.tweets_frame['tweets_processed'].map(lambda x: 'like' in x )]\\\n",
    "                                                    ['screen_name'].value_counts().index[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels_not_to_consider = [#traffic\n",
    "        '511NY', '511NYC', 'TotalTrafficNYC', '511nyNJ', '511NYMidHudson',\n",
    "       'Reported_NYC', '511ny456', '511nyAlbany', '511nyACE', '511ny123', \n",
    "        '511nyBDFV', '511nyWNY', '511nyRochester',\n",
    "       '511nyLongIsland', 'AllKindsWeather', \n",
    "       '511ny7',\n",
    "    #jobs\n",
    "    'tmj_nyc_jobs', 'tmj_nyc_adv', 'CalvaryHospJobs',\n",
    "       'tmj_RAM_cstsrv', 'tmj_nyc_mgmt', 'tmj_roc_cler',\n",
    "       'tmj_NAS_edu', 'tmj_nyc_it', 'tmj_NYC_schn', 'tmj_NJN_cstsrv',\n",
    "       'tmj_nwk_retail', 'tmj_nyc_nursing', 'tmj_nyc_sales', 'tmj_nyc_cler',\n",
    "       'USSJobs', 'tmj_nwk_sales', 'tmj_nyc_legal', 'tmj_nwk_eng',\n",
    "       'tmj_nyc_retail', 'tmj_nyc_transp', 'WGPNursingJobs', 'tmj_nwk_socsci',\n",
    "       'tmj_NYC_adm', 'nwkmeddevice', 'tmj_nyc_edu', 'tmj_nwk_secure',\n",
    "       'nwknursing',\n",
    "    'tmj_nyc_finance', 'tmj_NYC_secure', 'tmj_nyc_acct', 'tmj_nwk_cler',\n",
    "       'tmj_nyc_banking', 'tmj_ny_hrta', 'tmj_nwk_mgmt', 'tmj_nwk_auto',\n",
    "       'tmj_nyc_cstsrv', 'tmj_nyc_health', 'nwkhealth', 'tmj_nwk_cstsrv',\n",
    "       'tmj_nyc_manuf', 'tmj_cte_nursing', 'tmj_nyc_eng', 'tmj_nyc_itpm1',\n",
    "       'tmj_nyc_hr', 'tmj_NY_sales', 'CVSHealthJobs', 'tmj_NAS_mgmt',\n",
    "       'CompassJobBoard', 'tmj_nyc_hrta', 'tmj_NAS_nursing', 'BostonMarketJob',\n",
    "       'tmj_nyc_labor', 'MetsAvenue', 'tmj_nwk_schn', 'tmj_RAM_nursing',\n",
    "       'tmj_nwk_acct', 'tmj_nwk_jobs', 'tmj_nyc_art', 'tmj_nwk_labor',\n",
    "       'tmj_roc_eng', 'tmj_NAS_transp', 'CA_ROC_Jobs2', 'tmj_nyc_cosmo',\n",
    "       'tmj_RAM_edu', 'tmj_NAS_health', 'tmj_nwk_facmgmt', 'tmj_NAS_facmgmt',\n",
    "       'GodivaJobs', 'tmj_RAM_acct', 'tmj_roc_health', 'tmj_nyc_itdb',\n",
    "       'tmj_nwk_transp', 'tmj_nwk_edu', 'tmj_RAM_retail', 'tmj_RAM_mgmt',\n",
    "       'tmj_NAS_socsci', 'tmj_nwk_prod', 'tmj_nyc_realest', 'tmj_NJ_facmgmt', 'tmj_njn_retail',\n",
    "       'tmj_roc_nursing', 'tmj_nwk_finance', 'Fly_Sistah', 'tmj_NYC_skltrd',\n",
    "       'tmj_nya_nursing', 'tmj_nwk_web', 'tmj_roc_cstsrv', 'tmj_nys_jobs',\n",
    "       'tmj_njc_hrta', 'tmj_NAS_retail', 'tmj_roc_hrta',\n",
    "       'ChurchCathy', 'tmj_NAS_secure', 'tmj_RAM_art', 'tmj_NAS_labor',\n",
    "       'tmj_NAS_physici', 'tmj_nwk_skltrd', 'tmj_roc_sales', 'tmj_nwk_purch',\n",
    "       'tmj_NYS_NURSING', 'tmj_nwk_physici', 'tmj_njn_hrta', 'Mezikenyc',\n",
    "       'JCI_Jobs', 'tmj_NAS_acct', 'tmj_NYC_gensci', 'tmj_nya_eng',\n",
    "       'tmj_nwk_nonprft', 'tmj_roc_manuf', 'nwkitsupport', 'tmj_NY_LABOR',\n",
    "       'tmj_ny_mgmt', 'tmj_njn_health',\n",
    "       'tmj_nj_hrta', 'tmj_NAS_cstsrv', 'tmj_nwk_it', 'tmj_nya_transp',\n",
    "       'tmj_ct_nursing', 'tmj_NJ_sales', 'tmj_nya_acct',\n",
    "       'nwkmanuf', 'tmj_nys_cstsrv', 'tmj_njn_nursing',\n",
    "       'tmj_njn_mgmt', 'cbwaszak', 'tmj_NAS_cler', 'tmj_RAM_auto',\n",
    "       'tmj_nwk_art'\n",
    "    'WWEWomenMatter',\n",
    "    #photos\n",
    "    'ThomGambino', 'Xsanthemum', 'francesco212', 'Empressjurnee',\n",
    "       'andrerivera801', 'janice830', 'Ingridebap', 'StevieSoFetch_',\n",
    "       'EstebanDaHost', 'graceyhanderson', 'bccdny', 'brian_wood_'\n",
    "       ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
